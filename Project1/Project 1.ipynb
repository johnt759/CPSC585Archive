{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70aecb69",
   "metadata": {},
   "source": [
    "# Spring 2022\n",
    "# CPSC 585 Project 1\n",
    "## Raymond Carpio\n",
    "## Yu Pan\n",
    "## Sijie Shang\n",
    "## John Tu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad28f2",
   "metadata": {},
   "source": [
    "# 1. Use from dataset import * to load the module, then examine TRAINING_SET, TEST_SET, and MESSAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56c8d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING_SET:  ('A', [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "TEST_SET:  ('A', [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "MESSAGE:  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from dataset import * # Import the entire dataset\n",
    "import random # Needed to generate random numbers\n",
    "\n",
    "print(\"TRAINING_SET: \", TRAINING_SET[0])\n",
    "print(\"TEST_SET: \", TEST_SET[0])\n",
    "print(\"MESSAGE: \", MESSAGE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc6546",
   "metadata": {},
   "source": [
    "# 2. Implement a Python function show(image) to print() a letter image row-by-row using a hash mark ('#') for 1 and a space (' ') for 0. Test this function on some of the images in TRAINING_SET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448f94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "\n",
      "\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "\n",
      "\n",
      "    ########    \n",
      "    ########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "\n",
      "\n",
      "##########      \n",
      "##########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "##########      \n",
      "##########      \n",
      "                \n",
      "                \n",
      "\n",
      "\n",
      "##############  \n",
      "##############  \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "\n",
      "\n",
      "##############  \n",
      "##############  \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "########        \n",
      "                \n",
      "                \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show(image):\n",
    "    letter_len = len(image)\n",
    "    counter = 0\n",
    "    # Since there are 16 characters in each row\n",
    "    # and the image is 16 rows by 16 columns,\n",
    "    # consider adding a newline for each current image\n",
    "    # by setting the counter to 0 after printing\n",
    "    # out the 16th character.\n",
    "    for x in range(letter_len):\n",
    "        if image[x] == 1:\n",
    "            print('#', end='')\n",
    "        else:\n",
    "            print(' ', end='')\n",
    "        counter += 1\n",
    "        if counter == 16:\n",
    "            counter = 0\n",
    "            print('\\n', end='')\n",
    "\n",
    "for letters in TRAINING_SET[:6]:\n",
    "    show(letters[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03021a",
   "metadata": {},
   "source": [
    "# 3. Implement a perceptron to recognize the letter A by producing output +1 for images of the letter A and -1 for images of all other letters.\n",
    "## Begin with weights initialized to random values and a learning rate of 𝛼 = 0.01. Use Equation 1.1 in the textbook to predict whether the image is an A.\n",
    "## If the prediction is incorrect, train the perceptron by applying the update rule of Equation 1.4 in the textbook. Continue training on the items in TRAINING_SET until all items are correctly classified.\n",
    "### Hint: You may also want to examine the pseudocode on p. 35 of Artificial Intelligence Engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81e8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_A_training_value = TRAINING_SET[0][0]\n",
    "letter_A_training_img = TRAINING_SET[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random weights here.\n",
    "def get_random(num):\n",
    "    weight = [0] * (num+1)\n",
    "    for i in range(num+1):\n",
    "        weight[i] = random.randint(0, 10)\n",
    "        \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c9bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the dot product for the dataset and the random weights.\n",
    "def dot_prod_predict(data, weight):\n",
    "    dot_product = 0\n",
    "    len_data = len(data)+1 # Add 1 due to bias involved.\n",
    "    for i in range(len_data):\n",
    "        # Check if the current input data is a bias.\n",
    "        # Bias is not associated with any weight, so treat it\n",
    "        # as a 1 and simply add the weight into the dot product.\n",
    "        if i == 0:\n",
    "            dot_product += weight[i]\n",
    "        else:\n",
    "            dot_product += (data[i-1] * weight[i])\n",
    "    if dot_product > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d044aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for obtaining perceptrons of training data\n",
    "# First, generate random values for input weights and set learning rate to 0.01.\n",
    "# Then, using equation 1.1 (which is the sign for dot product between weights and data),\n",
    "# compare the predicted values to the actual values. If there are any mismatch between\n",
    "# the two, then update the weights until all the values can converge.\n",
    "# The update rule follows equation 1.4, which can be assumed as followed:\n",
    "# new weight = current weight + learning rate(y_actual-y_predict)*x_input\n",
    "def training_perceptron(input_value, input_img, data_set=TRAINING_SET):\n",
    "    # Based on the pseudocode from Artificial Intelligence Engines pg. 35\n",
    "    y_actual = [1 if letter[0] == input_value else -1 for letter in data_set]\n",
    "    weights = get_random(len(input_img)) # Generate the random values for the weights.\n",
    "    learn_rate = 0.01\n",
    "    # This variable indicates that the perceptron can still be trained in order to improve\n",
    "    # the set of weights such that all of the values in the dataset can converge correctly.\n",
    "    # If all the values can converge correctly, set this variable to false indicating that\n",
    "    # the set of weights doesn't need to be updated anymore.\n",
    "    keep_learning = True\n",
    "    \n",
    "    while keep_learning:\n",
    "        keep_learning = False # Assume that all the values in the dataset can converge correctly.\n",
    "        for i in range(len(data_set)):\n",
    "            x_in = data_set[i][1] # Be sure to refer to the bitmap image or else the dot product will not compute at all!\n",
    "            y_pred = dot_prod_predict(x_in, weights) # Get the dot products for the input values and weights.\n",
    "            \n",
    "            # Create a new list of weights and initialize them to zero.\n",
    "            temp_weights = [0] * len(weights)\n",
    "            \n",
    "            # If the sign of the predicted value of y is negative while the actual\n",
    "            # value of y is positive (and vice versa), apply the update rules to the weights.\n",
    "            error_value = y_actual[i] - y_pred\n",
    "            if (y_actual[i] == 1 and y_pred < 0) or (y_actual[i] == -1 and y_pred > 0):\n",
    "                # Populate the values over into the temporary weights.\n",
    "                keep_learning = True\n",
    "                for j in range(len(temp_weights)):\n",
    "                    if j == 0:\n",
    "                        temp_weights[j] = learn_rate*error_value\n",
    "                    else:\n",
    "                        temp_weights[j] = learn_rate*error_value*x_in[j-1]\n",
    "                    \n",
    "            # Now apply the new values into the current list of weights.\n",
    "            for k in range(len(weights)):\n",
    "                weights[k] = weights[k] + temp_weights[k]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7978ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.019999999999963, 2.340000000000105, 2.340000000000105, -8.799999999999901, -0.7999999999999184, 0.6000000000000822, -6.399999999999952, -6.499999999999949, -0.4999999999999608, -7.999999999999917, 1.000000000000104, -3.4599999999999844, -8.459999999999908, -1.5400000000000034, 5.460000000000097, 7.0, 3.0, -2.8800000000000043, 1.1200000000000614, -9.099999999999895, -9.099999999999895, 4.260000000000122, -4.739999999999988, 1.6000000000000405, -3.400000000000003, -5.579999999999969, 1.4200000000000617, -7.159999999999936, 1.840000000000126, -2.560000000000004, -3.560000000000004, 4.0, 6.0, -4.779999999999986, -0.7800000000000037, -1.4799999999999613, -4.479999999999995, -5.219999999999977, -3.2200000000000037, 1.659999999999998, 1.659999999999998, 0.11999999999999764, 1.1199999999999974, -2.059999999999983, -4.060000000000004, -0.059999999999960654, -6.059999999999959, 2.0, 4.0, -3.5200000000000022, 6.480000000000075, 2.7200000000001268, -3.2800000000000056, 0.660000000000061, 1.6600000000000832, 5.500000000000053, 7.500000000000053, 1.3600000000000403, 2.3600000000000625, 2.080000000000105, -5.919999999999963, -3.8400000000000025, 5.160000000000082, 1.0, 7.0, 1.280000000000019, 1.280000000000019, 3.4400000000001274, 2.440000000000105, 1.2600000000000615, 2.2600000000000837, 0.34000000000001807, -4.659999999999989, -0.5399999999999822, -2.540000000000004, 1.5600000000000618, 4.560000000000116, 5.540000000000052, -0.4600000000000015, 4.0, 2.0, 1.280000000000019, 0.2799999999999967, -1.5599999999999827, -2.560000000000005, -4.419999999999994, 0.5800000000000183, 1.5599999999999978, 7.560000000000052, -3.1200000000000028, 3.880000000000085, -2.420000000000004, -0.41999999999998205, -0.7800000000000018, 6.220000000000059, 5.0, 10.0, 0.8399999999999972, -3.1600000000000024, -6.8199999999999426, -3.820000000000005, 1.3600000000000616, 4.36000000000012, 0.4600000000000395, -3.540000000000004, -4.0000000000000036, 3.0000000000000844, 2.1600000000000623, 5.160000000000103, 1.5199999999999978, 6.520000000000053, 8.0, 9.0, 4.72000000000007, 5.72000000000007, -0.9599999999999611, -6.95999999999994, -3.8200000000000043, -4.819999999999986, 2.460000000000084, 2.460000000000084, 4.84000000000011, 3.8400000000001064, -3.8400000000000034, 1.16000000000004, 6.920000000000066, 5.920000000000066, 10.0, 6.0, -3.6000000000000023, -0.6000000000000025, 1.2200000000000828, -4.779999999999988, 1.2400000000000615, 0.24000000000003932, -0.08000000000000329, -1.080000000000003, 2.1600000000000623, -1.8400000000000036, -4.719999999999988, -5.719999999999966, 2.040000000000041, -2.9600000000000026, 6.0, 6.0, 6.100000000000083, 5.100000000000083, -4.479999999999994, 0.5200000000000609, 0.6600000000000823, -0.33999999999993935, -0.03999999999996065, 1.9600000000000835, 2.9600000000001057, -4.040000000000004, 0.9200000000000612, 2.9200000000001056, 0.09999999999999763, 6.100000000000062, 0.0, 8.0, 3.1600000000000206, 7.160000000000039, 1.840000000000062, 2.8400000000000842, -0.13999999999998197, 4.86000000000011, 8.020000000000042, 1.0199999999999982, 0.1999999999999976, 4.20000000000006, 1.6800000000000406, -0.3200000000000033, -1.3800000000000014, -0.38000000000000156, 0.0, 10.0, -1.8400000000000012, 8.16000000000004, -3.1600000000000037, -3.1600000000000037, -3.1400000000000037, -4.140000000000001, 7.020000000000042, 2.0199999999999982, -1.8000000000000018, 3.200000000000042, -3.320000000000003, -2.320000000000003, 2.46000000000002, 2.46000000000002, 6.0, 10.0, -3.2600000000000025, 3.7400000000000637, 2.480000000000084, -1.5200000000000042, 2.600000000000084, -0.39999999999998204, 0.7800000000000398, -1.220000000000004, 4.140000000000104, 3.1400000000000845, 1.7800000000000193, -1.2200000000000022, -2.1800000000000015, 3.8200000000000425, 10.0, 8.0, -1.2600000000000022, 3.7400000000000637, 2.480000000000084, 3.480000000000106, -0.39999999999998204, 3.6000000000001062, 4.780000000000111, -3.2200000000000037, 0.14000000000001805, 5.140000000000104, -0.22000000000000236, -2.220000000000002, 4.8200000000000465, 3.8200000000000425, 0.0, 5.0, 6.0, 1.0, 10.0, 8.0, 0.0, 10.0, 0.0, 7.0, 1.0, 7.0, 2.0, 8.0, 6.0, 8.0, 9.0, 10.0, 0.0, 1.0, 3.0, 10.0, 7.0, 6.0, 9.0, 6.0, 0.0, 1.0, 10.0, 7.0, 7.0, 9.0, 10.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "perceptron = training_perceptron(letter_A_training_value, letter_A_training_img)\n",
    "print(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae2daf",
   "metadata": {},
   "source": [
    "# 4. Use your trained perceptron to classify each image in TEST_SET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a04ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 1],\n",
       " ['B', -1],\n",
       " ['C', -1],\n",
       " ['D', -1],\n",
       " ['E', -1],\n",
       " ['F', -1],\n",
       " ['G', -1],\n",
       " ['H', -1],\n",
       " ['I', -1],\n",
       " ['J', -1],\n",
       " ['K', -1],\n",
       " ['L', -1],\n",
       " ['M', -1],\n",
       " ['N', -1],\n",
       " ['O', -1],\n",
       " ['P', -1],\n",
       " ['Q', -1],\n",
       " ['R', -1],\n",
       " ['S', -1],\n",
       " ['T', -1],\n",
       " ['U', -1],\n",
       " ['V', -1],\n",
       " ['W', -1],\n",
       " ['X', -1],\n",
       " ['Y', -1],\n",
       " ['Z', -1],\n",
       " ['A', 1],\n",
       " ['B', -1],\n",
       " ['C', -1],\n",
       " ['D', -1],\n",
       " ['E', -1],\n",
       " ['F', -1],\n",
       " ['G', -1],\n",
       " ['H', -1],\n",
       " ['I', -1],\n",
       " ['J', -1],\n",
       " ['K', -1],\n",
       " ['L', -1],\n",
       " ['M', -1],\n",
       " ['N', -1],\n",
       " ['O', -1],\n",
       " ['P', -1],\n",
       " ['Q', -1],\n",
       " ['R', -1],\n",
       " ['S', -1],\n",
       " ['T', -1],\n",
       " ['U', -1],\n",
       " ['V', -1],\n",
       " ['W', -1],\n",
       " ['X', -1],\n",
       " ['Y', -1],\n",
       " ['Z', -1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_train = []\n",
    "\n",
    "for letter, bitmap in TRAINING_SET:\n",
    "    perceptron_train.append([letter, dot_prod_predict(bitmap, perceptron)])\n",
    "    \n",
    "perceptron_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a305e170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', -1],\n",
       " ['B', -1],\n",
       " ['C', -1],\n",
       " ['D', -1],\n",
       " ['E', -1],\n",
       " ['F', -1],\n",
       " ['G', -1],\n",
       " ['H', -1],\n",
       " ['I', 1],\n",
       " ['J', -1],\n",
       " ['K', -1],\n",
       " ['L', -1],\n",
       " ['M', 1],\n",
       " ['N', -1],\n",
       " ['O', -1],\n",
       " ['P', -1],\n",
       " ['Q', 1],\n",
       " ['R', -1],\n",
       " ['S', -1],\n",
       " ['T', -1],\n",
       " ['U', -1],\n",
       " ['V', 1],\n",
       " ['W', 1],\n",
       " ['X', 1],\n",
       " ['Y', 1],\n",
       " ['Z', 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_test = []\n",
    "\n",
    "for letter, bitmap in TEST_SET:\n",
    "    perceptron_test.append([letter, dot_prod_predict(bitmap, perceptron)])\n",
    "    \n",
    "perceptron_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad6a3",
   "metadata": {},
   "source": [
    "# Does the perceptron correctly distinguish the letter A from the other letters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31283392",
   "metadata": {},
   "source": [
    "## A: Yes, the perceptron correctly distinguishes letter A from other letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754a7a8",
   "metadata": {},
   "source": [
    "# 5. Repeat experiment (3) for the rest of the letters, for a total of 26 trained perceptrons. If any perceptron fails to converge, implement the “pocket algorithm” as described at the end of Section 1.2.1.1 of the textbook on p. 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592019fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is similar to training_perceptron(), except done as a pocket algorithm.\n",
    "# If there is a mismatch for the signs between the predicted value and actual value of y,\n",
    "# then a misclassification exists, so increment the total number by 1.\n",
    "# If the number of misclassifications for the current set of weights is greater than 0,\n",
    "# then the new set of weights is better than the current set, so replace the current\n",
    "# with the better one. Otherwise, retain the current values and discard the new ones.\n",
    "def training_perceptron_pocket(input_value, input_img, data_set=TRAINING_SET):\n",
    "    y_actual = [1 if letter[0] == input_value else -1 for letter in data_set]\n",
    "    weights = get_random(len(input_img)) # Generate the random values for the weights.\n",
    "    curr_w = weights # Assume the current set of weights is the best one possible.\n",
    "    learn_rate = 0.01\n",
    "    \n",
    "    # Default value for number of epochs to perform is 100\n",
    "    for i in range(0, 100):\n",
    "        misclassifications = 0 # This value increments only if there is mismatch between predicted sign and actual sign.\n",
    "        for i in range(len(data_set)):\n",
    "            x_in = data_set[i][1] # Be sure to refer to the bitmap image or else the dot product will not compute at all!\n",
    "            y_pred = dot_prod_predict(x_in, weights) # Get the dot products for the input values and weights.\n",
    "            \n",
    "            # Create a new list of weights and initialize them to zero.\n",
    "            temp_weights = [0] * len(weights)\n",
    "            \n",
    "            # If the sign of the predicted value of y is negative while the actual\n",
    "            # value of y is positive (and vice versa), apply the update rules to the weights.\n",
    "            error_value = y_actual[i] - y_pred\n",
    "            if (y_actual[i] == 1 and y_pred < 0) or (y_actual[i] == -1 and y_pred > 0):\n",
    "                # Populate the values over into the temporary weights.\n",
    "                for j in range(len(temp_weights)):\n",
    "                    # Check if the bias perceptron exists.\n",
    "                    if j == 0:\n",
    "                        temp_weights[j] = learn_rate*error_value\n",
    "                    else:\n",
    "                        temp_weights[j] = learn_rate*error_value*x_in[j-1]\n",
    "                misclassifications += 1\n",
    "                    \n",
    "            # Now apply the new values into the current list of weights.\n",
    "            for k in range(len(weights)):\n",
    "                weights[k] = weights[k] + temp_weights[k]\n",
    "        # If the number of misclassifications is nonzero, then apply the new set of weights\n",
    "        # into the current set. Otherwise, break out of loop and return the set of weights.\n",
    "        if misclassifications != 0:\n",
    "            curr_w = weights\n",
    "        else:\n",
    "            break\n",
    "    return curr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e6d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "26\n",
      "Length of perceptron for all letters:  26\n"
     ]
    }
   ],
   "source": [
    "# Repeat problem 3, except do all of the letters from the training set.\n",
    "def training_perceptron_all(dataset):\n",
    "    perceptron_list = []\n",
    "    letter_list = []\n",
    "    img_list = []\n",
    "    # Obtain the 26 letters and their corresponding images.\n",
    "    for i in range(26):\n",
    "        letter_list.append(dataset[i][0])\n",
    "        img_list.append(dataset[i][1])\n",
    "    print(letter_list)\n",
    "    print(len(img_list))\n",
    "    # Now obtain the perceptrons by doing each letter and image.\n",
    "    for j in range(26):\n",
    "        weight = training_perceptron_pocket(letter_list[j], img_list[j])\n",
    "        perceptron_list.append(weight)\n",
    "    return perceptron_list\n",
    "\n",
    "all_letters_perceptron = training_perceptron_all(TRAINING_SET)\n",
    "# Ensure that there are 26 weights for each letter in the perceptron obtained above.\n",
    "print(\"Length of perceptron for all letters: \", len(all_letters_perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250ba20",
   "metadata": {},
   "source": [
    "# 6. Repeat experiment (4) for each trained perceptron on each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0b62ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', -1],\n",
       " ['B', 1],\n",
       " ['C', 1],\n",
       " ['D', -1],\n",
       " ['E', 1],\n",
       " ['F', 1],\n",
       " ['G', 1],\n",
       " ['H', -1],\n",
       " ['I', 1],\n",
       " ['J', -1],\n",
       " ['K', 1],\n",
       " ['L', 1],\n",
       " ['M', 1],\n",
       " ['N', 1],\n",
       " ['O', 1],\n",
       " ['P', 1],\n",
       " ['Q', 1],\n",
       " ['R', 1],\n",
       " ['S', -1],\n",
       " ['T', -1],\n",
       " ['U', -1],\n",
       " ['V', 1],\n",
       " ['W', 1],\n",
       " ['X', 1],\n",
       " ['Y', 1],\n",
       " ['Z', 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_perceptrons_pocket = []\n",
    "letter_list = []\n",
    "img_list = []\n",
    "\n",
    "for i in range(26):\n",
    "    letter_list.append(TEST_SET[i][0])\n",
    "    img_list.append(TEST_SET[i][1])\n",
    "\n",
    "for j in range(26):\n",
    "    all_perceptrons_pocket.append([letter_list[j], dot_prod_predict(img_list[j], all_letters_perceptron[j])])\n",
    "all_perceptrons_pocket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e60e1f",
   "metadata": {},
   "source": [
    "# What accuracy does your set of perceptrons achieve on the TEST_SET? If the accuracy is less than 100%, which test images are misclassified, and how do they compare to the training images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842fcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TEST_SET is 73.07692307692307%\n",
      "List of test images that are misclassified:\n",
      "['A', 'D', 'H', 'J', 'S', 'T', 'U']\n",
      "Here is how those images appear as:\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ############  \n",
      "  ############  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "                \n",
      "                \n",
      "##########      \n",
      "##########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "##########      \n",
      "##########      \n",
      "                \n",
      "                \n",
      "  ########      \n",
      "  ########      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ############  \n",
      "  ############  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "                \n",
      "                \n",
      "      ########  \n",
      "      ########  \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "        ######  \n",
      "        ######  \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "######          \n",
      "######          \n",
      "  ######        \n",
      "  ######        \n",
      "      ######    \n",
      "      ######    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##            \n",
      "  ##            \n",
      "    ########    \n",
      "    ########    \n",
      "            ##  \n",
      "            ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "############    \n",
      "############    \n",
      "##  ####  ##    \n",
      "##  ####  ##    \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "  ##############\n",
      "  ##############\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "total_num, num_match = 26, 26\n",
    "img_misclassified = []\n",
    "bit_misclassified = []\n",
    "\n",
    "for i in range(len(all_perceptrons_pocket)):\n",
    "    if all_perceptrons_pocket[i][1] == -1:\n",
    "        num_match -= 1\n",
    "        img_misclassified.append(all_perceptrons_pocket[i][0])\n",
    "        bit_misclassified.append([TRAINING_SET[i][1], TEST_SET[i][1]])\n",
    "\n",
    "acc_rate = (num_match/total_num) * 100\n",
    "\n",
    "print(\"Accuracy of TEST_SET is {}%\".format(acc_rate))\n",
    "\n",
    "if len(img_misclassified) == 0:\n",
    "    print(\"All test images appear to be classified correctly.\")\n",
    "else:\n",
    "    print(\"List of test images that are misclassified:\")\n",
    "    print(img_misclassified)\n",
    "    print(\"Here is how those images appear as:\")\n",
    "    for j in range(len(bit_misclassified)):\n",
    "        show(bit_misclassified[j][0])\n",
    "        show(bit_misclassified[j][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f73583",
   "metadata": {},
   "source": [
    "## A: The TEST_SET images appears somewhat similar to TRAINING_SET because the images generated may have bits that are not properly aligned with each other due to the noise that causes some bits to be flipped to a random number (say, 1 into 0 and 0 into 1.) Furthermore, the font in the TEST_SET images appear to be shifted to the right and are thinner than the ones from the TRAINING_SET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2cd40",
   "metadata": {},
   "source": [
    "# 7. Convert the images in TRAINING_SET, TEST_SET, and MESSAGE into two-dimensional NumPy arrays of size (# examples × # features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "427b4f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 256)\n",
      "(26, 256)\n",
      "(31, 256)\n",
      "\n",
      "The image of the first letter: \n",
      "\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "The letter list:  ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J'\n",
      " 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Needed to do NumPy functions\n",
    "\n",
    "# Convert the input array dataset into 2-dimensional NumPy array\n",
    "def convert_2d_array(input_data):\n",
    "    if len(input_data) == 0:\n",
    "        return None\n",
    "    output_data = []\n",
    "    if len(input_data[0]) == 2:\n",
    "        for x, y in input_data:\n",
    "            output_data.append(np.array(y))\n",
    "    else:\n",
    "        for x in input_data:\n",
    "            output_data.append(np.array(x))\n",
    "    return np.array(output_data)\n",
    "\n",
    "def letter_list(input_data):\n",
    "    if len(input_data) == 0:\n",
    "        return None\n",
    "    output_data = []\n",
    "    if len(input_data[0]) == 2:\n",
    "        for x, y in input_data:\n",
    "            output_data.append(np.array(x))\n",
    "    return np.array(output_data)\n",
    "\n",
    "TRAINING_SET_2D = convert_2d_array(TRAINING_SET)\n",
    "TEST_SET_2D = convert_2d_array(TEST_SET)\n",
    "MESSAGE_2D = convert_2d_array(MESSAGE)\n",
    "\n",
    "# Verify that each 2-dimensional NumPy array contains the same number of letters and the\n",
    "# same number of bitmaps as the originla arrays.\n",
    "print(TRAINING_SET_2D.shape)\n",
    "print(TEST_SET_2D.shape)\n",
    "print(MESSAGE_2D.shape)\n",
    "\n",
    "print(\"\\nThe image of the first letter: \\n\")\n",
    "show(TRAINING_SET_2D[0])\n",
    "\n",
    "letter_list=letter_list(TRAINING_SET)\n",
    "print('The letter list: ',letter_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4bde7",
   "metadata": {},
   "source": [
    "# 8. Repeat experiment (5) using NumPy operations.\n",
    "## You will still need loops for iterating over the perceptrons to be trained and running each training epoch, but mathematical operations such Equations 1.1 and 1.4 in the textbook should be implemented using NumPy array operations and methods such as np.dot().\n",
    "## Maintain the weights for each perceptron as a single row in a matrix W of size (# perceptrons × # weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a067dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]] \n",
      "\n",
      "The size of the training set:  52\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SET_2D = convert_2d_array(TRAINING_SET)\n",
    "weights=get_random(len(TRAINING_SET_2D[0])) \n",
    "#print(weights)\n",
    "\n",
    "def np_predict(data, weight):\n",
    "    new_weight=np.delete(weight, 0)\n",
    "    dot_product = np.dot(data,new_weight)+weight[0]\n",
    "    if dot_product > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "print('The training set: \\n',TRAINING_SET_2D, \"\\n\")\n",
    "print('The size of the training set: ',len(TRAINING_SET_2D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b5bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', -1]\n"
     ]
    }
   ],
   "source": [
    "def np_training_perceptron_pocket(input_value, input_img, data_set=TRAINING_SET_2D):\n",
    "    \n",
    "    y_actual = [1 if letter == input_value else -1 for letter in letter_list]\n",
    "    weights = get_random(len(input_img)) # Generate the random values for the weights.\n",
    "    curr_w = weights # Assume the current set of weights is the best one possible.\n",
    "    learn_rate = 0.01\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        misclassifications = 0 # This value increments only if there is mismatch between predicted sign and actual sign.\n",
    "        for i in range(len(data_set)):\n",
    "            x_in = data_set[i] \n",
    "            y_pred = np_predict(x_in, weights) # Get the dot products for the input values and weights.\n",
    "            \n",
    "            # Create a new list of weights and initialize them to zero.\n",
    "            temp_weights = np.zeros(len(weights))\n",
    "            \n",
    "            # If the sign of the predicted value of y is negative while the actual\n",
    "            # value of y is positive (and vice versa), apply the update rules to the weights.\n",
    "            error_value = y_actual[i] - y_pred\n",
    "            if (y_actual[i] == 1 and y_pred < 0) or (y_actual[i] == -1 and y_pred > 0):\n",
    "                # Populate the values over into the temporary weights.\n",
    "                for j in range(len(temp_weights)):\n",
    "                    # Check if the bias perceptron exists.\n",
    "                    if j == 0:\n",
    "                        temp_weights[j] = learn_rate*error_value\n",
    "                    else:\n",
    "                        temp_weights[j] = learn_rate*error_value*x_in[j-1]\n",
    "                    misclassifications += 1\n",
    "                # Now apply the new values into the current list of weights.\n",
    "                for k in range(len(weights)):\n",
    "                    weights[k] = weights[k] + temp_weights[k]\n",
    "        if misclassifications != 0:\n",
    "            curr_w = weights\n",
    "        else:\n",
    "            break\n",
    "    return curr_w\n",
    "            \n",
    "weight = np_training_perceptron_pocket(letter_list[2], TRAINING_SET_2D[2])\n",
    "print([letter_list[2], np_predict(TRAINING_SET_2D[2], weight)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee05655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of perceptron for all letters:  26\n"
     ]
    }
   ],
   "source": [
    "def training_perceptron_all(dataset):\n",
    "    perceptron_list = []\n",
    "    letter_list = []\n",
    "    img_list = []\n",
    "    # Obtain the 26 letters and their corresponding images.\n",
    "    for i in range(26):\n",
    "        letter_list.append(dataset[i][0])\n",
    "        img_list.append(dataset[i][1])\n",
    "    # Now obtain the perceptrons by doing each letter and image.\n",
    "    for j in range(26):\n",
    "        weight = np_training_perceptron_pocket(letter_list[j],  TRAINING_SET_2D[j])\n",
    "        perceptron_list.append(weight)\n",
    "    return perceptron_list\n",
    "\n",
    "all_letters_perceptron = training_perceptron_all(TRAINING_SET)\n",
    "# Ensure that there are 26 weights for each letter in the perceptron obtained above.\n",
    "print(\"Length of perceptron for all letters: \", len(all_letters_perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2cdf6",
   "metadata": {},
   "source": [
    "# 9. Repeat experiment (6), implementing the prediction step using a single call to np.matmul() to multiply the matrix of weights W by the TEST_SET matrix you created in experiment (7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b6a7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.54  0.46 -5.8  ... 10.    1.    9.  ]\n",
      " [ 5.22 -0.78 -3.88 ...  1.    9.   10.  ]\n",
      " [-0.76 -0.76 -2.24 ...  4.    5.   10.  ]\n",
      " ...\n",
      " [ 2.54  1.54 -4.9  ...  2.    9.    4.  ]\n",
      " [ 0.32  2.32 -0.14 ...  8.    7.    1.  ]\n",
      " [-4.   -1.    1.72 ...  6.    2.    9.  ]]\n"
     ]
    }
   ],
   "source": [
    "x=np.array(all_letters_perceptron)\n",
    "weight_split=np.hsplit(x, [0,1]) # Extract the bias from the perceptron set.\n",
    "print(weight_split[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56767f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Bias matrix:  (26, 26)\n",
      "[[ -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26\n",
      "   -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26  -1.26\n",
      "   -1.26  -1.26  -1.26  -1.26  -1.26  -1.26]\n",
      " [-10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34\n",
      "  -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34 -10.34\n",
      "  -10.34 -10.34 -10.34 -10.34 -10.34 -10.34]\n",
      " [ -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42\n",
      "   -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42  -8.42\n",
      "   -8.42  -8.42  -8.42  -8.42  -8.42  -8.42]\n",
      " [ -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94\n",
      "   -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94  -2.94\n",
      "   -2.94  -2.94  -2.94  -2.94  -2.94  -2.94]\n",
      " [ -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32\n",
      "   -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32  -6.32\n",
      "   -6.32  -6.32  -6.32  -6.32  -6.32  -6.32]\n",
      " [  0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2\n",
      "    0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2    0.2\n",
      "    0.2    0.2    0.2    0.2    0.2    0.2 ]\n",
      " [ -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88\n",
      "   -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88  -2.88\n",
      "   -2.88  -2.88  -2.88  -2.88  -2.88  -2.88]\n",
      " [ -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28\n",
      "   -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28  -8.28\n",
      "   -8.28  -8.28  -8.28  -8.28  -8.28  -8.28]\n",
      " [ -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94\n",
      "   -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94  -0.94\n",
      "   -0.94  -0.94  -0.94  -0.94  -0.94  -0.94]\n",
      " [ -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5\n",
      "   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5\n",
      "   -9.5   -9.5   -9.5   -9.5   -9.5   -9.5 ]\n",
      " [ -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4\n",
      "   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4\n",
      "   -8.4   -8.4   -8.4   -8.4   -8.4   -8.4 ]\n",
      " [ -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26\n",
      "   -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26  -0.26\n",
      "   -0.26  -0.26  -0.26  -0.26  -0.26  -0.26]\n",
      " [ -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48\n",
      "   -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48  -6.48\n",
      "   -6.48  -6.48  -6.48  -6.48  -6.48  -6.48]\n",
      " [ -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68\n",
      "   -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68  -1.68\n",
      "   -1.68  -1.68  -1.68  -1.68  -1.68  -1.68]\n",
      " [ -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84\n",
      "   -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84  -2.84\n",
      "   -2.84  -2.84  -2.84  -2.84  -2.84  -2.84]\n",
      " [ -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24\n",
      "   -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24  -8.24\n",
      "   -8.24  -8.24  -8.24  -8.24  -8.24  -8.24]\n",
      " [ -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16\n",
      "   -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16  -6.16\n",
      "   -6.16  -6.16  -6.16  -6.16  -6.16  -6.16]\n",
      " [ -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3\n",
      "   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3\n",
      "   -7.3   -7.3   -7.3   -7.3   -7.3   -7.3 ]\n",
      " [ -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22\n",
      "   -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22  -0.22\n",
      "   -0.22  -0.22  -0.22  -0.22  -0.22  -0.22]\n",
      " [ -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4\n",
      "   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4\n",
      "   -2.4   -2.4   -2.4   -2.4   -2.4   -2.4 ]\n",
      " [ -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9\n",
      "   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9\n",
      "   -7.9   -7.9   -7.9   -7.9   -7.9   -7.9 ]\n",
      " [ -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92\n",
      "   -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92  -1.92\n",
      "   -1.92  -1.92  -1.92  -1.92  -1.92  -1.92]\n",
      " [ -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96\n",
      "   -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96  -5.96\n",
      "   -5.96  -5.96  -5.96  -5.96  -5.96  -5.96]\n",
      " [ -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92\n",
      "   -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92  -2.92\n",
      "   -2.92  -2.92  -2.92  -2.92  -2.92  -2.92]\n",
      " [ -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46\n",
      "   -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46  -5.46\n",
      "   -5.46  -5.46  -5.46  -5.46  -5.46  -5.46]\n",
      " [ -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.\n",
      "   -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.    -7.\n",
      "   -7.    -7.    -7.    -7.    -7.    -7.  ]]\n"
     ]
    }
   ],
   "source": [
    "new_bias=np.tile(weight_split[1], (1, 26))\n",
    "print(\"New Bias matrix: \", np.shape(new_bias))\n",
    "print(new_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39383b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  (26, 256)\n",
      "TEST_SET:  (26, 256)\n",
      "New TEST_SET:  (256, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"weights: \",np.shape(weight_split[2]))\n",
    "print(\"TEST_SET: \", np.shape(TEST_SET_2D))\n",
    "# Be sure to transpose the 2-dimensional TEST_SET array for matmul() to work properly.\n",
    "TEST_SET_TRANSPOSE=TEST_SET_2D.transpose()\n",
    "print(\"New TEST_SET: \",np.shape(TEST_SET_TRANSPOSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae7148cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the matrix:  (26, 26)\n",
      "[[-9.3000e+00 -2.2860e+01 -1.9340e+01 -2.3820e+01 -2.9340e+01 -6.8340e+01\n",
      "  -8.6600e+00 -7.5800e+00  3.4540e+01 -7.6100e+01 -2.9820e+01  5.2200e+00\n",
      "   7.6180e+01  1.7060e+01 -5.2200e+00 -5.1980e+01  2.8500e+01 -4.3500e+01\n",
      "  -2.9980e+01 -6.2000e-01  1.1140e+01  5.2500e+01  1.0730e+02  1.4660e+01\n",
      "   2.9780e+01  1.4580e+01]\n",
      " [-1.0580e+01  1.6700e+01  5.2200e+00  3.1000e+00 -1.8220e+01 -3.3300e+01\n",
      "  -9.8600e+00 -7.1400e+00  6.9800e+00 -2.0860e+01 -1.9100e+01 -2.5400e+00\n",
      "   5.9540e+01 -1.6620e+01  1.3100e+01 -6.1800e+00 -1.7300e+01 -9.7000e+00\n",
      "  -1.0740e+01 -3.7180e+01  4.2200e+00  4.8260e+01  6.1500e+01  3.5980e+01\n",
      "   8.1000e+00  2.2000e-01]\n",
      " [-4.7620e+01 -1.3100e+01 -2.4620e+01 -4.6900e+01 -1.3800e+00 -2.3500e+01\n",
      "  -1.3180e+01 -1.3540e+01  3.1900e+01 -8.6600e+00 -2.7740e+01 -9.5000e+00\n",
      "   9.6780e+01 -1.9780e+01 -6.3700e+01 -2.6300e+01 -7.9400e+00 -1.9020e+01\n",
      "   1.0780e+01  1.5340e+01  7.9400e+00  4.3140e+01  1.3426e+02  1.9060e+01\n",
      "   1.7780e+01  5.1980e+01]\n",
      " [ 5.1420e+01  3.1400e+00 -1.6100e+01  2.0420e+01 -2.7060e+01 -4.4420e+01\n",
      "   1.0300e+01  2.3540e+01 -9.7000e+00 -5.7340e+01 -1.2500e+01  9.6600e+00\n",
      "   7.8940e+01 -2.0000e-02  1.4220e+01 -1.4260e+01  4.1940e+01  2.6600e+00\n",
      "  -2.5100e+01 -8.3000e+00  4.3800e+00  6.1060e+01  1.1442e+02  6.3380e+01\n",
      "   4.9140e+01  1.4060e+01]\n",
      " [-3.9880e+01  3.1800e+01 -5.5600e+00 -2.1800e+01  3.9560e+01  1.0480e+01\n",
      "   6.1200e+00 -8.1600e+00  7.2000e+00 -1.5680e+01  4.4560e+01  1.6520e+01\n",
      "   7.9360e+01 -1.2720e+01 -3.8760e+01  1.8400e+00  2.1200e+01  1.7880e+01\n",
      "   1.5200e+01  2.4000e+01 -5.4400e+00  2.6280e+01  9.6840e+01  1.5920e+01\n",
      "   1.5560e+01  1.2520e+01]\n",
      " [-1.5000e+01  1.1920e+01 -1.7360e+01 -1.6520e+01  2.9960e+01  3.5200e+00\n",
      "  -2.3480e+01 -4.0000e-01  1.2000e+00 -4.2080e+01  3.8000e+01  1.8000e+01\n",
      "   1.0096e+02  1.9200e+00 -3.6240e+01 -5.4000e+00 -2.1480e+01  1.6800e+00\n",
      "  -3.0000e+00  2.8800e+01  2.2000e+00  3.6600e+01  9.1200e+01  5.6000e-01\n",
      "  -5.5200e+00  9.1200e+00]\n",
      " [ 1.6400e+01 -1.0520e+01 -1.0280e+01 -2.7000e+01 -1.8400e+00 -5.1280e+01\n",
      "   9.6400e+00 -7.0800e+00  2.6760e+01 -5.4640e+01 -1.8440e+01  9.3600e+00\n",
      "   7.0440e+01 -6.2000e+00 -2.6040e+01 -5.4480e+01  2.8840e+01 -4.1120e+01\n",
      "   1.1760e+01  7.4400e+00 -7.6800e+00  5.1960e+01  1.0824e+02  2.7440e+01\n",
      "   2.4520e+01  2.5640e+01]\n",
      " [-3.2080e+01 -2.0480e+01 -2.8000e+01 -3.2680e+01 -3.6200e+01 -4.1720e+01\n",
      "  -2.4800e+01  4.6400e+00  2.1200e+00 -2.5400e+01 -4.6440e+01 -3.9120e+01\n",
      "   7.9080e+01 -8.4400e+00 -2.6520e+01 -3.6200e+01 -3.9600e+00 -4.4600e+01\n",
      "  -7.1200e+00  3.3080e+01  9.3200e+00  6.8520e+01  9.2720e+01  7.2000e+00\n",
      "   1.8400e+01  1.1880e+01]\n",
      " [-4.4140e+01  4.6600e+00 -1.7220e+01 -9.4000e-01  4.5800e+00 -3.7020e+01\n",
      "  -1.1940e+01 -6.0860e+01  2.7820e+01 -2.8820e+01  1.1740e+01  2.1300e+01\n",
      "   6.4780e+01 -5.8820e+01 -2.8540e+01 -3.0140e+01 -1.3860e+01 -5.1000e+00\n",
      "  -2.2740e+01  2.7460e+01 -4.2620e+01  6.4940e+01  9.0940e+01  4.2820e+01\n",
      "   4.7740e+01  3.0500e+01]\n",
      " [-5.7020e+01 -1.7260e+01 -3.5540e+01 -2.6980e+01 -1.1940e+01 -2.8020e+01\n",
      "  -3.3980e+01 -6.9420e+01  5.8600e+00 -4.4500e+01 -1.5700e+01 -8.4600e+00\n",
      "   6.8860e+01 -5.5300e+01 -4.5540e+01 -2.8780e+01  4.9000e+00 -2.5740e+01\n",
      "  -4.2700e+01  3.6900e+01 -6.2340e+01  1.1740e+01  7.7620e+01  3.8180e+01\n",
      "   2.3900e+01  6.2780e+01]\n",
      " [-3.1720e+01 -8.2400e+00 -2.6200e+01 -7.0400e+00  1.6000e+00 -2.1120e+01\n",
      "  -3.2560e+01  5.8000e+00 -2.4800e+00 -2.5720e+01  3.7080e+01  2.4000e+00\n",
      "   1.4804e+02  1.2400e+00 -1.7760e+01 -2.2760e+01  1.0800e+01 -7.8000e+00\n",
      "  -7.1200e+00  1.9960e+01 -7.0400e+00  1.1624e+02  1.3044e+02  3.4200e+01\n",
      "   3.8160e+01  3.0760e+01]\n",
      " [ 2.0200e+00  8.6000e-01 -1.4780e+01 -4.5000e+00 -2.1100e+01 -4.3420e+01\n",
      "   6.6200e+00 -2.6500e+01  2.3000e+00 -5.8740e+01  1.3620e+01  9.6600e+00\n",
      "   9.1140e+01 -3.6020e+01 -1.0540e+01 -3.9740e+01  1.9980e+01 -2.6420e+01\n",
      "  -1.8540e+01 -1.6860e+01 -8.3800e+00  1.0366e+02  9.7820e+01  1.8820e+01\n",
      "   4.6000e-01 -3.4180e+01]\n",
      " [-2.2080e+01 -2.3680e+01 -4.8880e+01  2.0800e+00 -3.6640e+01 -5.8360e+01\n",
      "  -3.7320e+01 -4.1400e+01  2.8760e+01 -3.5000e+01 -1.3160e+01 -3.6000e+00\n",
      "   5.7560e+01 -6.9600e+00 -1.1680e+01 -4.0600e+01  3.0800e+00 -2.3720e+01\n",
      "  -4.6960e+01  2.5760e+01 -3.3920e+01  5.3520e+01  4.2160e+01  1.8280e+01\n",
      "  -9.4000e+00  1.4800e+01]\n",
      " [-2.4240e+01  3.2000e+00 -2.3680e+01 -5.6000e+00 -1.3240e+01 -3.7880e+01\n",
      "  -1.8480e+01  7.3200e+00  2.8200e+01 -2.3440e+01 -2.0880e+01 -7.7200e+00\n",
      "   1.0184e+02  1.7600e+00 -7.2000e+00 -2.0960e+01  5.6000e+00 -2.7160e+01\n",
      "   1.6800e+00  1.9400e+01  2.5040e+01  5.0280e+01  1.1540e+02  1.1480e+01\n",
      "   2.6440e+01  5.4360e+01]\n",
      " [-4.4760e+01 -1.8400e+00  9.2000e-01 -5.6400e+00 -1.3040e+01 -2.9200e+01\n",
      "  -8.4000e-01 -1.2840e+01  1.6840e+01 -1.6240e+01  1.7240e+01 -2.6800e+00\n",
      "   1.0876e+02 -7.2000e-01  5.4400e+00 -3.0360e+01  3.9200e+01 -3.3320e+01\n",
      "  -1.1200e+01  3.1520e+01  3.7400e+01  7.0200e+01  1.0648e+02  3.8360e+01\n",
      "   2.8400e+01  3.3960e+01]\n",
      " [-4.5960e+01 -1.4600e+01 -6.0800e+00 -1.4520e+01 -1.9800e+01 -2.1360e+01\n",
      "  -2.4080e+01 -2.1040e+01  6.9200e+00 -1.3720e+01  3.1200e+00 -4.3600e+00\n",
      "   8.4840e+01 -2.4920e+01 -1.8400e+00 -2.0800e+00  1.7560e+01  1.1400e+01\n",
      "  -2.0520e+01 -9.4000e+00 -2.3840e+01  4.9360e+01  1.0240e+02  1.9200e+01\n",
      "   1.7240e+01  6.6400e+00]\n",
      " [-3.0760e+01 -1.3720e+01 -1.4400e+00 -2.0400e+01 -2.5600e+01 -5.8960e+01\n",
      "   4.9200e+00 -9.2000e-01  5.5200e+00 -4.5440e+01 -4.4920e+01 -1.5320e+01\n",
      "   6.4880e+01  8.9200e+00  1.4920e+01 -5.8640e+01  4.5560e+01 -4.4320e+01\n",
      "  -2.0000e-01  5.3760e+01  3.4800e+01  6.8360e+01  9.7360e+01  1.6560e+01\n",
      "   3.5520e+01  3.7960e+01]\n",
      " [ 1.2540e+01 -2.1500e+01 -1.1620e+01 -7.4000e-01 -1.6540e+01 -3.7060e+01\n",
      "  -1.1000e+00  2.5420e+01  1.9540e+01 -1.9000e+00  6.8200e+00 -1.2020e+01\n",
      "   1.2134e+02  2.0780e+01  1.3540e+01 -2.4660e+01  6.9460e+01 -7.5000e+00\n",
      "  -7.9400e+00 -1.7860e+01  2.5400e+00  9.3980e+01  1.0818e+02  4.2420e+01\n",
      "   7.6200e+00  5.5420e+01]\n",
      " [-3.0200e+00 -2.5740e+01 -5.0340e+01 -1.2340e+01 -9.8600e+00 -2.4700e+01\n",
      "  -2.5660e+01 -4.8780e+01  6.1800e+00 -2.7420e+01  4.3500e+01  6.9400e+00\n",
      "   9.9300e+01 -2.0020e+01 -4.3580e+01 -2.3380e+01  2.4500e+01  1.1800e+00\n",
      "  -3.9300e+01  3.7780e+01 -5.5300e+01  7.1820e+01  9.3700e+01  2.6860e+01\n",
      "   6.6200e+00  2.4380e+01]\n",
      " [-4.2360e+01 -6.5560e+01 -9.2480e+01 -3.4520e+01 -3.7240e+01 -4.5760e+01\n",
      "  -7.1360e+01 -4.6440e+01  3.0440e+01 -3.3240e+01  3.4160e+01 -2.5760e+01\n",
      "   1.0536e+02 -3.1920e+01 -6.1920e+01 -7.4360e+01 -8.4800e+00 -5.6440e+01\n",
      "  -7.8720e+01  1.7240e+01 -3.9360e+01  3.9840e+01  8.6520e+01  1.1200e+00\n",
      "  -5.6400e+00 -4.3200e+00]\n",
      " [-2.2020e+01 -2.1260e+01 -3.9620e+01 -3.0020e+01 -2.2980e+01 -5.1820e+01\n",
      "  -2.9260e+01 -1.6580e+01  3.4220e+01 -5.5800e+00 -2.1020e+01 -1.8860e+01\n",
      "   1.0254e+02  3.4000e-01 -3.2060e+01 -3.9940e+01  4.6000e-01 -4.0300e+01\n",
      "  -6.6000e-01  1.3860e+01  7.2200e+00  2.8780e+01  1.0630e+02  8.6000e-01\n",
      "   4.8200e+00  4.3780e+01]\n",
      " [-4.0000e+01 -2.8400e+01 -5.2360e+01 -4.2440e+01 -3.0000e+01 -3.5640e+01\n",
      "  -3.8680e+01 -3.6360e+01  2.0000e+01 -5.9680e+01  1.8400e+00 -2.4760e+01\n",
      "   6.0680e+01 -2.6840e+01 -4.6080e+01 -2.0800e+01  2.2560e+01 -7.0800e+00\n",
      "  -4.3960e+01  1.2000e+01 -4.6720e+01  4.6760e+01  8.7840e+01  4.6600e+01\n",
      "   4.1000e+01  9.1600e+00]\n",
      " [-3.4320e+01 -3.0720e+01 -7.1560e+01 -1.1320e+01 -2.9800e+01 -4.0720e+01\n",
      "  -7.7360e+01 -2.8680e+01  1.6480e+01 -6.6400e+01  9.3200e+00 -2.2160e+01\n",
      "   4.6200e+01 -3.2040e+01 -3.0440e+01 -3.5560e+01  2.0160e+01 -1.7120e+01\n",
      "  -6.0560e+01 -2.6400e+00 -3.3160e+01  4.5840e+01  6.0080e+01  9.0000e+00\n",
      "   1.0960e+01  1.0800e+01]\n",
      " [-4.0200e+01 -3.1240e+01 -3.9880e+01 -3.1520e+01 -2.1600e+01 -4.1720e+01\n",
      "  -1.5960e+01 -1.1200e+01  1.6720e+01 -2.6280e+01  2.5200e+00 -8.9600e+00\n",
      "   3.6960e+01 -1.5680e+01 -2.7160e+01 -4.2200e+01  5.1600e+00 -1.1880e+01\n",
      "  -3.3240e+01 -9.6000e+00 -2.3360e+01  4.1200e+00  7.1480e+01 -2.2280e+01\n",
      "   2.0000e+00 -2.6960e+01]\n",
      " [-7.5220e+01 -2.1300e+01 -5.0500e+01 -3.1460e+01 -2.7000e+00 -9.9400e+00\n",
      "  -5.0780e+01 -4.8860e+01  6.9800e+00 -3.1940e+01  1.8660e+01 -6.3800e+00\n",
      "   7.3820e+01 -4.4140e+01 -6.6900e+01 -2.7500e+01 -2.9020e+01 -1.7620e+01\n",
      "  -3.3100e+01  3.2540e+01 -3.3340e+01  2.2940e+01  8.0740e+01  5.1700e+01\n",
      "   3.9940e+01  4.7900e+01]\n",
      " [-3.1160e+01 -5.3200e+00 -3.4080e+01  4.1600e+00 -3.1200e+00 -4.3280e+01\n",
      "  -3.1720e+01 -2.0040e+01  3.5560e+01 -1.6840e+01  2.2400e+00  7.4400e+00\n",
      "   1.2732e+02  2.8400e+01 -2.2000e+01 -4.1880e+01  1.1360e+01 -2.8800e+01\n",
      "  -3.1520e+01  2.5480e+01  2.7360e+01  8.1200e+01  1.0020e+02  1.2320e+01\n",
      "   7.8800e+00  5.7520e+01]]\n"
     ]
    }
   ],
   "source": [
    "matmul=np.matmul(weight_split[2], TEST_SET_TRANSPOSE) + new_bias # Don't forget to add the bias back.\n",
    "prediction=matmul\n",
    "print(\"The size of the matrix: \", np.shape(prediction))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965901a",
   "metadata": {},
   "source": [
    "# 10. Verify that this version computes the same weights and achieves the same accuracy as the experiments in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb84684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', -1],\n",
       " ['B', 1],\n",
       " ['C', -1],\n",
       " ['D', 1],\n",
       " ['E', 1],\n",
       " ['F', 1],\n",
       " ['G', 1],\n",
       " ['H', 1],\n",
       " ['I', 1],\n",
       " ['J', -1],\n",
       " ['K', 1],\n",
       " ['L', 1],\n",
       " ['M', 1],\n",
       " ['N', 1],\n",
       " ['O', 1],\n",
       " ['P', -1],\n",
       " ['Q', 1],\n",
       " ['R', -1],\n",
       " ['S', -1],\n",
       " ['T', 1],\n",
       " ['U', 1],\n",
       " ['V', 1],\n",
       " ['W', 1],\n",
       " ['X', -1],\n",
       " ['Y', 1],\n",
       " ['Z', 1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result=[]\n",
    "result=[]\n",
    "for i in range(0, 26):\n",
    "    # Compare each perceptron weight with its corresponding letter.\n",
    "    # If the weight is larger than 0, then append 1. Otherwise, append -1.\n",
    "    if prediction[i][i] > 0:\n",
    "        prediction_result.append(1)\n",
    "    else:\n",
    "        prediction_result.append(-1)\n",
    "for j in range(26):\n",
    "    result.append([letter_list[j], prediction_result[j]])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0bd3411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TEST_SET is 73.07692307692307%\n",
      "List of test images that are misclassified:\n",
      "['A', 'C', 'J', 'P', 'R', 'S', 'X']\n",
      "Here is how those images appear as:\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ############  \n",
      "  ############  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "                \n",
      "                \n",
      "    ########    \n",
      "    ########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "      ########  \n",
      "      ########  \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "        ######  \n",
      "        ######  \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "          ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "  ##      ##    \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "########        \n",
      "                \n",
      "                \n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "                \n",
      "                \n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ######    ##  \n",
      "  ######    ##  \n",
      "                \n",
      "                \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "######          \n",
      "######          \n",
      "  ######        \n",
      "  ######        \n",
      "      ######    \n",
      "      ######    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##            \n",
      "  ##            \n",
      "    ########    \n",
      "    ########    \n",
      "            ##  \n",
      "            ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "total_num, num_match = 26, 26\n",
    "img_misclassified = []\n",
    "bit_misclassified = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    if result[i][1] == -1:\n",
    "        num_match -= 1\n",
    "        img_misclassified.append(result[i][0])\n",
    "        bit_misclassified.append([TRAINING_SET[i][1], TEST_SET[i][1]])\n",
    "\n",
    "acc_rate = (num_match/total_num) * 100\n",
    "\n",
    "print(\"Accuracy of TEST_SET is {}%\".format(acc_rate))\n",
    "\n",
    "if len(img_misclassified) == 0:\n",
    "    print(\"All test images appear to be classified correctly.\")\n",
    "else:\n",
    "    print(\"List of test images that are misclassified:\")\n",
    "    print(img_misclassified)\n",
    "    print(\"Here is how those images appear as:\")\n",
    "    for j in range(len(bit_misclassified)):\n",
    "        show(bit_misclassified[j][0])\n",
    "        show(bit_misclassified[j][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f504a7",
   "metadata": {},
   "source": [
    "## A: The accuracy obtained for Part 2 matches the accuracy for Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb4bb4",
   "metadata": {},
   "source": [
    "# 11. Predict the letters for MESSAGE. How well do the perceptrons perform? Does any image cause more than one perceptron to return +1? Does any image fail to return +1 for at least one perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23baf31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 [['C', 1], ['D', 1], ['E', 1], ['F', 1], ['G', 1], ['H', 1], ['I', 1], ['J', 1], ['K', 1], ['M', 1], ['N', 1], ['O', 1], ['Q', 1], ['S', 1], ['T', 1], ['U', 1], ['V', 1], ['Y', 1], ['Z', 1], ['B', 1], ['D', 1], ['F', 1], ['H', 1], ['K', 1], ['N', 1], ['Q', 1], ['R', 1], ['F', 1], ['E', 1], ['F', 1], ['I', 1], ['J', 1], ['M', 1], ['R', 1], ['T', 1], ['U', 1], ['X', 1], ['Y', 1], ['H', 1], ['J', 1], ['O', 1], ['V', 1], ['C', 1], ['E', 1], ['F', 1], ['G', 1], ['I', 1], ['K', 1], ['S', 1], ['Y', 1], ['Z', 1], ['B', 1], ['R', 1], ['A', 1], ['B', 1], ['C', 1], ['D', 1], ['E', 1], ['F', 1], ['G', 1], ['H', 1], ['I', 1], ['J', 1], ['K', 1], ['L', 1], ['M', 1], ['N', 1], ['O', 1], ['P', 1], ['Q', 1], ['R', 1], ['S', 1], ['T', 1], ['U', 1], ['V', 1], ['W', 1], ['Y', 1], ['Z', 1], ['C', 1], ['E', 1], ['F', 1], ['I', 1], ['K', 1], ['L', 1], ['M', 1], ['R', 1], ['S', 1], ['T', 1], ['V', 1], ['Y', 1], ['Z', 1], ['M', 1], ['N', 1], ['O', 1], ['U', 1], ['C', 1], ['G', 1], ['O', 1], ['B', 1], ['C', 1], ['D', 1], ['H', 1], ['L', 1], ['O', 1], ['Q', 1], ['V', 1], ['W', 1], ['F', 1], ['G', 1], ['I', 1], ['Q', 1], ['S', 1], ['T', 1], ['X', 1], ['Y', 1], ['A', 1], ['B', 1], ['C', 1], ['D', 1], ['E', 1], ['F', 1], ['G', 1], ['H', 1], ['I', 1], ['J', 1], ['K', 1], ['M', 1], ['N', 1], ['O', 1], ['P', 1], ['Q', 1], ['R', 1], ['S', 1], ['T', 1], ['U', 1], ['V', 1], ['W', 1], ['Y', 1], ['Z', 1], ['A', 1], ['D', 1], ['M', 1], ['O', 1], ['R', 1], ['U', 1], ['V', 1], ['B', 1], ['D', 1], ['E', 1], ['F', 1], ['I', 1], ['M', 1], ['P', 1], ['R', 1], ['S', 1], ['V', 1], ['W', 1], ['X', 1], ['B', 1], ['D', 1], ['I', 1], ['M', 1], ['O', 1], ['R', 1], ['C', 1], ['E', 1], ['J', 1], ['N', 1], ['S', 1], ['U', 1], ['X', 1], ['Z', 1], ['O', 1], ['U', 1], ['Y', 1], ['B', 1], ['D', 1], ['E', 1], ['F', 1], ['G', 1], ['H', 1], ['J', 1], ['K', 1], ['M', 1], ['N', 1], ['O', 1], ['P', 1], ['Q', 1], ['R', 1], ['U', 1], ['W', 1], ['Y', 1], ['Z', 1], ['O', 1], ['U', 1], ['A', 1], ['B', 1], ['C', 1], ['D', 1], ['E', 1], ['F', 1], ['G', 1], ['I', 1], ['K', 1], ['L', 1], ['M', 1], ['N', 1], ['O', 1], ['P', 1], ['R', 1], ['S', 1], ['T', 1], ['V', 1], ['W', 1], ['X', 1], ['Y', 1], ['Z', 1], ['C', 1], ['L', 1], ['B', 1], ['D', 1], ['E', 1], ['L', 1], ['O', 1], ['P', 1], ['X', 1]]\n",
      "\n",
      "25 [['C', 1], ['B', 1], ['F', 1], ['E', 1], ['H', 1], ['C', 1], ['B', 1], ['A', 1], ['C', 1], ['M', 1], ['C', 1], ['B', 1], ['F', 1], ['A', 1], ['A', 1], ['B', 1], ['B', 1], ['C', 1], ['O', 1], ['B', 1], ['O', 1], ['U', 1], ['A', 1], ['C', 1], ['B', 1]]\n"
     ]
    }
   ],
   "source": [
    "# Scenario1: all perceptrons that perform +1\n",
    "message_predict = []\n",
    "for i in range(len(MESSAGE)):\n",
    "    for j in range(len(all_letters_perceptron)):\n",
    "        n = np_predict(MESSAGE[i], all_letters_perceptron[j])\n",
    "        letter = letter_list[j]\n",
    "        \n",
    "        # will just return the letters that are +1\n",
    "        if n == 1:\n",
    "            message_predict.append([letter, n])\n",
    "            \n",
    "print(len(message_predict), message_predict)\n",
    "print()\n",
    "\n",
    "# Scenario2: the first (ideally correct) perceptron that performs +1 \n",
    "message_predict_two = []\n",
    "for i in range(len(MESSAGE)):\n",
    "    for j in range(len(all_letters_perceptron)):\n",
    "        n = np_predict(MESSAGE[i], all_letters_perceptron[j])\n",
    "        letter = letter_list[j]\n",
    "        \n",
    "        # will just return the first letter that is +1 and will exit out once it finds a match\n",
    "        if n == 1:\n",
    "            message_predict_two.append([letter, n])\n",
    "            break\n",
    "            \n",
    "print(len(message_predict_two), message_predict_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80dcd7e",
   "metadata": {},
   "source": [
    "## A: Yes, sometimes more than one perceptron returns +1 like in Scenario 1\n",
    "## Also some images of MESSAGE fails to return a single +1 perceptron as exemplified by Scenario 2 where there are supposed to be 31 letters in the message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202de873",
   "metadata": {},
   "source": [
    "# 12. Now examine the images in MESSAGE. What does it say in English? (Note that there are no spaces between words.) Why do you suppose this was chosen as the message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10fb805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ##############\n",
      "  ############# \n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "   #    ##    # \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "  #             \n",
      "                \n",
      "  ##  #     ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ############  \n",
      "  ############  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "# ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  #          #  \n",
      "                \n",
      "                \n",
      "##############  \n",
      "  #  #    ####  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####          \n",
      "  ####      ##  \n",
      "  #### #  ####  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "         #      \n",
      "                \n",
      "##############  \n",
      "  ####    ####  \n",
      "   ###      ##  \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "### ####        \n",
      "                \n",
      "                \n",
      "               #\n",
      "     #          \n",
      "                \n",
      "                \n",
      "    ### ####    \n",
      "      #####     \n",
      "      ####      \n",
      "      ####      \n",
      "      ####      \n",
      "     #####      \n",
      "      ####      \n",
      "      ####      \n",
      "      ####      \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "  #             \n",
      "                \n",
      "####    ####    \n",
      "####    ####    \n",
      "###     ####    \n",
      "####    ## #    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ###         \n",
      "                \n",
      "       #        \n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ###     ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##   #  \n",
      "    ##  ##      \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##### ######  \n",
      "  ############  \n",
      "       #        \n",
      "                \n",
      "# ##########    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  # ##    ####  \n",
      "  ####    ####  \n",
      "  ###     ####  \n",
      "  ####    ####  \n",
      "############    \n",
      "############   #\n",
      "                \n",
      "                \n",
      "  ########## ## \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "#####     ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "                \n",
      "       #        \n",
      "                \n",
      "                \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ## #\n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      ##  ##    \n",
      "#     ##  ##    \n",
      "     #      ##  \n",
      "    ##      ##  \n",
      "  ##          ##\n",
      "  ##    #     ##\n",
      "                \n",
      "                \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "   #####        \n",
      "    ####        \n",
      "    ####     #  \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########  #   \n",
      "  # ######      \n",
      "                \n",
      "                \n",
      "####      ####  \n",
      "######    ####  \n",
      "########  ####  \n",
      "##############  \n",
      "# ##  ########  \n",
      "# ##    ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####       ###  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "                \n",
      "              # \n",
      "                \n",
      "                \n",
      "    ########    \n",
      "    ########    \n",
      "  ## #    ####  \n",
      "  ####    ####  \n",
      "###             \n",
      "####            \n",
      "###             \n",
      "####            \n",
      "####    ######  \n",
      "####    ######  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ##########  \n",
      "    ##########  \n",
      "              # \n",
      "                \n",
      "####      ####  \n",
      "####      #### #\n",
      "####      ####  \n",
      "## #      ####  \n",
      "####      ####  \n",
      "#####     ####  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "##############  \n",
      "##############  \n",
      "######  ######  \n",
      "######  ######  \n",
      "####      ## #  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "    ########    \n",
      "      ####      \n",
      "      ####      \n",
      "      ####      \n",
      "   #  ####      \n",
      "      ####      \n",
      "      ###       \n",
      "      ####      \n",
      "      ####      \n",
      "    ########    \n",
      "                \n",
      "         #   #  \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "  ##############\n",
      "  ##############\n",
      "  ##        ##  \n",
      "  ###       ##  \n",
      "          ##    \n",
      "          ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      #         \n",
      "      ##        \n",
      " #  ##   #    ##\n",
      "    ##        ##\n",
      "  ##############\n",
      "  ##############\n",
      "                \n",
      "                \n",
      "      ##        \n",
      "    ######      \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "##############  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "      #     #   \n",
      "#               \n",
      "                \n",
      "              # \n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ######## #  \n",
      "#   ########    \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ######    ##  \n",
      "  ######    ##  \n",
      "  #             \n",
      "              # \n",
      "  ########      \n",
      "  ########      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "    ##    #  #  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "   #######      \n",
      "  ########      \n",
      "            #   \n",
      "                \n",
      "  ##########    \n",
      "####      ####  \n",
      "####      ## #  \n",
      "  ####          \n",
      "    ######      \n",
      "        ####    \n",
      "          ####  \n",
      "####      # ##  \n",
      "##### #   ####  \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "      ########  \n",
      "      ########  \n",
      "#       ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####  # \n",
      "        ####    \n",
      "        ####    \n",
      "####    ####    \n",
      "## # #  ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "#####   ###     \n",
      "####    ####    \n",
      "####    ####    \n",
      "####  # ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "         #      \n",
      "                \n",
      "####    # ####  \n",
      "####      ####  \n",
      "######  ######  \n",
      "######  ######  \n",
      "############### \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "####  ##  #### #\n",
      "####  ##  ####  \n",
      "# ##      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "############ #  \n",
      "  ####    ## #  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  #####         \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "                \n",
      "                \n",
      "               #\n",
      "                \n",
      "                \n",
      "                \n",
      "  ##########    \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ## ## \n",
      "####      ####  \n",
      "####      ###   \n",
      "####  ##  ####  \n",
      "####  ########  \n",
      "  ##########  # \n",
      "        ####    \n",
      "        ######  \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "####      ####  \n",
      "####  #   ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ##########    \n",
      "                \n",
      "           #    \n",
      "                \n",
      "                \n",
      "          #     \n",
      "       #        \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####       #\n",
      "    ####      # \n",
      "    ####        \n",
      "    ####        \n",
      "    ####     #  \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "              # \n",
      "    ## #####  # \n",
      "  ####    ##### \n",
      "####        ##  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####        ##  \n",
      "  ####    ####  \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "               #\n",
      "######    ####  \n",
      "  ####    ####  \n",
      "  ####   #####  \n",
      "  ####  ####    \n",
      "  ########      \n",
      "  ########      \n",
      "  #### ## ##    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "             #  \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "                \n",
      "########        \n",
      "########        \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####      ## #\n",
      "  ###  #    ##  \n",
      "   ###    ####  \n",
      "  ####    ####  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ### ####    \n",
      "      ####      \n",
      "      ####      \n",
      "      ####      \n",
      "      # ##      \n",
      "    ########    \n",
      "          #     \n",
      "                \n",
      " #              \n",
      "                \n",
      "                \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(MESSAGE)):\n",
    "    show(MESSAGE[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339697d8",
   "metadata": {},
   "source": [
    "## THEFIVEBOXINGWIZARDSJUMPQUICKLY\n",
    "## This was used because it is the shortest sentence that contains all 26 letters of the English alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759c68f",
   "metadata": {},
   "source": [
    "# 13. Modify your training loop to record the accuracy achieved in each epoch. Then plot a learning curve showing the error rate as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "849e67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_training_perceptron_pocket_acc_epoch(input_value, input_img, data_set=TRAINING_SET_2D):\n",
    "    y_actual = [1 if letter == input_value else -1 for letter in letter_list]\n",
    "    weights = get_random(len(input_img)) # Generate the random values for the weights.\n",
    "    curr_w = weights # Assume the current set of weights is the best one possible.\n",
    "    learn_rate = 0.01\n",
    "    \n",
    "    #added containers\n",
    "    epochs = 0\n",
    "    corrects = 0\n",
    "    err_list = []\n",
    "    result_list = []\n",
    "    \n",
    "    for n in range(0, 100):\n",
    "        misclassifications = 0 # This value increments only if there is mismatch between predicted sign and actual sign.\n",
    "        for i in range(len(data_set)):\n",
    "            x_in = data_set[i] \n",
    "            y_pred = np_predict(x_in, weights) # Get the dot products for the input values and weights.\n",
    "            \n",
    "            # Create a new list of weights and initialize them to zero.\n",
    "            temp_weights = np.zeros(len(weights))\n",
    "            \n",
    "            # If the sign of the predicted value of y is negative while the actual\n",
    "            # value of y is positive (and vice versa), apply the update rules to the weights.\n",
    "            error_value = y_actual[i] - y_pred\n",
    "            if (y_actual[i] == 1 and y_pred < 0) or (y_actual[i] == -1 and y_pred > 0):\n",
    "                # Populate the values over into the temporary weights.\n",
    "                for j in range(len(temp_weights)):\n",
    "                    if j == 0:\n",
    "                        temp_weights[j] = learn_rate*error_value\n",
    "                    else:\n",
    "                        temp_weights[j] = learn_rate*error_value*x_in[j-1]\n",
    "                    misclassifications += 1\n",
    "                # Now apply the new values into the current list of weights.\n",
    "                for k in range(len(weights)):\n",
    "                    weights[k] = weights[k] + temp_weights[k]\n",
    "                \n",
    "            #find how many correct predictions there are\n",
    "            #total correct predictions/total observations(data_set=52)\n",
    "            else:\n",
    "                corrects += 1\n",
    "        \n",
    "        #number of correct predictions at every epoch\n",
    "        epochs += 1\n",
    "        result_list.append([\"epochs=\",epochs])\n",
    "        print(\"num epochs\", epochs)\n",
    "        print(\"accuracy rate\", (corrects/52) * 100)\n",
    "        \n",
    "        # For the error rate, be sure to subtract 1 from the accuracy rate obtained.\n",
    "        err_tup = (epochs, (1-(corrects/52)))\n",
    "        err_list.append(err_tup)\n",
    "        \n",
    "        #reset corrects for the next epoch cycle\n",
    "        corrects = 0\n",
    "        \n",
    "        if misclassifications != 0:\n",
    "            curr_w = weights\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    print(\"error rate list:\", err_list)\n",
    "    return err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "265ede5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num epochs 1\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 2\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 3\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 4\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 5\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 6\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 7\n",
      "accuracy rate 3.8461538461538463\n",
      "num epochs 8\n",
      "accuracy rate 30.76923076923077\n",
      "num epochs 9\n",
      "accuracy rate 48.07692307692308\n",
      "num epochs 10\n",
      "accuracy rate 55.769230769230774\n",
      "num epochs 11\n",
      "accuracy rate 61.53846153846154\n",
      "num epochs 12\n",
      "accuracy rate 75.0\n",
      "num epochs 13\n",
      "accuracy rate 82.6923076923077\n",
      "num epochs 14\n",
      "accuracy rate 86.53846153846155\n",
      "num epochs 15\n",
      "accuracy rate 88.46153846153845\n",
      "num epochs 16\n",
      "accuracy rate 88.46153846153845\n",
      "num epochs 17\n",
      "accuracy rate 90.38461538461539\n",
      "num epochs 18\n",
      "accuracy rate 94.23076923076923\n",
      "num epochs 19\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 20\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 21\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 22\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 23\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 24\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 25\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 26\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 27\n",
      "accuracy rate 94.23076923076923\n",
      "num epochs 28\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 29\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 30\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 31\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 32\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 33\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 34\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 35\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 36\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 37\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 38\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 39\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 40\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 41\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 42\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 43\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 44\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 45\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 46\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 47\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 48\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 49\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 50\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 51\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 52\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 53\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 54\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 55\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 56\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 57\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 58\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 59\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 60\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 61\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 62\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 63\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 64\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 65\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 66\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 67\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 68\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 69\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 70\n",
      "accuracy rate 98.07692307692307\n",
      "num epochs 71\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 72\n",
      "accuracy rate 96.15384615384616\n",
      "num epochs 73\n",
      "accuracy rate 100.0\n",
      "\n",
      "error rate list: [(1, 0.9615384615384616), (2, 0.9615384615384616), (3, 0.9615384615384616), (4, 0.9615384615384616), (5, 0.9615384615384616), (6, 0.9615384615384616), (7, 0.9615384615384616), (8, 0.6923076923076923), (9, 0.5192307692307692), (10, 0.4423076923076923), (11, 0.3846153846153846), (12, 0.25), (13, 0.17307692307692313), (14, 0.13461538461538458), (15, 0.11538461538461542), (16, 0.11538461538461542), (17, 0.09615384615384615), (18, 0.05769230769230771), (19, 0.038461538461538436), (20, 0.038461538461538436), (21, 0.038461538461538436), (22, 0.038461538461538436), (23, 0.038461538461538436), (24, 0.038461538461538436), (25, 0.038461538461538436), (26, 0.038461538461538436), (27, 0.05769230769230771), (28, 0.038461538461538436), (29, 0.019230769230769273), (30, 0.038461538461538436), (31, 0.019230769230769273), (32, 0.038461538461538436), (33, 0.019230769230769273), (34, 0.038461538461538436), (35, 0.019230769230769273), (36, 0.038461538461538436), (37, 0.019230769230769273), (38, 0.038461538461538436), (39, 0.019230769230769273), (40, 0.038461538461538436), (41, 0.019230769230769273), (42, 0.038461538461538436), (43, 0.019230769230769273), (44, 0.038461538461538436), (45, 0.019230769230769273), (46, 0.038461538461538436), (47, 0.019230769230769273), (48, 0.038461538461538436), (49, 0.019230769230769273), (50, 0.019230769230769273), (51, 0.038461538461538436), (52, 0.019230769230769273), (53, 0.038461538461538436), (54, 0.019230769230769273), (55, 0.038461538461538436), (56, 0.019230769230769273), (57, 0.038461538461538436), (58, 0.019230769230769273), (59, 0.038461538461538436), (60, 0.019230769230769273), (61, 0.038461538461538436), (62, 0.019230769230769273), (63, 0.038461538461538436), (64, 0.019230769230769273), (65, 0.038461538461538436), (66, 0.019230769230769273), (67, 0.038461538461538436), (68, 0.019230769230769273), (69, 0.038461538461538436), (70, 0.019230769230769273), (71, 0.038461538461538436), (72, 0.038461538461538436), (73, 0.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_list = np_training_perceptron_pocket_acc_epoch(letter_list[0], TRAINING_SET_2D[0])\n",
    "print()\n",
    "#print(len(new_list), new_list)\n",
    "\n",
    "epoch_list, err_list = [], []\n",
    "for x in range(len(new_list)):\n",
    "    epoch_list.append(new_list[x][0])\n",
    "    err_list.append(new_list[x][1])\n",
    "#print(epoch_list)\n",
    "#print(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea51b501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt # Needed to do matplotlib graphs\n",
    "\n",
    "# Build the plot of the learning curve via matplotlib.\n",
    "plt.title(\"Error Rate vs Epochs\")\n",
    "plt.xlabel(\"Epochs (n)\")\n",
    "plt.ylabel(\"Error Rate (%)\")\n",
    "plt.plot(epoch_list, err_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23cce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
