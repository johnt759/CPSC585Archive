{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ac1176",
   "metadata": {},
   "source": [
    "# Spring 2022\n",
    "# CPSC 585 Project 2\n",
    "## Raymond Carpio\n",
    "## Yu Pan\n",
    "## Sijie Shang\n",
    "## John Tu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436bbfe",
   "metadata": {},
   "source": [
    "# 1. As with Project 1, convert the images in TRAINING_SET, TEST_SET, and MESSAGE into two-dimensional NumPy arrays of size (# examples Ã— # features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3984e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 256)\n",
      "(26, 256)\n",
      "(31, 256)\n",
      "\n",
      "The image of the first letter: \n",
      "\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "The letter list:  ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J'\n",
      " 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']\n"
     ]
    }
   ],
   "source": [
    "from dataset import * # Import the entire dataset\n",
    "import random # Needed to generate random numbers\n",
    "import numpy as np # Needed to do NumPy functions\n",
    "\n",
    "# Convert the input array dataset into 2-dimensional NumPy array\n",
    "def convert_2d_array(input_data):\n",
    "    if len(input_data) == 0:\n",
    "        return None\n",
    "    output_data = []\n",
    "    if len(input_data[0]) == 2:\n",
    "        for x, y in input_data:\n",
    "            output_data.append(np.array(y))\n",
    "    else:\n",
    "        for x in input_data:\n",
    "            output_data.append(np.array(x))\n",
    "    return np.array(output_data)\n",
    "\n",
    "def letter_list(input_data):\n",
    "    if len(input_data) == 0:\n",
    "        return None\n",
    "    output_data = []\n",
    "    if len(input_data[0]) == 2:\n",
    "        for x, y in input_data:\n",
    "            output_data.append(np.array(x))\n",
    "    return np.array(output_data)\n",
    "\n",
    "def show(image):\n",
    "    letter_len = len(image)\n",
    "    counter = 0\n",
    "    # Since there are 16 characters in each row\n",
    "    # and the image is 16 rows by 16 columns,\n",
    "    # consider adding a newline for each current image\n",
    "    # by setting the counter to 0 after printing\n",
    "    # out the 16th character.\n",
    "    for x in range(letter_len):\n",
    "        if image[x] == 1:\n",
    "            print('#', end='')\n",
    "        else:\n",
    "            print(' ', end='')\n",
    "        counter += 1\n",
    "        if counter == 16:\n",
    "            counter = 0\n",
    "            print('\\n', end='')\n",
    "\n",
    "TRAINING_SET_2D = convert_2d_array(TRAINING_SET)\n",
    "TEST_SET_2D = convert_2d_array(TEST_SET)\n",
    "MESSAGE_2D = convert_2d_array(MESSAGE)\n",
    "\n",
    "# Verify that each 2-dimensional NumPy array contains the same number of letters and the\n",
    "# same number of bitmaps as the originla arrays.\n",
    "print(TRAINING_SET_2D.shape)\n",
    "print(TEST_SET_2D.shape)\n",
    "print(MESSAGE_2D.shape)\n",
    "\n",
    "print(\"\\nThe image of the first letter: \\n\")\n",
    "show(TRAINING_SET_2D[0])\n",
    "\n",
    "letter_list=letter_list(TRAINING_SET)\n",
    "print('The letter list: ',letter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779eb8ff",
   "metadata": {},
   "source": [
    "# 2. Rather than training 26 different perceptrons as you did in Project 1, this time you will use a single network with 26 possible outputs.\n",
    "# In order to use the character labels in TRAINING_SET and TEST_SET, convert them into integer class vectors using ord(), then into 26 one-hot encoded categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569ab036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Needed to do TensorFlow operations\n",
    "train_vec, test_vec = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b720fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "# Obtain all of the character labels from TRAINING_SET and TEST_SET\n",
    "# and convert them into integer class vectors, followed by\n",
    "# converting into one-hot encoded categorical features.\n",
    "for i in range(len(TRAINING_SET)):\n",
    "    train_vec.append(ord(TRAINING_SET[i][0])-ord('A'))\n",
    "print(train_vec)\n",
    "train_cat = tf.keras.utils.to_categorical(train_vec, num_classes=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760cdd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 26)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2c0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(TEST_SET)):\n",
    "    test_vec.append(ord(TEST_SET[j][0])-ord('A'))\n",
    "print(test_vec)\n",
    "test_cat = tf.keras.utils.to_categorical(test_vec, num_classes=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c8e206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f9ae96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e63dd4",
   "metadata": {},
   "source": [
    "# 3. Create a Sequential Keras model with a Dense hidden layer and a Dense output layer with softmax activation and categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb32f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden_layer (Dense)        (None, 32)                8224      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,082\n",
      "Trainable params: 9,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model with two Dense layers:\n",
    "# One as a hidden layer and the other as an output layer\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(256,)))\n",
    "model.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer\"))\n",
    "model.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f336e1",
   "metadata": {},
   "source": [
    "# 4. compile and fit the model to the training set. Train the model until the accuracy is as high as possible. You may wish to use an EarlyStopping callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0e19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0775344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99999\n",
      "2/2 - 0s - loss: 3.4489 - categorical_accuracy: 0.0385 - 325ms/epoch - 163ms/step\n",
      "Epoch 2/99999\n",
      "2/2 - 0s - loss: 3.3180 - categorical_accuracy: 0.0577 - 4ms/epoch - 2ms/step\n",
      "Epoch 3/99999\n",
      "2/2 - 0s - loss: 3.2216 - categorical_accuracy: 0.0962 - 4ms/epoch - 2ms/step\n",
      "Epoch 4/99999\n",
      "2/2 - 0s - loss: 3.1451 - categorical_accuracy: 0.1346 - 3ms/epoch - 1ms/step\n",
      "Epoch 5/99999\n",
      "2/2 - 0s - loss: 3.0867 - categorical_accuracy: 0.1538 - 3ms/epoch - 2ms/step\n",
      "Epoch 6/99999\n",
      "2/2 - 0s - loss: 3.0257 - categorical_accuracy: 0.2115 - 3ms/epoch - 2ms/step\n",
      "Epoch 7/99999\n",
      "2/2 - 0s - loss: 2.9731 - categorical_accuracy: 0.2115 - 4ms/epoch - 2ms/step\n",
      "Epoch 8/99999\n",
      "2/2 - 0s - loss: 2.9277 - categorical_accuracy: 0.2115 - 3ms/epoch - 2ms/step\n",
      "Epoch 9/99999\n",
      "2/2 - 0s - loss: 2.8812 - categorical_accuracy: 0.2308 - 3ms/epoch - 2ms/step\n",
      "Epoch 10/99999\n",
      "2/2 - 0s - loss: 2.8392 - categorical_accuracy: 0.2308 - 4ms/epoch - 2ms/step\n",
      "Epoch 11/99999\n",
      "2/2 - 0s - loss: 2.7985 - categorical_accuracy: 0.2885 - 3ms/epoch - 2ms/step\n",
      "Epoch 12/99999\n",
      "2/2 - 0s - loss: 2.7556 - categorical_accuracy: 0.2885 - 3ms/epoch - 2ms/step\n",
      "Epoch 13/99999\n",
      "2/2 - 0s - loss: 2.7131 - categorical_accuracy: 0.2885 - 3ms/epoch - 1ms/step\n",
      "Epoch 14/99999\n",
      "2/2 - 0s - loss: 2.6754 - categorical_accuracy: 0.3077 - 3ms/epoch - 2ms/step\n",
      "Epoch 15/99999\n",
      "2/2 - 0s - loss: 2.6332 - categorical_accuracy: 0.3077 - 3ms/epoch - 1ms/step\n",
      "Epoch 16/99999\n",
      "2/2 - 0s - loss: 2.5918 - categorical_accuracy: 0.3269 - 3ms/epoch - 2ms/step\n",
      "Epoch 17/99999\n",
      "2/2 - 0s - loss: 2.5493 - categorical_accuracy: 0.3269 - 2ms/epoch - 999us/step\n",
      "Epoch 18/99999\n",
      "2/2 - 0s - loss: 2.5075 - categorical_accuracy: 0.3654 - 3ms/epoch - 1ms/step\n",
      "Epoch 19/99999\n",
      "2/2 - 0s - loss: 2.4644 - categorical_accuracy: 0.3462 - 3ms/epoch - 2ms/step\n",
      "Epoch 20/99999\n",
      "2/2 - 0s - loss: 2.4202 - categorical_accuracy: 0.3462 - 3ms/epoch - 2ms/step\n",
      "Epoch 21/99999\n",
      "2/2 - 0s - loss: 2.3780 - categorical_accuracy: 0.3846 - 2ms/epoch - 1ms/step\n",
      "Epoch 22/99999\n",
      "2/2 - 0s - loss: 2.3358 - categorical_accuracy: 0.4423 - 4ms/epoch - 2ms/step\n",
      "Epoch 23/99999\n",
      "2/2 - 0s - loss: 2.2955 - categorical_accuracy: 0.4615 - 3ms/epoch - 2ms/step\n",
      "Epoch 24/99999\n",
      "2/2 - 0s - loss: 2.2528 - categorical_accuracy: 0.4808 - 3ms/epoch - 2ms/step\n",
      "Epoch 25/99999\n",
      "2/2 - 0s - loss: 2.2121 - categorical_accuracy: 0.5000 - 4ms/epoch - 2ms/step\n",
      "Epoch 26/99999\n",
      "2/2 - 0s - loss: 2.1690 - categorical_accuracy: 0.5192 - 3ms/epoch - 2ms/step\n",
      "Epoch 27/99999\n",
      "2/2 - 0s - loss: 2.1266 - categorical_accuracy: 0.5385 - 3ms/epoch - 1ms/step\n",
      "Epoch 28/99999\n",
      "2/2 - 0s - loss: 2.0866 - categorical_accuracy: 0.5769 - 3ms/epoch - 1ms/step\n",
      "Epoch 29/99999\n",
      "2/2 - 0s - loss: 2.0453 - categorical_accuracy: 0.5962 - 3ms/epoch - 2ms/step\n",
      "Epoch 30/99999\n",
      "2/2 - 0s - loss: 2.0032 - categorical_accuracy: 0.6154 - 4ms/epoch - 2ms/step\n",
      "Epoch 31/99999\n",
      "2/2 - 0s - loss: 1.9609 - categorical_accuracy: 0.6346 - 5ms/epoch - 2ms/step\n",
      "Epoch 32/99999\n",
      "2/2 - 0s - loss: 1.9192 - categorical_accuracy: 0.6346 - 4ms/epoch - 2ms/step\n",
      "Epoch 33/99999\n",
      "2/2 - 0s - loss: 1.8777 - categorical_accuracy: 0.6731 - 4ms/epoch - 2ms/step\n",
      "Epoch 34/99999\n",
      "2/2 - 0s - loss: 1.8373 - categorical_accuracy: 0.6538 - 4ms/epoch - 2ms/step\n",
      "Epoch 35/99999\n",
      "2/2 - 0s - loss: 1.7963 - categorical_accuracy: 0.6731 - 3ms/epoch - 2ms/step\n",
      "Epoch 36/99999\n",
      "2/2 - 0s - loss: 1.7547 - categorical_accuracy: 0.6923 - 4ms/epoch - 2ms/step\n",
      "Epoch 37/99999\n",
      "2/2 - 0s - loss: 1.7139 - categorical_accuracy: 0.7308 - 4ms/epoch - 2ms/step\n",
      "Epoch 38/99999\n",
      "2/2 - 0s - loss: 1.6742 - categorical_accuracy: 0.7308 - 3ms/epoch - 2ms/step\n",
      "Epoch 39/99999\n",
      "2/2 - 0s - loss: 1.6367 - categorical_accuracy: 0.7692 - 4ms/epoch - 2ms/step\n",
      "Epoch 40/99999\n",
      "2/2 - 0s - loss: 1.5988 - categorical_accuracy: 0.7885 - 3ms/epoch - 1ms/step\n",
      "Epoch 41/99999\n",
      "2/2 - 0s - loss: 1.5611 - categorical_accuracy: 0.8269 - 5ms/epoch - 2ms/step\n",
      "Epoch 42/99999\n",
      "2/2 - 0s - loss: 1.5219 - categorical_accuracy: 0.8462 - 4ms/epoch - 2ms/step\n",
      "Epoch 43/99999\n",
      "2/2 - 0s - loss: 1.4842 - categorical_accuracy: 0.8462 - 3ms/epoch - 2ms/step\n",
      "Epoch 44/99999\n",
      "2/2 - 0s - loss: 1.4478 - categorical_accuracy: 0.8462 - 4ms/epoch - 2ms/step\n",
      "Epoch 45/99999\n",
      "2/2 - 0s - loss: 1.4149 - categorical_accuracy: 0.8462 - 3ms/epoch - 2ms/step\n",
      "Epoch 46/99999\n",
      "2/2 - 0s - loss: 1.3777 - categorical_accuracy: 0.8654 - 3ms/epoch - 2ms/step\n",
      "Epoch 47/99999\n",
      "2/2 - 0s - loss: 1.3444 - categorical_accuracy: 0.8654 - 3ms/epoch - 1ms/step\n",
      "Epoch 48/99999\n",
      "2/2 - 0s - loss: 1.3090 - categorical_accuracy: 0.8654 - 4ms/epoch - 2ms/step\n",
      "Epoch 49/99999\n",
      "2/2 - 0s - loss: 1.2761 - categorical_accuracy: 0.8654 - 3ms/epoch - 2ms/step\n",
      "Epoch 50/99999\n",
      "2/2 - 0s - loss: 1.2425 - categorical_accuracy: 0.8654 - 3ms/epoch - 1ms/step\n",
      "Epoch 51/99999\n",
      "2/2 - 0s - loss: 1.2091 - categorical_accuracy: 0.8654 - 3ms/epoch - 1ms/step\n",
      "Epoch 52/99999\n",
      "2/2 - 0s - loss: 1.1793 - categorical_accuracy: 0.8654 - 2ms/epoch - 1ms/step\n",
      "Epoch 53/99999\n",
      "2/2 - 0s - loss: 1.1468 - categorical_accuracy: 0.8654 - 2ms/epoch - 1ms/step\n",
      "Epoch 54/99999\n",
      "2/2 - 0s - loss: 1.1168 - categorical_accuracy: 0.8846 - 3ms/epoch - 2ms/step\n",
      "Epoch 55/99999\n",
      "2/2 - 0s - loss: 1.0866 - categorical_accuracy: 0.8846 - 4ms/epoch - 2ms/step\n",
      "Epoch 56/99999\n",
      "2/2 - 0s - loss: 1.0562 - categorical_accuracy: 0.8846 - 3ms/epoch - 2ms/step\n",
      "Epoch 57/99999\n",
      "2/2 - 0s - loss: 1.0291 - categorical_accuracy: 0.8846 - 3ms/epoch - 2ms/step\n",
      "Epoch 58/99999\n",
      "2/2 - 0s - loss: 1.0006 - categorical_accuracy: 0.8846 - 3ms/epoch - 2ms/step\n",
      "Epoch 59/99999\n",
      "2/2 - 0s - loss: 0.9742 - categorical_accuracy: 0.9231 - 4ms/epoch - 2ms/step\n",
      "Epoch 60/99999\n",
      "2/2 - 0s - loss: 0.9475 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 61/99999\n",
      "2/2 - 0s - loss: 0.9231 - categorical_accuracy: 0.9615 - 2ms/epoch - 1ms/step\n",
      "Epoch 62/99999\n",
      "2/2 - 0s - loss: 0.8989 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 63/99999\n",
      "2/2 - 0s - loss: 0.8760 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 64/99999\n",
      "2/2 - 0s - loss: 0.8536 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 65/99999\n",
      "2/2 - 0s - loss: 0.8317 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 66/99999\n",
      "2/2 - 0s - loss: 0.8098 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 67/99999\n",
      "2/2 - 0s - loss: 0.7891 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 68/99999\n",
      "2/2 - 0s - loss: 0.7711 - categorical_accuracy: 0.9615 - 2ms/epoch - 1ms/step\n",
      "Epoch 69/99999\n",
      "2/2 - 0s - loss: 0.7509 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 70/99999\n",
      "2/2 - 0s - loss: 0.7316 - categorical_accuracy: 0.9615 - 4ms/epoch - 2ms/step\n",
      "Epoch 71/99999\n",
      "2/2 - 0s - loss: 0.7137 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 72/99999\n",
      "2/2 - 0s - loss: 0.6958 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 73/99999\n",
      "2/2 - 0s - loss: 0.6794 - categorical_accuracy: 0.9615 - 5ms/epoch - 2ms/step\n",
      "Epoch 74/99999\n",
      "2/2 - 0s - loss: 0.6621 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 75/99999\n",
      "2/2 - 0s - loss: 0.6462 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 76/99999\n",
      "2/2 - 0s - loss: 0.6310 - categorical_accuracy: 0.9615 - 2ms/epoch - 1ms/step\n",
      "Epoch 77/99999\n",
      "2/2 - 0s - loss: 0.6158 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 78/99999\n",
      "2/2 - 0s - loss: 0.6009 - categorical_accuracy: 0.9615 - 3ms/epoch - 1ms/step\n",
      "Epoch 79/99999\n",
      "2/2 - 0s - loss: 0.5866 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 80/99999\n",
      "2/2 - 0s - loss: 0.5732 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 81/99999\n",
      "2/2 - 0s - loss: 0.5612 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 82/99999\n",
      "2/2 - 0s - loss: 0.5478 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 83/99999\n",
      "2/2 - 0s - loss: 0.5366 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 84/99999\n",
      "2/2 - 0s - loss: 0.5236 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 85/99999\n",
      "2/2 - 0s - loss: 0.5118 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 86/99999\n",
      "2/2 - 0s - loss: 0.5008 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 87/99999\n",
      "2/2 - 0s - loss: 0.4906 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 88/99999\n",
      "2/2 - 0s - loss: 0.4796 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 89/99999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 0.4691 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 90/99999\n",
      "2/2 - 0s - loss: 0.4589 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 91/99999\n",
      "2/2 - 0s - loss: 0.4497 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 92/99999\n",
      "2/2 - 0s - loss: 0.4402 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 93/99999\n",
      "2/2 - 0s - loss: 0.4311 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 94/99999\n",
      "2/2 - 0s - loss: 0.4232 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 95/99999\n",
      "2/2 - 0s - loss: 0.4142 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 96/99999\n",
      "2/2 - 0s - loss: 0.4052 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 97/99999\n",
      "2/2 - 0s - loss: 0.3975 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 98/99999\n",
      "2/2 - 0s - loss: 0.3890 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 99/99999\n",
      "2/2 - 0s - loss: 0.3815 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 100/99999\n",
      "2/2 - 0s - loss: 0.3741 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 101/99999\n",
      "2/2 - 0s - loss: 0.3663 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 102/99999\n",
      "2/2 - 0s - loss: 0.3590 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 103/99999\n",
      "2/2 - 0s - loss: 0.3527 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 104/99999\n",
      "2/2 - 0s - loss: 0.3456 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 105/99999\n",
      "2/2 - 0s - loss: 0.3396 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 106/99999\n",
      "2/2 - 0s - loss: 0.3331 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 107/99999\n",
      "2/2 - 0s - loss: 0.3267 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 108/99999\n",
      "2/2 - 0s - loss: 0.3202 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 109/99999\n",
      "2/2 - 0s - loss: 0.3144 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 110/99999\n",
      "2/2 - 0s - loss: 0.3086 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 111/99999\n",
      "2/2 - 0s - loss: 0.3031 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 112/99999\n",
      "2/2 - 0s - loss: 0.2983 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 113/99999\n",
      "2/2 - 0s - loss: 0.2926 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 114/99999\n",
      "2/2 - 0s - loss: 0.2882 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 115/99999\n",
      "2/2 - 0s - loss: 0.2829 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 116/99999\n",
      "2/2 - 0s - loss: 0.2776 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 117/99999\n",
      "2/2 - 0s - loss: 0.2729 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 118/99999\n",
      "2/2 - 0s - loss: 0.2679 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 119/99999\n",
      "2/2 - 0s - loss: 0.2628 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 120/99999\n",
      "2/2 - 0s - loss: 0.2585 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 121/99999\n",
      "2/2 - 0s - loss: 0.2542 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 122/99999\n",
      "2/2 - 0s - loss: 0.2498 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 123/99999\n",
      "2/2 - 0s - loss: 0.2458 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 124/99999\n",
      "2/2 - 0s - loss: 0.2419 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 125/99999\n",
      "2/2 - 0s - loss: 0.2376 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 126/99999\n",
      "2/2 - 0s - loss: 0.2342 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 127/99999\n",
      "2/2 - 0s - loss: 0.2301 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 128/99999\n",
      "2/2 - 0s - loss: 0.2266 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 129/99999\n",
      "2/2 - 0s - loss: 0.2230 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 130/99999\n",
      "2/2 - 0s - loss: 0.2196 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 131/99999\n",
      "2/2 - 0s - loss: 0.2157 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 132/99999\n",
      "2/2 - 0s - loss: 0.2119 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 133/99999\n",
      "2/2 - 0s - loss: 0.2087 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 134/99999\n",
      "2/2 - 0s - loss: 0.2057 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 135/99999\n",
      "2/2 - 0s - loss: 0.2018 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 136/99999\n",
      "2/2 - 0s - loss: 0.1993 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 137/99999\n",
      "2/2 - 0s - loss: 0.1961 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 138/99999\n",
      "2/2 - 0s - loss: 0.1935 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 139/99999\n",
      "2/2 - 0s - loss: 0.1902 - categorical_accuracy: 1.0000 - 2ms/epoch - 988us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d3f7037d60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
    "model.fit(x=TRAINING_SET_2D, y=train_cat, epochs=99999, verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc2b3f",
   "metadata": {},
   "source": [
    "# 5. evaluate the model on TEST_SET. What accuracy do you obtain? If the accuracy is less than 100%, which test images are misclassified? (You may wish to use the show(image) function you defined in the previous project.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc24371e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step - loss: 1.9577 - categorical_accuracy: 0.4615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9576576948165894, 0.4615384638309479]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=TEST_SET_2D, y=test_cat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78de6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.57962155e-02, 1.16395526e-01, 1.02143027e-02, 2.53121499e-02,\n",
       "        1.46460179e-02, 5.12883940e-04, 5.07182896e-01, 1.21274078e-02,\n",
       "        1.75571125e-02, 5.07955148e-04, 5.93203993e-04, 8.13959241e-02,\n",
       "        8.42021778e-03, 4.94308397e-03, 1.18948650e-02, 5.66664431e-03,\n",
       "        2.50583352e-03, 1.00880787e-01, 9.35576973e-04, 2.87757255e-03,\n",
       "        6.30665291e-03, 7.43914643e-05, 9.38219018e-04, 3.71220929e-04,\n",
       "        1.61492487e-03, 2.03284062e-02],\n",
       "       [2.78142025e-03, 4.48902130e-01, 6.62311260e-03, 5.48501674e-04,\n",
       "        1.22337043e-01, 1.99291250e-03, 1.26616145e-02, 1.42757362e-03,\n",
       "        5.78744635e-02, 8.82589491e-04, 7.46692205e-03, 3.22373281e-03,\n",
       "        2.70746229e-03, 4.42104589e-04, 6.09944051e-04, 1.35740684e-02,\n",
       "        1.29720444e-04, 2.54259072e-02, 5.06526034e-04, 2.69179363e-02,\n",
       "        1.86799938e-04, 3.18633283e-05, 1.03688209e-04, 1.38401589e-03,\n",
       "        7.24144187e-03, 2.54016519e-01],\n",
       "       [3.15506421e-02, 1.04982927e-01, 1.45304397e-01, 2.26869099e-02,\n",
       "        1.58159465e-01, 1.32350978e-02, 2.77694523e-01, 5.91969304e-03,\n",
       "        6.47234870e-03, 2.00837199e-03, 2.37808819e-03, 4.53543775e-02,\n",
       "        1.54397322e-03, 4.04007500e-04, 5.23174293e-02, 1.39212841e-02,\n",
       "        1.10182809e-02, 2.33017504e-02, 4.75920038e-03, 1.93170120e-03,\n",
       "        3.18124285e-03, 2.56859494e-04, 2.59082386e-04, 1.30383487e-04,\n",
       "        1.74596743e-03, 6.94820508e-02],\n",
       "       [2.45406642e-03, 4.64262396e-01, 6.92988634e-02, 5.87386601e-02,\n",
       "        2.43886169e-02, 2.06806697e-03, 9.20084044e-02, 2.14610212e-02,\n",
       "        1.31233390e-02, 1.75105967e-03, 2.08400679e-03, 4.51981463e-02,\n",
       "        5.36511978e-03, 2.52254331e-03, 1.83195695e-02, 9.77507606e-03,\n",
       "        3.75394826e-04, 2.24571470e-02, 2.48109084e-02, 5.12662604e-02,\n",
       "        1.09489565e-03, 8.48393465e-05, 9.79721517e-05, 5.67876268e-04,\n",
       "        6.16502902e-03, 6.02607019e-02],\n",
       "       [1.18827767e-04, 5.92985526e-02, 3.05029191e-03, 8.15775056e-05,\n",
       "        5.67653835e-01, 6.23816885e-02, 2.47168378e-03, 2.70587363e-04,\n",
       "        8.82199965e-03, 7.71703257e-04, 7.87070543e-02, 4.37759794e-03,\n",
       "        2.87619041e-04, 2.09927966e-05, 3.57083882e-05, 1.24909095e-02,\n",
       "        2.00243248e-05, 5.56991668e-03, 9.49679816e-04, 2.77796686e-02,\n",
       "        2.59849694e-05, 9.12912765e-06, 5.71692508e-05, 2.10895599e-03,\n",
       "        2.29743533e-02, 1.39664501e-01],\n",
       "       [1.38802672e-04, 1.05704404e-02, 3.79706151e-03, 8.03103030e-05,\n",
       "        2.12100327e-01, 3.98414314e-01, 6.35499251e-04, 3.38295271e-04,\n",
       "        4.03103000e-03, 4.44310671e-03, 1.16548628e-01, 1.50089141e-03,\n",
       "        2.07569203e-04, 3.13025666e-05, 1.96100300e-05, 3.67672145e-02,\n",
       "        4.71718013e-05, 4.19317698e-03, 5.87245682e-03, 2.00582724e-02,\n",
       "        3.60389349e-05, 4.25977269e-05, 3.34767683e-04, 3.69881466e-03,\n",
       "        1.25780240e-01, 5.03121242e-02],\n",
       "       [4.02720319e-03, 5.74796870e-02, 9.76708345e-03, 5.65886544e-03,\n",
       "        8.37576091e-02, 1.01002492e-03, 6.31682336e-01, 1.86641980e-03,\n",
       "        3.31340241e-03, 2.22277435e-04, 2.93667108e-04, 1.13440789e-01,\n",
       "        2.04570024e-04, 5.74026635e-05, 1.54342437e-02, 8.45825765e-04,\n",
       "        6.71175250e-04, 6.71536336e-03, 1.01118628e-03, 5.45726682e-04,\n",
       "        1.37170928e-03, 1.38491687e-05, 4.26017687e-05, 3.23207605e-05,\n",
       "        3.65966931e-04, 6.01686686e-02],\n",
       "       [3.24812755e-02, 1.97127033e-02, 7.94427004e-03, 2.10942980e-02,\n",
       "        8.00146721e-03, 1.17771626e-02, 1.07602574e-01, 9.62505266e-02,\n",
       "        2.22316501e-03, 2.61084700e-04, 7.94075429e-03, 1.43104298e-02,\n",
       "        1.73783213e-01, 1.35328829e-01, 7.92563614e-03, 4.33165878e-02,\n",
       "        2.45231646e-03, 1.28826782e-01, 3.84002633e-04, 2.49962416e-03,\n",
       "        3.86445709e-02, 2.50617042e-03, 8.00238848e-02, 2.73760669e-02,\n",
       "        1.09839709e-02, 1.63486823e-02],\n",
       "       [8.47099349e-03, 1.52243618e-02, 1.59595255e-03, 3.16878577e-05,\n",
       "        8.57007783e-03, 8.91965901e-05, 1.14254514e-03, 1.03354373e-03,\n",
       "        6.78243279e-01, 5.05329017e-03, 3.80714948e-04, 2.71459576e-04,\n",
       "        6.27191737e-03, 2.32637612e-04, 3.25581903e-04, 3.20436549e-03,\n",
       "        3.49718030e-03, 2.36912630e-04, 4.83010057e-03, 2.32029527e-01,\n",
       "        6.53152092e-05, 5.92263765e-04, 1.61709278e-04, 3.58186371e-04,\n",
       "        9.89743881e-03, 1.81897413e-02],\n",
       "       [7.02108908e-03, 1.86582748e-02, 1.09112859e-02, 6.26486493e-03,\n",
       "        8.74633552e-04, 3.32329975e-04, 4.02658060e-03, 1.35017373e-02,\n",
       "        3.14734038e-03, 6.38553560e-01, 1.69144908e-03, 5.77914470e-04,\n",
       "        4.29875636e-03, 1.58993271e-03, 1.57268904e-02, 5.02995588e-03,\n",
       "        1.03789857e-02, 1.25546036e-02, 1.76846415e-01, 2.39467919e-02,\n",
       "        1.08642178e-02, 9.27060377e-03, 1.91902684e-03, 7.10229995e-03,\n",
       "        8.00053682e-03, 6.90997578e-03],\n",
       "       [3.68052133e-04, 3.24787083e-03, 2.89748167e-03, 6.37399149e-04,\n",
       "        2.40148865e-02, 4.20871899e-02, 2.82963680e-04, 2.04888708e-03,\n",
       "        1.19221639e-02, 5.16457343e-03, 4.88501012e-01, 3.22463084e-03,\n",
       "        2.20505521e-03, 6.32857613e-04, 9.48425368e-05, 1.79200508e-02,\n",
       "        2.25243610e-04, 2.21805796e-02, 2.73712375e-03, 2.42661200e-02,\n",
       "        5.28997800e-04, 2.81374669e-04, 5.60216885e-03, 2.20818877e-01,\n",
       "        9.94305685e-02, 1.86789613e-02],\n",
       "       [1.87956583e-04, 3.92501205e-02, 4.13951138e-03, 4.88210411e-04,\n",
       "        2.00577334e-01, 2.92209047e-03, 1.11308675e-02, 2.62859394e-03,\n",
       "        3.31124738e-02, 1.12998139e-04, 5.85063035e-03, 6.42624557e-01,\n",
       "        7.90032034e-04, 7.56693553e-05, 4.59561823e-04, 6.55602338e-03,\n",
       "        9.30092356e-05, 2.42106244e-03, 2.11942650e-04, 3.55630927e-03,\n",
       "        3.40524944e-04, 5.30974030e-05, 1.13786147e-04, 4.08298394e-04,\n",
       "        1.09415699e-03, 4.08011079e-02],\n",
       "       [2.43975185e-02, 1.34903546e-02, 1.07614743e-02, 4.18287935e-03,\n",
       "        3.40160653e-02, 6.25928342e-02, 1.54517160e-03, 7.73855951e-03,\n",
       "        1.94092025e-03, 2.55495440e-02, 4.09692407e-01, 9.18053463e-03,\n",
       "        1.98730528e-02, 5.46909915e-03, 7.38612202e-04, 1.58859372e-01,\n",
       "        3.05117923e-03, 5.80694638e-02, 1.61899403e-02, 7.05198105e-03,\n",
       "        1.96460392e-02, 1.30666373e-03, 5.95760159e-03, 3.76622602e-02,\n",
       "        5.14056608e-02, 9.63080954e-03],\n",
       "       [3.84032167e-02, 7.51905963e-02, 1.47342198e-02, 4.78893630e-02,\n",
       "        2.48987023e-02, 1.00228675e-02, 1.71416864e-01, 2.43269987e-02,\n",
       "        7.84055737e-04, 3.56339879e-04, 9.61285643e-03, 3.42972502e-02,\n",
       "        1.92301825e-01, 9.07752588e-02, 7.24763190e-03, 3.87725756e-02,\n",
       "        1.45762402e-03, 1.28387973e-01, 2.01391568e-03, 7.29659107e-03,\n",
       "        2.21164543e-02, 8.39871354e-04, 4.13938612e-03, 6.72716741e-03,\n",
       "        2.73194956e-03, 4.32583764e-02],\n",
       "       [2.84355413e-02, 1.47217721e-01, 4.79316041e-02, 1.12538181e-01,\n",
       "        3.52322008e-03, 4.55420086e-04, 1.65201366e-01, 6.70311507e-03,\n",
       "        1.20716216e-03, 7.75899738e-04, 3.99748569e-05, 9.26251616e-03,\n",
       "        1.06022228e-03, 7.71142426e-04, 4.31159258e-01, 1.40211650e-03,\n",
       "        4.11633030e-03, 2.51802597e-02, 4.68159327e-03, 6.69607718e-04,\n",
       "        1.58230786e-03, 5.71063501e-05, 6.54501564e-05, 2.08557813e-05,\n",
       "        2.25291864e-04, 5.71670849e-03],\n",
       "       [2.51913304e-03, 1.49125472e-01, 1.82292182e-02, 2.86851567e-03,\n",
       "        2.19975226e-02, 1.36629241e-02, 2.32299487e-03, 9.62003041e-03,\n",
       "        4.04979885e-02, 1.41307488e-02, 1.74644757e-02, 1.46264222e-03,\n",
       "        4.18429449e-03, 2.20730994e-03, 4.29854292e-04, 4.85759944e-01,\n",
       "        2.89956632e-04, 3.24955322e-02, 4.34884569e-03, 4.65122871e-02,\n",
       "        6.01107313e-04, 4.34620684e-04, 6.97612588e-04, 4.76274081e-03,\n",
       "        5.70793003e-02, 6.62948638e-02],\n",
       "       [5.01002138e-03, 9.95352585e-03, 1.17808729e-01, 5.62933795e-02,\n",
       "        2.24717357e-03, 1.22832821e-03, 4.29210216e-01, 3.42870764e-02,\n",
       "        7.57419271e-04, 6.61374535e-04, 5.02854309e-05, 1.04946541e-02,\n",
       "        2.69917864e-03, 3.04672722e-04, 2.16247320e-01, 7.35362154e-03,\n",
       "        6.35132268e-02, 1.16344206e-02, 4.18198109e-03, 8.86830385e-04,\n",
       "        1.09393084e-02, 2.21774634e-03, 3.01639386e-03, 1.39588141e-04,\n",
       "        5.64998074e-04, 8.29858519e-03],\n",
       "       [3.50641785e-03, 1.09275684e-01, 1.30007602e-02, 1.46355219e-02,\n",
       "        2.66933125e-02, 1.50757683e-02, 1.18639115e-02, 1.49115119e-02,\n",
       "        1.08681105e-01, 2.24396717e-02, 2.24380400e-02, 4.05300129e-03,\n",
       "        1.04800425e-02, 4.47761733e-03, 5.04447671e-04, 1.19772457e-01,\n",
       "        5.30835823e-04, 2.39365056e-01, 6.04638644e-03, 7.73747340e-02,\n",
       "        8.48255819e-04, 8.37986416e-04, 2.76942621e-03, 1.95652824e-02,\n",
       "        3.04370541e-02, 1.20415725e-01],\n",
       "       [1.84722692e-02, 4.28348929e-01, 7.03077167e-02, 7.99115282e-03,\n",
       "        1.14133224e-01, 8.81508552e-03, 8.19449127e-02, 3.19942227e-03,\n",
       "        2.06962973e-02, 4.08452423e-03, 3.98900453e-03, 9.14279930e-03,\n",
       "        6.69149682e-03, 1.92409876e-04, 1.49001796e-02, 9.12014395e-03,\n",
       "        2.42933887e-03, 5.46552539e-02, 4.40273108e-03, 1.86970662e-02,\n",
       "        1.61979790e-03, 1.37115829e-04, 1.52281704e-04, 5.20146743e-04,\n",
       "        5.48534980e-03, 1.09871387e-01],\n",
       "       [8.23397655e-03, 5.90726919e-03, 2.98753045e-02, 2.95562739e-03,\n",
       "        9.90509167e-02, 1.04220890e-01, 1.58818420e-02, 5.00476779e-03,\n",
       "        9.39747877e-03, 9.83808115e-02, 2.95006670e-02, 5.74679673e-03,\n",
       "        1.30134029e-03, 7.06503197e-05, 8.46168608e-04, 1.55795796e-03,\n",
       "        1.56538971e-02, 1.35926262e-03, 1.83827043e-01, 1.30884096e-01,\n",
       "        1.11555494e-02, 4.79987776e-03, 1.54171675e-03, 6.52722828e-03,\n",
       "        5.86660728e-02, 1.67652786e-01],\n",
       "       [2.13020458e-03, 2.34195180e-02, 2.07793061e-02, 2.37828828e-02,\n",
       "        7.87008032e-02, 7.62823550e-03, 5.07338524e-01, 2.82700965e-03,\n",
       "        2.51647172e-04, 4.26488259e-05, 2.66595976e-03, 2.43960068e-01,\n",
       "        4.33834968e-03, 8.24774732e-04, 9.71703697e-03, 3.62930656e-03,\n",
       "        1.27722532e-03, 3.36547382e-03, 1.67737191e-03, 9.62681719e-04,\n",
       "        4.45384532e-02, 2.06572760e-04, 7.12044595e-04, 4.37808194e-04,\n",
       "        1.31034746e-03, 1.34757869e-02],\n",
       "       [1.40473899e-02, 1.81700494e-02, 1.23500742e-01, 3.03554572e-02,\n",
       "        3.44638452e-02, 9.37721040e-03, 4.09161001e-02, 5.42111509e-02,\n",
       "        4.21783812e-02, 1.82933232e-03, 7.82179087e-03, 2.72883147e-01,\n",
       "        8.50941520e-03, 1.84712419e-03, 2.52463762e-02, 2.21748557e-02,\n",
       "        5.01135699e-02, 8.26767180e-03, 7.96936266e-03, 1.80884786e-02,\n",
       "        1.25689760e-01, 2.24129781e-02, 1.36481617e-02, 6.99654408e-03,\n",
       "        2.74594240e-02, 1.18216900e-02],\n",
       "       [6.75752014e-02, 4.50041052e-03, 2.94459891e-02, 3.30765545e-02,\n",
       "        4.45361137e-02, 1.55232633e-02, 2.20814645e-02, 5.23698181e-02,\n",
       "        5.79804182e-03, 2.62325685e-02, 4.15692478e-02, 1.62743911e-01,\n",
       "        3.03132064e-03, 2.07488192e-03, 1.75137483e-02, 2.40000784e-02,\n",
       "        1.27709266e-02, 1.18347527e-02, 2.37336084e-02, 1.83737255e-03,\n",
       "        2.89311349e-01, 3.80277187e-02, 2.66271532e-02, 1.62224825e-02,\n",
       "        2.24913675e-02, 5.07067610e-03],\n",
       "       [8.91628675e-03, 4.15073857e-02, 1.08526079e-02, 1.07782416e-03,\n",
       "        4.09981143e-03, 1.77913008e-03, 1.50560793e-02, 1.22195343e-02,\n",
       "        1.22980691e-01, 1.10498732e-02, 5.01634032e-02, 1.17689569e-03,\n",
       "        1.00560188e-01, 6.61865994e-02, 8.88758164e-04, 3.74627858e-02,\n",
       "        1.69809740e-02, 3.90637256e-02, 1.68806314e-03, 8.14959258e-02,\n",
       "        3.30408732e-03, 4.44225082e-03, 2.52879001e-02, 5.06344885e-02,\n",
       "        2.79550463e-01, 1.15743298e-02],\n",
       "       [5.27397469e-02, 5.69054484e-02, 5.47390711e-03, 7.38033140e-03,\n",
       "        1.09806154e-02, 5.78917284e-03, 2.96492819e-02, 3.94321233e-02,\n",
       "        1.56134635e-01, 2.39973385e-02, 1.49337063e-02, 4.04465012e-03,\n",
       "        3.18743885e-02, 4.70521823e-02, 1.70762204e-02, 1.31449141e-02,\n",
       "        4.23410870e-02, 1.36809498e-02, 6.21117232e-03, 5.91395088e-02,\n",
       "        5.41970767e-02, 2.56567392e-02, 5.43330312e-02, 7.08464980e-02,\n",
       "        1.26403868e-01, 3.05813998e-02],\n",
       "       [4.71658818e-03, 7.24381134e-02, 3.54159996e-02, 1.56576128e-03,\n",
       "        1.37497708e-01, 1.24121644e-02, 5.52241541e-02, 1.25374913e-03,\n",
       "        1.76239051e-02, 1.73199885e-02, 4.18376774e-02, 4.56937309e-03,\n",
       "        6.76515605e-03, 3.51748226e-04, 2.86773453e-03, 9.83698759e-03,\n",
       "        5.09238569e-03, 1.98396370e-02, 2.74986848e-02, 5.14090061e-02,\n",
       "        1.32623501e-03, 5.03253192e-04, 1.52985580e-04, 2.07437971e-03,\n",
       "        4.54938076e-02, 4.24912781e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(TEST_SET_2D)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c1363b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'G'],\n",
       " ['B', 'B'],\n",
       " ['C', 'G'],\n",
       " ['D', 'B'],\n",
       " ['E', 'E'],\n",
       " ['F', 'F'],\n",
       " ['G', 'G'],\n",
       " ['H', 'M'],\n",
       " ['I', 'I'],\n",
       " ['J', 'J'],\n",
       " ['K', 'K'],\n",
       " ['L', 'L'],\n",
       " ['M', 'K'],\n",
       " ['N', 'M'],\n",
       " ['O', 'O'],\n",
       " ['P', 'P'],\n",
       " ['Q', 'G'],\n",
       " ['R', 'R'],\n",
       " ['S', 'B'],\n",
       " ['T', 'S'],\n",
       " ['U', 'G'],\n",
       " ['V', 'L'],\n",
       " ['W', 'U'],\n",
       " ['X', 'Y'],\n",
       " ['Y', 'I'],\n",
       " ['Z', 'Z']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the actual and predicted letters from TEST_SET_2D.\n",
    "result_list = []\n",
    "for num in range(len(predicted)):\n",
    "    max_val = 0\n",
    "    actual = chr(num+ord('A'))\n",
    "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    result_list.append([actual, predict])\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd7aa893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images correctly classified: 46.15384615384615%\n",
      "Here are the list of test images that are misclassified and how they appear:\n",
      "Actual    Predicted\n",
      "A         G\n",
      "C         G\n",
      "D         B\n",
      "H         M\n",
      "M         K\n",
      "N         M\n",
      "Q         G\n",
      "S         B\n",
      "T         S\n",
      "U         G\n",
      "V         L\n",
      "W         U\n",
      "X         Y\n",
      "Y         I\n"
     ]
    }
   ],
   "source": [
    "# Verify the accuracy obtained for evaluate().\n",
    "list_misclassified = []\n",
    "num_total, num_correct = 26, 26\n",
    "for i in range(len(result_list)):\n",
    "    if result_list[i][0] != result_list[i][1]:\n",
    "        num_correct -= 1\n",
    "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
    "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_misclassified) == 0:\n",
    "    print(\"All test images are classified correctly.\")\n",
    "else:\n",
    "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_misclassified)):\n",
    "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7710e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Expected letter:\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##########      \n",
      "##########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "##########      \n",
      "##########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##########    \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ####\n",
      "  ####      ####\n",
      "  ##  ##  ##  ##\n",
      "  ##  ##  ##  ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "######  ######  \n",
      "######  ######  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ####        \n",
      "    ####        \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ####      ####\n",
      "  ####      ####\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "########  ####  \n",
      "########  ####  \n",
      "####  ########  \n",
      "####  ########  \n",
      "####    ######  \n",
      "####    ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ####\n",
      "  ####      ####\n",
      "  ##  ##  ##  ##\n",
      "  ##  ##  ##  ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####  ######    \n",
      "####  ######    \n",
      "  ########      \n",
      "  ########      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "######          \n",
      "######          \n",
      "  ######        \n",
      "  ######        \n",
      "      ######    \n",
      "      ######    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##########    \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "##  ####  ##    \n",
      "##  ####  ##    \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##            \n",
      "  ##            \n",
      "    ########    \n",
      "    ########    \n",
      "            ##  \n",
      "            ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ######        \n",
      "  ######        \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ############  \n",
      "  ############  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "##############  \n",
      "##############  \n",
      "######  ######  \n",
      "######  ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Use show() to display the misclassified images.\n",
    "for j in range(len(list_misclassified)):\n",
    "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
    "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
    "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
    "    print(\"================================================\")\n",
    "    print(\"Expected letter:\")\n",
    "    show(TRAINING_SET_2D[letter_train])\n",
    "    print(\"Predicted letter:\")\n",
    "    show(TEST_SET_2D[letter_test])\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886e9540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.80513433e-03, 6.94840075e-03, 2.45298799e-02, 3.46022006e-03,\n",
       "        9.15288031e-02, 1.17964290e-01, 1.69864651e-02, 5.24634123e-03,\n",
       "        7.83467665e-03, 1.14577293e-01, 2.92379092e-02, 4.42538038e-03,\n",
       "        1.53283135e-03, 9.59579411e-05, 1.40713702e-03, 2.52897269e-03,\n",
       "        1.70374941e-02, 1.87924749e-03, 1.93711683e-01, 9.52554941e-02,\n",
       "        1.50200007e-02, 4.28356137e-03, 1.75854890e-03, 7.74574792e-03,\n",
       "        6.38042316e-02, 1.62394300e-01],\n",
       "       [3.62713933e-02, 2.10895408e-02, 1.13307601e-02, 2.61840764e-02,\n",
       "        6.75748754e-03, 1.06659988e-02, 1.25159234e-01, 9.67902616e-02,\n",
       "        1.48571958e-03, 3.40868108e-04, 5.57172764e-03, 1.44618684e-02,\n",
       "        1.94013461e-01, 8.80643278e-02, 1.14752837e-02, 5.37421703e-02,\n",
       "        3.44386417e-03, 1.35382786e-01, 7.64088472e-04, 2.12556031e-03,\n",
       "        4.28436287e-02, 2.80452636e-03, 7.51823038e-02, 1.37292547e-02,\n",
       "        9.08599421e-03, 1.12339016e-02],\n",
       "       [1.41967612e-04, 7.41118565e-03, 1.06650812e-03, 2.19675829e-04,\n",
       "        8.12293112e-01, 9.05305371e-02, 9.29790374e-04, 2.41004760e-04,\n",
       "        1.42151999e-04, 3.80217716e-05, 3.45634446e-02, 1.76100321e-02,\n",
       "        4.64815384e-05, 6.65721063e-06, 8.68332700e-06, 4.58662026e-03,\n",
       "        2.58037880e-06, 9.49661364e-04, 1.28677953e-03, 6.28895767e-04,\n",
       "        1.53322457e-04, 5.05983394e-07, 1.96286801e-05, 1.83204305e-04,\n",
       "        7.64909317e-04, 2.61746701e-02],\n",
       "       [9.31220129e-05, 6.47717295e-03, 4.80370456e-03, 8.21404392e-04,\n",
       "        1.64223790e-01, 6.65266871e-01, 2.46466225e-04, 4.52723209e-04,\n",
       "        1.89162776e-04, 5.04564901e-04, 8.99003595e-02, 6.22369815e-03,\n",
       "        1.22758807e-04, 2.76395585e-05, 2.57403317e-05, 4.22760062e-02,\n",
       "        1.87730348e-05, 1.59484090e-03, 1.88353355e-03, 7.28874176e-04,\n",
       "        1.49006228e-04, 2.27453984e-05, 6.33028249e-05, 5.40573208e-04,\n",
       "        5.79674728e-03, 7.54629774e-03],\n",
       "       [1.52001716e-03, 2.46131956e-03, 5.56854029e-05, 4.29575402e-06,\n",
       "        9.62121179e-04, 8.55159233e-05, 7.28692394e-05, 8.01061047e-04,\n",
       "        7.90425599e-01, 5.26257791e-03, 9.98374773e-04, 5.90326636e-06,\n",
       "        2.63618445e-03, 5.43002352e-05, 3.22153437e-06, 6.62124192e-04,\n",
       "        2.80502223e-04, 1.83569951e-04, 5.63964713e-04, 1.59553871e-01,\n",
       "        3.92823094e-05, 4.03814192e-05, 4.56031645e-04, 3.45000112e-03,\n",
       "        6.22770237e-03, 2.31936239e-02],\n",
       "       [7.42009317e-04, 1.88255706e-06, 2.83467700e-04, 7.57080779e-05,\n",
       "        3.05555602e-07, 1.09439850e-06, 3.43155756e-04, 2.36260500e-02,\n",
       "        1.16413648e-05, 1.24702128e-02, 3.11785755e-07, 1.36933215e-06,\n",
       "        8.39949134e-05, 8.00437774e-05, 3.52565013e-03, 3.92744369e-05,\n",
       "        6.57248646e-02, 4.21787627e-06, 1.41570740e-03, 3.92967668e-05,\n",
       "        2.31301263e-02, 8.51108253e-01, 1.58286635e-02, 1.56379363e-04,\n",
       "        1.27812603e-03, 2.81367302e-05],\n",
       "       [9.98091782e-05, 6.64573759e-02, 3.26850475e-03, 7.01664248e-05,\n",
       "        4.50019449e-01, 4.49907221e-02, 2.53980816e-03, 4.37225564e-04,\n",
       "        1.94815919e-02, 9.96324350e-04, 8.33095163e-02, 4.34424402e-03,\n",
       "        3.86810338e-04, 2.72835277e-05, 2.70119417e-05, 1.58246215e-02,\n",
       "        2.13054718e-05, 9.97049455e-03, 9.69204353e-04, 5.86679652e-02,\n",
       "        2.57215670e-05, 8.24983636e-06, 8.64910689e-05, 4.18749172e-03,\n",
       "        3.16582955e-02, 2.02124417e-01],\n",
       "       [1.45137520e-03, 8.77783656e-01, 4.29074373e-03, 1.27137396e-02,\n",
       "        4.78997687e-03, 3.80486686e-04, 1.92706741e-03, 1.73893920e-03,\n",
       "        1.03789777e-03, 2.06227909e-04, 1.63943332e-03, 9.60718666e-04,\n",
       "        1.91650249e-03, 4.64982033e-04, 4.97711997e-04, 2.47168243e-02,\n",
       "        1.28831989e-05, 4.60524857e-02, 1.67622624e-04, 2.35910481e-03,\n",
       "        7.09809654e-04, 2.31283002e-06, 6.57684996e-06, 2.23452822e-04,\n",
       "        5.60294313e-04, 1.33891338e-02],\n",
       "       [6.51726825e-03, 8.93497840e-04, 2.89340932e-02, 6.57029822e-03,\n",
       "        2.28658428e-05, 1.14660988e-05, 2.64524873e-02, 3.92218754e-02,\n",
       "        1.48189874e-05, 7.75337685e-04, 4.21310460e-06, 7.48701059e-05,\n",
       "        2.92471983e-03, 8.85379384e-04, 5.89774966e-01, 8.91580494e-05,\n",
       "        1.07956499e-01, 1.60960329e-03, 3.83915976e-02, 2.46575277e-04,\n",
       "        1.32944569e-01, 1.05813500e-02, 2.46569747e-03, 2.05466771e-04,\n",
       "        3.06896283e-04, 2.12435424e-03],\n",
       "       [9.01108235e-03, 4.50106934e-02, 9.41857323e-03, 1.29212416e-03,\n",
       "        4.65858588e-03, 1.43591675e-03, 1.17579708e-02, 1.14922719e-02,\n",
       "        1.50351137e-01, 1.38727715e-02, 6.74276426e-02, 1.31359161e-03,\n",
       "        1.02288164e-01, 5.78841381e-02, 7.47457263e-04, 3.41259837e-02,\n",
       "        1.58493221e-02, 3.23022306e-02, 2.22043833e-03, 1.16698384e-01,\n",
       "        3.58490972e-03, 4.38135723e-03, 2.05234773e-02, 5.87167889e-02,\n",
       "        2.12679923e-01, 1.09551009e-02],\n",
       "       [9.42766419e-05, 7.69260386e-03, 2.45326868e-04, 7.58825126e-06,\n",
       "        6.22235984e-03, 5.64701295e-05, 6.23477681e-05, 7.10131368e-04,\n",
       "        8.31090868e-01, 2.22361574e-04, 9.49245470e-04, 3.76620825e-04,\n",
       "        1.37043640e-03, 6.74679104e-05, 1.11408135e-05, 3.86949535e-03,\n",
       "        3.35570112e-05, 3.06006368e-05, 2.16100627e-04, 1.32889405e-01,\n",
       "        5.39262965e-06, 6.01630090e-05, 4.92376821e-05, 5.46889263e-04,\n",
       "        6.94174645e-03, 6.17833389e-03],\n",
       "       [1.12015931e-02, 1.27286953e-03, 7.92913997e-05, 3.43904248e-04,\n",
       "        9.83293830e-06, 5.80161668e-06, 8.70604417e-04, 2.18580682e-02,\n",
       "        2.54100651e-05, 1.84193796e-05, 1.97637673e-05, 2.09881114e-06,\n",
       "        2.43663564e-01, 6.97472990e-01, 4.05944622e-04, 2.69005570e-04,\n",
       "        8.31828627e-04, 5.07646939e-03, 3.92619368e-05, 9.88437212e-04,\n",
       "        3.82752274e-03, 8.92609940e-04, 6.72577135e-03, 3.08604003e-03,\n",
       "        1.25605729e-04, 8.87377770e-04],\n",
       "       [1.90890580e-03, 1.39991511e-02, 7.80497789e-02, 4.67280950e-03,\n",
       "        4.08817548e-03, 1.13012611e-04, 8.27161908e-01, 6.80276309e-04,\n",
       "        3.51721828e-05, 2.24594944e-04, 4.67377868e-06, 5.00751147e-03,\n",
       "        1.26878600e-04, 6.09714562e-06, 3.80661301e-02, 5.29114550e-05,\n",
       "        3.31605971e-03, 2.86741415e-03, 5.82180638e-03, 2.28128323e-04,\n",
       "        6.29091228e-04, 1.31894931e-05, 9.48388606e-06, 2.26879456e-06,\n",
       "        2.19503854e-05, 1.28926504e-02],\n",
       "       [1.82174100e-03, 4.17593355e-06, 1.21136669e-04, 3.13955534e-05,\n",
       "        1.68475111e-07, 1.38561325e-06, 3.74385808e-03, 1.05881318e-01,\n",
       "        1.96868677e-06, 2.11490851e-06, 5.28921760e-07, 2.81938696e-06,\n",
       "        1.22206332e-02, 1.29756453e-02, 5.43938018e-04, 5.53917416e-05,\n",
       "        7.23459478e-03, 3.63736792e-04, 4.70959094e-06, 2.34399849e-05,\n",
       "        1.41897835e-02, 1.58063357e-03, 8.34240377e-01, 4.66128206e-03,\n",
       "        1.08033688e-04, 1.85278375e-04],\n",
       "       [3.41946445e-03, 2.91232974e-03, 1.34578222e-04, 1.56869592e-05,\n",
       "        1.10110606e-03, 1.35772993e-04, 1.59477437e-04, 1.40064792e-03,\n",
       "        8.26588452e-01, 1.02048730e-02, 7.35851063e-04, 1.60214731e-05,\n",
       "        2.15425389e-03, 8.69324213e-05, 1.00504885e-05, 1.06808951e-03,\n",
       "        6.61693979e-04, 4.13647125e-04, 1.03869999e-03, 1.18001260e-01,\n",
       "        5.88963012e-05, 9.37508812e-05, 5.63645328e-04, 1.94549654e-03,\n",
       "        6.89258380e-03, 2.01868117e-02],\n",
       "       [4.47046477e-03, 8.82285163e-02, 2.81136259e-02, 1.75805413e-03,\n",
       "        1.05614170e-01, 8.84359796e-03, 6.00694232e-02, 1.84571720e-03,\n",
       "        1.29160872e-02, 2.79027671e-02, 3.63476351e-02, 6.14544749e-03,\n",
       "        6.64646737e-03, 6.00060099e-04, 3.31145851e-03, 1.00301681e-02,\n",
       "        3.03602032e-03, 2.77061891e-02, 4.97475713e-02, 5.73405176e-02,\n",
       "        1.60840445e-03, 5.90978190e-04, 2.96031940e-04, 3.37161450e-03,\n",
       "        4.24491949e-02, 4.11009848e-01],\n",
       "       [8.08373570e-01, 3.91692016e-03, 2.86787422e-03, 3.31777334e-03,\n",
       "        4.42982964e-05, 4.94942287e-06, 2.26480030e-02, 1.34293605e-02,\n",
       "        9.89216423e-05, 7.05575658e-05, 2.77321391e-07, 3.33244243e-05,\n",
       "        2.31641456e-02, 6.74064225e-03, 7.06638694e-02, 1.91964253e-04,\n",
       "        2.51449142e-02, 1.51628340e-02, 1.49094136e-04, 7.45892685e-05,\n",
       "        2.57585873e-03, 2.73128826e-04, 2.93230230e-04, 4.13412818e-06,\n",
       "        1.18671396e-05, 7.43864977e-04],\n",
       "       [3.07422318e-03, 1.28780052e-01, 1.58948004e-02, 2.03214344e-02,\n",
       "        1.91355571e-02, 1.26248403e-02, 1.71346124e-02, 1.76832415e-02,\n",
       "        6.23672344e-02, 1.82384141e-02, 1.37411254e-02, 2.90676719e-03,\n",
       "        1.69900823e-02, 5.63875120e-03, 8.12343380e-04, 8.11877325e-02,\n",
       "        8.46645853e-04, 3.14210624e-01, 6.90341042e-03, 6.64644986e-02,\n",
       "        1.27651787e-03, 7.89237500e-04, 3.15115671e-03, 1.47921899e-02,\n",
       "        2.10937597e-02, 1.33940756e-01],\n",
       "       [2.18268135e-03, 4.47189450e-01, 7.13476613e-02, 5.81844449e-02,\n",
       "        2.53189672e-02, 1.58452475e-03, 1.07276425e-01, 2.03999095e-02,\n",
       "        1.75200738e-02, 2.13412032e-03, 1.79785956e-03, 4.35712673e-02,\n",
       "        5.13217878e-03, 2.24558800e-03, 2.30016522e-02, 1.14573333e-02,\n",
       "        4.83479293e-04, 3.42679508e-02, 2.12360080e-02, 5.06905057e-02,\n",
       "        9.72019334e-04, 1.06367173e-04, 1.07433647e-04, 5.27516007e-04,\n",
       "        7.26902625e-03, 4.39954549e-02],\n",
       "       [3.54981236e-03, 2.02003773e-02, 2.01848261e-02, 1.52830093e-03,\n",
       "        3.00520426e-03, 3.96179035e-04, 1.29652480e-02, 8.35573860e-03,\n",
       "        3.00954503e-04, 3.72292548e-02, 1.48329802e-03, 6.92037283e-04,\n",
       "        4.58507333e-03, 4.35692229e-04, 1.18098734e-02, 1.10651704e-03,\n",
       "        4.31048311e-03, 1.74501371e-02, 7.80334175e-01, 1.79854576e-02,\n",
       "        4.48634103e-03, 4.40489734e-04, 2.20092433e-03, 3.04216892e-03,\n",
       "        2.37412332e-03, 3.95473316e-02],\n",
       "       [2.48690788e-03, 1.06159132e-04, 3.69877016e-05, 3.29786490e-05,\n",
       "        1.13007691e-05, 6.30064960e-06, 7.45593352e-05, 2.54127686e-03,\n",
       "        5.19476831e-04, 9.48717415e-01, 6.44234096e-05, 3.07970072e-06,\n",
       "        1.44451740e-04, 4.64443219e-05, 2.34428968e-04, 1.58851311e-04,\n",
       "        7.73729640e-04, 1.29991997e-04, 3.16739045e-02, 1.30401750e-03,\n",
       "        1.54797430e-03, 3.06107802e-03, 7.81400071e-04, 1.50542695e-03,\n",
       "        3.26528214e-03, 7.72130792e-04],\n",
       "       [2.91727833e-03, 4.80569797e-05, 6.25819259e-04, 2.70661060e-03,\n",
       "        1.80916334e-06, 1.53373621e-05, 3.61475890e-04, 1.13367453e-01,\n",
       "        9.97822372e-06, 1.12876054e-02, 2.24053583e-05, 1.79012077e-05,\n",
       "        1.18463265e-03, 2.63246038e-04, 3.78660252e-03, 1.19379911e-04,\n",
       "        5.66015625e-03, 1.94936256e-05, 1.75853651e-02, 3.41876410e-04,\n",
       "        8.04966569e-01, 2.47269310e-02, 3.80913424e-03, 1.61080004e-03,\n",
       "        4.19868575e-03, 3.45408916e-04],\n",
       "       [4.74163145e-03, 1.30542117e-04, 6.54194355e-06, 7.93107665e-06,\n",
       "        3.16614546e-06, 1.49921402e-06, 6.21356885e-05, 9.28379223e-03,\n",
       "        2.59690405e-05, 7.18163449e-07, 8.16891479e-05, 1.91169477e-07,\n",
       "        8.67148459e-01, 8.63649547e-02, 3.14660338e-06, 1.34811664e-04,\n",
       "        7.18290175e-05, 2.29251385e-03, 3.08648907e-07, 2.58719287e-04,\n",
       "        1.24673569e-03, 2.74136055e-05, 9.86490399e-03, 1.64740309e-02,\n",
       "        1.20043092e-04, 1.64625933e-03],\n",
       "       [1.67677848e-04, 6.91078901e-02, 1.83591433e-02, 1.86246186e-02,\n",
       "        3.95607762e-02, 8.60259309e-02, 6.46381115e-04, 2.56114663e-03,\n",
       "        5.39741362e-04, 1.67725270e-03, 4.27508987e-02, 4.50232299e-03,\n",
       "        1.35539833e-03, 8.27600947e-04, 4.31394612e-04, 6.46951616e-01,\n",
       "        1.17426083e-04, 4.12746407e-02, 4.79217013e-03, 2.52837944e-03,\n",
       "        4.08372347e-04, 1.38461270e-04, 9.98894393e-05, 7.36258691e-04,\n",
       "        9.56935249e-03, 6.24529691e-03],\n",
       "       [2.19991035e-03, 1.18448306e-05, 1.19321998e-02, 4.63437260e-04,\n",
       "        2.87436677e-07, 5.36504388e-07, 3.17826681e-03, 2.13331729e-02,\n",
       "        8.80418611e-06, 2.35544489e-04, 1.70724036e-07, 1.63353695e-06,\n",
       "        7.10298191e-04, 9.96157542e-05, 1.14741050e-01, 1.50928345e-05,\n",
       "        8.11788142e-01, 3.66602559e-04, 2.19661673e-03, 2.74955455e-05,\n",
       "        1.21368757e-02, 1.13715092e-02, 6.99597597e-03, 4.06524523e-05,\n",
       "        9.86143787e-05, 4.56973212e-05],\n",
       "       [7.12031173e-03, 1.61453063e-04, 3.97457741e-03, 1.35457225e-03,\n",
       "        1.63060413e-05, 9.88516331e-06, 1.80773120e-02, 8.68243277e-02,\n",
       "        1.43954112e-05, 2.03773918e-04, 1.61840235e-05, 8.62600209e-05,\n",
       "        9.55962669e-03, 3.74216679e-03, 9.63952988e-02, 1.12207148e-04,\n",
       "        4.68242206e-02, 7.66935293e-04, 4.26064618e-03, 1.16612231e-04,\n",
       "        6.14079773e-01, 4.05681692e-02, 5.96708916e-02, 3.13739362e-03,\n",
       "        1.07963791e-03, 1.82710285e-03],\n",
       "       [4.86330682e-05, 4.05398151e-03, 1.66303755e-04, 3.22308870e-06,\n",
       "        2.35103630e-03, 3.63217041e-05, 2.29062825e-05, 4.42954391e-04,\n",
       "        8.92153680e-01, 2.94440601e-04, 6.81274629e-04, 1.76343281e-04,\n",
       "        7.76855566e-04, 2.59648332e-05, 4.84379416e-06, 2.00878759e-03,\n",
       "        2.95879145e-05, 1.94178156e-05, 1.53987086e-04, 8.63511264e-02,\n",
       "        4.35307720e-06, 5.49100223e-05, 3.64521111e-05, 3.90011090e-04,\n",
       "        5.99974673e-03, 3.71286203e-03],\n",
       "       [2.73406156e-03, 1.59022899e-03, 7.17503190e-01, 1.10543901e-02,\n",
       "        5.54525433e-03, 1.22752432e-02, 1.29670560e-01, 2.77780392e-03,\n",
       "        1.20177836e-04, 4.03632643e-04, 2.30864374e-04, 9.03796498e-03,\n",
       "        1.79218725e-04, 1.60085146e-05, 2.10372116e-02, 8.44331516e-04,\n",
       "        2.90736258e-02, 7.98978552e-04, 3.14337909e-02, 1.17236678e-03,\n",
       "        9.16254986e-03, 7.54218199e-04, 1.90766616e-04, 3.65379092e-05,\n",
       "        2.35479744e-03, 1.00022266e-02],\n",
       "       [7.49229395e-04, 4.98605780e-02, 1.63965265e-03, 5.20531437e-04,\n",
       "        8.43807235e-02, 1.80187747e-02, 4.55428031e-04, 2.44110497e-03,\n",
       "        2.38561933e-03, 5.12944942e-04, 6.97590828e-01, 2.00148951e-03,\n",
       "        6.01729425e-03, 6.91416208e-04, 5.57747262e-06, 2.33380646e-02,\n",
       "        1.05646377e-05, 9.02289897e-03, 8.07204633e-04, 2.54355222e-02,\n",
       "        1.03239506e-03, 1.23750597e-05, 8.79830099e-04, 4.17596474e-02,\n",
       "        1.13480100e-02, 1.90822612e-02],\n",
       "       [5.25644555e-06, 2.86000152e-03, 2.37059707e-04, 4.77915251e-04,\n",
       "        3.13748345e-02, 5.26519318e-04, 1.48704066e-03, 1.74018613e-04,\n",
       "        1.14395414e-04, 1.69576438e-07, 1.76503978e-04, 9.60548460e-01,\n",
       "        1.45002850e-05, 1.05281424e-06, 2.20664388e-05, 1.84433273e-04,\n",
       "        2.42857936e-07, 1.24079990e-04, 1.58593889e-06, 9.59010140e-06,\n",
       "        8.44039605e-05, 1.68010502e-07, 9.72272346e-07, 5.88031344e-06,\n",
       "        3.25215592e-06, 1.56563148e-03],\n",
       "       [1.85149955e-04, 3.06572346e-03, 6.85748761e-04, 1.28182919e-05,\n",
       "        6.31784729e-04, 8.69378680e-04, 8.31078141e-05, 2.65721901e-04,\n",
       "        4.17964486e-03, 1.00121377e-02, 7.54836425e-02, 1.86886246e-05,\n",
       "        4.11079545e-03, 9.15296085e-04, 3.27085327e-05, 9.79736075e-03,\n",
       "        3.70216410e-04, 4.28615883e-03, 1.33286486e-03, 2.26245895e-02,\n",
       "        1.85888784e-04, 6.97874872e-04, 2.15052953e-03, 5.11623658e-02,\n",
       "        8.04645658e-01, 2.19420739e-03]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict how the message will appear via Keras Sequential model.\n",
    "predicted_message = model.predict(MESSAGE_2D)\n",
    "predicted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd134ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMEFIVEBOYINGWIZARBSJUMPQUICKLY\n"
     ]
    }
   ],
   "source": [
    "message_list = \"\"\n",
    "for num in range(len(predicted_message)):\n",
    "    max_val = 0\n",
    "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    message_list += predict\n",
    "print(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9156e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy between predicted and actual message: 87.09677419354838%\n",
      "Here is the list of letters that are a mismatch between predicted and actual message:\n",
      "Actual    Predicted\n",
      "T         S\n",
      "H         M\n",
      "X         Y\n",
      "D         B\n"
     ]
    }
   ],
   "source": [
    "# Compare the predicted message with the actual message\n",
    "# by measuring the accuracy between the two.\n",
    "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
    "list_mismatch = []\n",
    "num_total, num_correct = len(actual_message), len(message_list)\n",
    "for i in range(len(actual_message)):\n",
    "    if message_list[i] != actual_message[i]:\n",
    "        num_correct -= 1\n",
    "        list_mismatch.append([actual_message[i], message_list[i]])\n",
    "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_mismatch) == 0:\n",
    "    print(\"The message appears to be decoded correctly.\")\n",
    "else:\n",
    "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_mismatch)):\n",
    "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e95bb0",
   "metadata": {},
   "source": [
    "# 6. How does this model compare with the performance of your perceptron models in Project 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa65f7c",
   "metadata": {},
   "source": [
    "## A: The accuracy obtained for the Keras Sequential model ended up being different from the perceptrons model for Project 1 as there are hidden layers involved for the former, and the performance will gradually worsen as more hidden layers are added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198255a",
   "metadata": {},
   "source": [
    "# 7. All of the letters in MESSAGE were likely not decoded correctly, so letâ€™s try to improve the performance of the model by adding additional hidden layers. Add two additional hidden layers of the same size as your original hidden layer, then repeat experiments (4) and (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c24533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
      "                                                                 \n",
      " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,194\n",
      "Trainable params: 11,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Do experiments 4 and 5 again, but this time, add two more hidden layers.\n",
    "model2 = tf.keras.models.Sequential()\n",
    "model2.add(tf.keras.Input(shape=(256,)))\n",
    "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
    "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
    "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
    "model2.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ed043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99999\n",
      "2/2 - 0s - loss: 3.2637 - categorical_accuracy: 0.0385 - 266ms/epoch - 133ms/step\n",
      "Epoch 2/99999\n",
      "2/2 - 0s - loss: 3.2172 - categorical_accuracy: 0.0962 - 5ms/epoch - 2ms/step\n",
      "Epoch 3/99999\n",
      "2/2 - 0s - loss: 3.1803 - categorical_accuracy: 0.0962 - 3ms/epoch - 2ms/step\n",
      "Epoch 4/99999\n",
      "2/2 - 0s - loss: 3.1518 - categorical_accuracy: 0.0769 - 4ms/epoch - 2ms/step\n",
      "Epoch 5/99999\n",
      "2/2 - 0s - loss: 3.1257 - categorical_accuracy: 0.1154 - 3ms/epoch - 2ms/step\n",
      "Epoch 6/99999\n",
      "2/2 - 0s - loss: 3.1003 - categorical_accuracy: 0.1154 - 3ms/epoch - 2ms/step\n",
      "Epoch 7/99999\n",
      "2/2 - 0s - loss: 3.0762 - categorical_accuracy: 0.1154 - 3ms/epoch - 2ms/step\n",
      "Epoch 8/99999\n",
      "2/2 - 0s - loss: 3.0459 - categorical_accuracy: 0.1154 - 3ms/epoch - 1ms/step\n",
      "Epoch 9/99999\n",
      "2/2 - 0s - loss: 3.0196 - categorical_accuracy: 0.1154 - 4ms/epoch - 2ms/step\n",
      "Epoch 10/99999\n",
      "2/2 - 0s - loss: 2.9849 - categorical_accuracy: 0.1538 - 4ms/epoch - 2ms/step\n",
      "Epoch 11/99999\n",
      "2/2 - 0s - loss: 2.9540 - categorical_accuracy: 0.1538 - 3ms/epoch - 2ms/step\n",
      "Epoch 12/99999\n",
      "2/2 - 0s - loss: 2.9236 - categorical_accuracy: 0.1538 - 6ms/epoch - 3ms/step\n",
      "Epoch 13/99999\n",
      "2/2 - 0s - loss: 2.8891 - categorical_accuracy: 0.1538 - 3ms/epoch - 2ms/step\n",
      "Epoch 14/99999\n",
      "2/2 - 0s - loss: 2.8543 - categorical_accuracy: 0.1538 - 4ms/epoch - 2ms/step\n",
      "Epoch 15/99999\n",
      "2/2 - 0s - loss: 2.8176 - categorical_accuracy: 0.1731 - 3ms/epoch - 2ms/step\n",
      "Epoch 16/99999\n",
      "2/2 - 0s - loss: 2.7779 - categorical_accuracy: 0.2308 - 3ms/epoch - 2ms/step\n",
      "Epoch 17/99999\n",
      "2/2 - 0s - loss: 2.7381 - categorical_accuracy: 0.2115 - 4ms/epoch - 2ms/step\n",
      "Epoch 18/99999\n",
      "2/2 - 0s - loss: 2.7007 - categorical_accuracy: 0.2308 - 3ms/epoch - 1ms/step\n",
      "Epoch 19/99999\n",
      "2/2 - 0s - loss: 2.6553 - categorical_accuracy: 0.2115 - 3ms/epoch - 1ms/step\n",
      "Epoch 20/99999\n",
      "2/2 - 0s - loss: 2.6150 - categorical_accuracy: 0.2115 - 3ms/epoch - 2ms/step\n",
      "Epoch 21/99999\n",
      "2/2 - 0s - loss: 2.5718 - categorical_accuracy: 0.2500 - 4ms/epoch - 2ms/step\n",
      "Epoch 22/99999\n",
      "2/2 - 0s - loss: 2.5253 - categorical_accuracy: 0.2692 - 4ms/epoch - 2ms/step\n",
      "Epoch 23/99999\n",
      "2/2 - 0s - loss: 2.4797 - categorical_accuracy: 0.2885 - 4ms/epoch - 2ms/step\n",
      "Epoch 24/99999\n",
      "2/2 - 0s - loss: 2.4308 - categorical_accuracy: 0.2885 - 4ms/epoch - 2ms/step\n",
      "Epoch 25/99999\n",
      "2/2 - 0s - loss: 2.3819 - categorical_accuracy: 0.3077 - 3ms/epoch - 2ms/step\n",
      "Epoch 26/99999\n",
      "2/2 - 0s - loss: 2.3292 - categorical_accuracy: 0.3269 - 3ms/epoch - 2ms/step\n",
      "Epoch 27/99999\n",
      "2/2 - 0s - loss: 2.2808 - categorical_accuracy: 0.3654 - 3ms/epoch - 2ms/step\n",
      "Epoch 28/99999\n",
      "2/2 - 0s - loss: 2.2275 - categorical_accuracy: 0.3846 - 3ms/epoch - 2ms/step\n",
      "Epoch 29/99999\n",
      "2/2 - 0s - loss: 2.1728 - categorical_accuracy: 0.4231 - 3ms/epoch - 2ms/step\n",
      "Epoch 30/99999\n",
      "2/2 - 0s - loss: 2.1153 - categorical_accuracy: 0.4423 - 3ms/epoch - 2ms/step\n",
      "Epoch 31/99999\n",
      "2/2 - 0s - loss: 2.0595 - categorical_accuracy: 0.4423 - 3ms/epoch - 1ms/step\n",
      "Epoch 32/99999\n",
      "2/2 - 0s - loss: 2.0014 - categorical_accuracy: 0.4615 - 3ms/epoch - 1ms/step\n",
      "Epoch 33/99999\n",
      "2/2 - 0s - loss: 1.9420 - categorical_accuracy: 0.4808 - 3ms/epoch - 2ms/step\n",
      "Epoch 34/99999\n",
      "2/2 - 0s - loss: 1.8811 - categorical_accuracy: 0.5192 - 3ms/epoch - 2ms/step\n",
      "Epoch 35/99999\n",
      "2/2 - 0s - loss: 1.8186 - categorical_accuracy: 0.5385 - 3ms/epoch - 1ms/step\n",
      "Epoch 36/99999\n",
      "2/2 - 0s - loss: 1.7599 - categorical_accuracy: 0.5962 - 2ms/epoch - 1ms/step\n",
      "Epoch 37/99999\n",
      "2/2 - 0s - loss: 1.6953 - categorical_accuracy: 0.5962 - 2ms/epoch - 1ms/step\n",
      "Epoch 38/99999\n",
      "2/2 - 0s - loss: 1.6358 - categorical_accuracy: 0.6346 - 4ms/epoch - 2ms/step\n",
      "Epoch 39/99999\n",
      "2/2 - 0s - loss: 1.5726 - categorical_accuracy: 0.6731 - 3ms/epoch - 2ms/step\n",
      "Epoch 40/99999\n",
      "2/2 - 0s - loss: 1.5146 - categorical_accuracy: 0.6923 - 3ms/epoch - 2ms/step\n",
      "Epoch 41/99999\n",
      "2/2 - 0s - loss: 1.4512 - categorical_accuracy: 0.7115 - 3ms/epoch - 2ms/step\n",
      "Epoch 42/99999\n",
      "2/2 - 0s - loss: 1.3890 - categorical_accuracy: 0.6923 - 3ms/epoch - 1ms/step\n",
      "Epoch 43/99999\n",
      "2/2 - 0s - loss: 1.3303 - categorical_accuracy: 0.7500 - 4ms/epoch - 2ms/step\n",
      "Epoch 44/99999\n",
      "2/2 - 0s - loss: 1.2710 - categorical_accuracy: 0.8077 - 4ms/epoch - 2ms/step\n",
      "Epoch 45/99999\n",
      "2/2 - 0s - loss: 1.2106 - categorical_accuracy: 0.8462 - 3ms/epoch - 2ms/step\n",
      "Epoch 46/99999\n",
      "2/2 - 0s - loss: 1.1514 - categorical_accuracy: 0.8654 - 3ms/epoch - 2ms/step\n",
      "Epoch 47/99999\n",
      "2/2 - 0s - loss: 1.0894 - categorical_accuracy: 0.8846 - 3ms/epoch - 1ms/step\n",
      "Epoch 48/99999\n",
      "2/2 - 0s - loss: 1.0352 - categorical_accuracy: 0.9038 - 3ms/epoch - 2ms/step\n",
      "Epoch 49/99999\n",
      "2/2 - 0s - loss: 0.9821 - categorical_accuracy: 0.9231 - 3ms/epoch - 1ms/step\n",
      "Epoch 50/99999\n",
      "2/2 - 0s - loss: 0.9278 - categorical_accuracy: 0.9231 - 3ms/epoch - 1ms/step\n",
      "Epoch 51/99999\n",
      "2/2 - 0s - loss: 0.8751 - categorical_accuracy: 0.9423 - 4ms/epoch - 2ms/step\n",
      "Epoch 52/99999\n",
      "2/2 - 0s - loss: 0.8270 - categorical_accuracy: 0.9423 - 3ms/epoch - 2ms/step\n",
      "Epoch 53/99999\n",
      "2/2 - 0s - loss: 0.7786 - categorical_accuracy: 0.9423 - 4ms/epoch - 2ms/step\n",
      "Epoch 54/99999\n",
      "2/2 - 0s - loss: 0.7355 - categorical_accuracy: 0.9423 - 4ms/epoch - 2ms/step\n",
      "Epoch 55/99999\n",
      "2/2 - 0s - loss: 0.6966 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 56/99999\n",
      "2/2 - 0s - loss: 0.6558 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 57/99999\n",
      "2/2 - 0s - loss: 0.6179 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 58/99999\n",
      "2/2 - 0s - loss: 0.5824 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 59/99999\n",
      "2/2 - 0s - loss: 0.5503 - categorical_accuracy: 0.9808 - 2ms/epoch - 950us/step\n",
      "Epoch 60/99999\n",
      "2/2 - 0s - loss: 0.5181 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 61/99999\n",
      "2/2 - 0s - loss: 0.4898 - categorical_accuracy: 0.9808 - 5ms/epoch - 2ms/step\n",
      "Epoch 62/99999\n",
      "2/2 - 0s - loss: 0.4607 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 63/99999\n",
      "2/2 - 0s - loss: 0.4368 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 64/99999\n",
      "2/2 - 0s - loss: 0.4119 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 65/99999\n",
      "2/2 - 0s - loss: 0.3916 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 66/99999\n",
      "2/2 - 0s - loss: 0.3723 - categorical_accuracy: 0.9808 - 2ms/epoch - 1ms/step\n",
      "Epoch 67/99999\n",
      "2/2 - 0s - loss: 0.3531 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 68/99999\n",
      "2/2 - 0s - loss: 0.3357 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 69/99999\n",
      "2/2 - 0s - loss: 0.3186 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 70/99999\n",
      "2/2 - 0s - loss: 0.3048 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 71/99999\n",
      "2/2 - 0s - loss: 0.2900 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 72/99999\n",
      "2/2 - 0s - loss: 0.2773 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 73/99999\n",
      "2/2 - 0s - loss: 0.2645 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 74/99999\n",
      "2/2 - 0s - loss: 0.2525 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 75/99999\n",
      "2/2 - 0s - loss: 0.2393 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 76/99999\n",
      "2/2 - 0s - loss: 0.2297 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 77/99999\n",
      "2/2 - 0s - loss: 0.2193 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 78/99999\n",
      "2/2 - 0s - loss: 0.2110 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 79/99999\n",
      "2/2 - 0s - loss: 0.2040 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 80/99999\n",
      "2/2 - 0s - loss: 0.1963 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 81/99999\n",
      "2/2 - 0s - loss: 0.1847 - categorical_accuracy: 1.0000 - 6ms/epoch - 3ms/step\n",
      "Epoch 82/99999\n",
      "2/2 - 0s - loss: 0.1781 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 83/99999\n",
      "2/2 - 0s - loss: 0.1716 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 84/99999\n",
      "2/2 - 0s - loss: 0.1648 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 85/99999\n",
      "2/2 - 0s - loss: 0.1577 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 86/99999\n",
      "2/2 - 0s - loss: 0.1509 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 87/99999\n",
      "2/2 - 0s - loss: 0.1456 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 88/99999\n",
      "2/2 - 0s - loss: 0.1407 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 89/99999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 0.1357 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 90/99999\n",
      "2/2 - 0s - loss: 0.1299 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 91/99999\n",
      "2/2 - 0s - loss: 0.1276 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 92/99999\n",
      "2/2 - 0s - loss: 0.1220 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 93/99999\n",
      "2/2 - 0s - loss: 0.1168 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 94/99999\n",
      "2/2 - 0s - loss: 0.1138 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 95/99999\n",
      "2/2 - 0s - loss: 0.1092 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 96/99999\n",
      "2/2 - 0s - loss: 0.1054 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 97/99999\n",
      "2/2 - 0s - loss: 0.1013 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 98/99999\n",
      "2/2 - 0s - loss: 0.0978 - categorical_accuracy: 1.0000 - 2ms/epoch - 1ms/step\n",
      "Epoch 99/99999\n",
      "2/2 - 0s - loss: 0.0948 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 100/99999\n",
      "2/2 - 0s - loss: 0.0926 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 101/99999\n",
      "2/2 - 0s - loss: 0.0890 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 102/99999\n",
      "2/2 - 0s - loss: 0.0860 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 103/99999\n",
      "2/2 - 0s - loss: 0.0837 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 104/99999\n",
      "2/2 - 0s - loss: 0.0811 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 105/99999\n",
      "2/2 - 0s - loss: 0.0784 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 106/99999\n",
      "2/2 - 0s - loss: 0.0763 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 107/99999\n",
      "2/2 - 0s - loss: 0.0737 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 108/99999\n",
      "2/2 - 0s - loss: 0.0713 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 109/99999\n",
      "2/2 - 0s - loss: 0.0695 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 110/99999\n",
      "2/2 - 0s - loss: 0.0674 - categorical_accuracy: 1.0000 - 5ms/epoch - 3ms/step\n",
      "Epoch 111/99999\n",
      "2/2 - 0s - loss: 0.0656 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 112/99999\n",
      "2/2 - 0s - loss: 0.0636 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 113/99999\n",
      "2/2 - 0s - loss: 0.0622 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 114/99999\n",
      "2/2 - 0s - loss: 0.0601 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 115/99999\n",
      "2/2 - 0s - loss: 0.0585 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 116/99999\n",
      "2/2 - 0s - loss: 0.0570 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 117/99999\n",
      "2/2 - 0s - loss: 0.0553 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d3f8486ca0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
    "model2.fit(x=TRAINING_SET_2D, y=train_cat, epochs=99999, verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94f0590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step - loss: 3.0493 - categorical_accuracy: 0.3462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.049349546432495, 0.3461538553237915]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x=TEST_SET_2D, y=test_cat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92270312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.54389101e-02, 3.59087922e-02, 2.38030851e-02, 8.83013054e-05,\n",
       "        3.89921740e-02, 2.44219613e-04, 1.68247804e-01, 2.56465049e-03,\n",
       "        1.21479388e-06, 4.18645534e-04, 5.61349792e-04, 1.46704987e-01,\n",
       "        2.53102946e-04, 3.92360426e-03, 6.92489445e-02, 2.49486880e-06,\n",
       "        1.62511511e-04, 1.18834410e-04, 2.30849022e-04, 1.56575588e-05,\n",
       "        8.56121551e-05, 5.38697746e-03, 4.66835380e-01, 4.19443211e-04,\n",
       "        5.07606183e-06, 3.37357691e-04],\n",
       "       [2.35362859e-06, 9.38594434e-03, 3.58163239e-03, 1.39523181e-04,\n",
       "        8.00664723e-03, 5.00571549e-01, 8.02069320e-04, 1.13413364e-06,\n",
       "        2.96379067e-02, 2.30892678e-03, 3.93001828e-04, 1.51291752e-04,\n",
       "        2.56899352e-06, 5.78165782e-06, 1.40080083e-04, 3.05245876e-01,\n",
       "        5.65796290e-06, 1.13460432e-04, 8.53920123e-04, 8.44584629e-02,\n",
       "        2.97527922e-08, 2.15204036e-05, 6.06978138e-05, 1.54864756e-04,\n",
       "        1.11322687e-03, 5.28419577e-02],\n",
       "       [2.38889377e-04, 1.47901736e-02, 4.81610119e-01, 7.56423447e-07,\n",
       "        6.44868538e-02, 9.38256271e-03, 2.75614500e-01, 1.37334340e-04,\n",
       "        1.69130381e-05, 3.22450251e-05, 1.68401129e-05, 1.40487015e-01,\n",
       "        3.40140068e-05, 2.01921750e-04, 1.49693456e-03, 1.77728594e-04,\n",
       "        3.89127899e-06, 1.67124153e-06, 2.32442646e-04, 3.32234893e-04,\n",
       "        2.19289541e-06, 1.70219806e-04, 6.00220030e-03, 3.43899679e-04,\n",
       "        1.21168177e-05, 4.17427905e-03],\n",
       "       [2.40092631e-05, 1.04063135e-02, 4.36260272e-03, 3.90088111e-02,\n",
       "        4.18970652e-04, 2.60199439e-02, 1.58650090e-03, 1.14056120e-05,\n",
       "        2.01096252e-01, 9.67371278e-03, 5.79321175e-04, 2.01990450e-04,\n",
       "        1.19635888e-06, 1.83694056e-06, 2.42472952e-03, 7.49536902e-02,\n",
       "        6.63496321e-05, 2.37276356e-04, 1.57328937e-02, 3.08398277e-01,\n",
       "        2.76801472e-07, 8.41939618e-05, 3.59361700e-04, 1.58559473e-04,\n",
       "        3.16948742e-02, 2.72496611e-01],\n",
       "       [3.14537445e-08, 1.40438715e-05, 1.23400154e-04, 1.67508070e-08,\n",
       "        2.16408409e-02, 9.18343246e-01, 6.74781448e-04, 9.70135083e-10,\n",
       "        8.34898837e-03, 1.20894925e-03, 1.24416931e-03, 2.61577429e-06,\n",
       "        9.92144393e-08, 1.89829170e-05, 1.89242437e-05, 1.76204890e-02,\n",
       "        1.80180812e-08, 4.90293996e-06, 1.92104017e-05, 1.61700249e-02,\n",
       "        7.08561890e-11, 5.93717687e-06, 9.45368811e-07, 9.12815239e-03,\n",
       "        1.43627753e-04, 5.26775280e-03],\n",
       "       [6.91011248e-10, 3.13638310e-07, 1.17946468e-06, 2.21026273e-08,\n",
       "        1.02588572e-04, 7.28075504e-01, 3.44788577e-06, 2.36303581e-11,\n",
       "        2.15725258e-01, 4.60236392e-04, 7.49325263e-05, 2.90296898e-09,\n",
       "        1.40537804e-09, 1.12092799e-07, 1.40268889e-06, 1.92394406e-02,\n",
       "        8.71968553e-10, 6.21140146e-07, 6.96713960e-06, 3.38669531e-02,\n",
       "        7.57198286e-13, 3.57045082e-07, 3.41064399e-09, 7.47957383e-04,\n",
       "        2.14774482e-04, 1.47784606e-03],\n",
       "       [3.18720879e-04, 1.96259213e-03, 3.48076113e-02, 4.21959134e-09,\n",
       "        2.74473485e-02, 2.01055082e-04, 7.38179624e-01, 6.67505592e-05,\n",
       "        1.35777425e-08, 8.53414747e-07, 2.18353252e-05, 1.80056348e-01,\n",
       "        3.80204474e-05, 2.45341193e-03, 2.28178711e-03, 1.05387223e-07,\n",
       "        1.45535591e-06, 3.31685982e-07, 2.53685848e-05, 8.35750143e-07,\n",
       "        1.82470114e-06, 1.13586480e-04, 1.16454018e-02, 3.09006631e-04,\n",
       "        1.07400027e-07, 6.60503210e-05],\n",
       "       [3.06894863e-03, 9.91216861e-04, 2.69442477e-04, 7.75648732e-06,\n",
       "        1.12979367e-01, 2.63258204e-04, 5.92771743e-04, 9.91965760e-04,\n",
       "        5.26146891e-07, 1.97491306e-03, 8.12846795e-02, 3.59978876e-04,\n",
       "        6.95071742e-02, 6.73133910e-01, 4.02857811e-04, 3.03161669e-05,\n",
       "        9.37880541e-05, 2.59950683e-02, 8.10090423e-05, 1.49856726e-06,\n",
       "        1.35190479e-04, 1.62153644e-03, 1.91831626e-02, 7.01381452e-03,\n",
       "        2.86071622e-06, 1.30360104e-05],\n",
       "       [9.98307002e-08, 2.13271562e-07, 4.63109745e-06, 1.56726146e-05,\n",
       "        5.98370207e-06, 5.34874573e-03, 2.97396055e-05, 1.19351355e-08,\n",
       "        8.61967564e-01, 1.70670057e-04, 1.87271689e-05, 1.58663269e-07,\n",
       "        3.11538484e-09, 4.11786203e-07, 1.76351296e-05, 1.27562671e-04,\n",
       "        1.11028463e-07, 6.65817979e-08, 3.44895525e-04, 1.28515735e-01,\n",
       "        2.32630665e-10, 3.69990630e-06, 6.75279637e-08, 1.68183426e-04,\n",
       "        2.63316184e-03, 6.26240333e-04],\n",
       "       [1.98759348e-03, 1.43006118e-03, 3.63293402e-05, 2.75815606e-01,\n",
       "        6.10495554e-05, 1.06623966e-05, 8.59660446e-04, 1.19098825e-02,\n",
       "        1.06091250e-03, 9.93370116e-02, 2.30651535e-03, 3.39295686e-04,\n",
       "        1.41780420e-05, 1.00464713e-05, 2.45582476e-01, 1.48961728e-04,\n",
       "        2.21691743e-01, 6.00316655e-03, 8.22265968e-02, 1.15540926e-04,\n",
       "        6.89676777e-03, 2.47325338e-02, 1.43945515e-02, 2.29649449e-04,\n",
       "        2.12522224e-03, 6.73932722e-04],\n",
       "       [2.18837340e-06, 1.60915733e-04, 2.25181459e-04, 3.16162623e-05,\n",
       "        2.27691280e-03, 1.40733287e-01, 3.69252462e-04, 4.09530116e-07,\n",
       "        1.43210381e-01, 1.41760632e-02, 1.35955125e-01, 1.98528528e-06,\n",
       "        2.28808149e-05, 1.52399472e-04, 2.24114527e-04, 1.62215188e-01,\n",
       "        6.23295955e-06, 3.52436421e-03, 3.07366671e-03, 1.12052836e-01,\n",
       "        7.72077655e-08, 5.39270877e-05, 8.60547243e-06, 1.68289140e-01,\n",
       "        3.35397162e-02, 7.96935409e-02],\n",
       "       [1.43080120e-04, 3.37849073e-02, 5.51623292e-02, 1.50596361e-05,\n",
       "        1.56209677e-01, 9.22459811e-02, 1.58986956e-01, 7.18744341e-05,\n",
       "        1.22006742e-04, 5.93787467e-04, 2.94766651e-04, 4.78339255e-01,\n",
       "        1.75649911e-05, 6.08315284e-04, 1.61084847e-03, 2.00211303e-03,\n",
       "        2.03739201e-05, 3.98575721e-05, 5.29748504e-04, 1.90616003e-03,\n",
       "        2.31124159e-06, 9.98186530e-04, 1.00939805e-02, 5.04841795e-04,\n",
       "        4.62866810e-05, 5.64967003e-03],\n",
       "       [1.28700631e-03, 5.34939906e-03, 8.68858071e-04, 1.04256351e-05,\n",
       "        4.51551266e-02, 2.04365868e-02, 9.45104752e-03, 3.26382346e-04,\n",
       "        6.17238256e-05, 2.26561609e-03, 2.07812801e-01, 4.17894358e-03,\n",
       "        1.20663960e-02, 5.31091802e-02, 4.88791475e-03, 9.93545866e-04,\n",
       "        1.40401348e-03, 1.97915062e-02, 2.22509238e-03, 2.37563509e-04,\n",
       "        3.31184332e-04, 4.24809521e-03, 6.05264620e-04, 6.01904750e-01,\n",
       "        4.23185818e-04, 5.68433257e-04],\n",
       "       [4.32638172e-03, 7.52374006e-04, 6.68366556e-04, 1.41336940e-07,\n",
       "        1.75998852e-01, 3.45792301e-04, 1.04510048e-02, 4.37860930e-04,\n",
       "        2.87481008e-08, 1.19386830e-04, 1.42085338e-02, 1.24949990e-02,\n",
       "        4.70124371e-02, 7.08496749e-01, 5.32581995e-04, 1.81320138e-06,\n",
       "        3.39630642e-05, 7.31967913e-04, 4.13610214e-05, 5.51507810e-07,\n",
       "        7.22971599e-05, 7.72766187e-04, 8.67199898e-03, 1.38184754e-02,\n",
       "        7.01324382e-07, 8.49202115e-06],\n",
       "       [1.76666677e-02, 5.01239263e-02, 1.72603037e-02, 7.82208145e-03,\n",
       "        4.96349437e-03, 4.25766979e-04, 1.32364467e-01, 4.50621452e-03,\n",
       "        4.30569722e-04, 5.18758642e-03, 5.05368807e-04, 1.44570060e-02,\n",
       "        2.51201200e-05, 1.11209236e-04, 6.09928727e-01, 6.04814049e-05,\n",
       "        3.44150909e-03, 1.10126188e-04, 2.74924142e-03, 5.34284569e-04,\n",
       "        2.79129483e-04, 8.57753959e-03, 1.13559321e-01, 3.12154385e-04,\n",
       "        2.10299098e-04, 4.38754959e-03],\n",
       "       [1.63532405e-07, 1.26705650e-04, 1.65669462e-05, 2.77476956e-05,\n",
       "        1.38681993e-04, 4.24198806e-01, 5.05375101e-06, 5.74157930e-08,\n",
       "        1.60594076e-01, 2.48591322e-03, 2.11452105e-04, 1.50391443e-07,\n",
       "        6.50221295e-07, 7.88833859e-07, 1.51141130e-05, 3.77501875e-01,\n",
       "        3.12309112e-07, 8.64095418e-05, 3.01637338e-04, 2.93068085e-02,\n",
       "        2.16878226e-09, 5.53233349e-06, 5.61399929e-07, 2.40985129e-04,\n",
       "        1.42475171e-03, 3.30916583e-03],\n",
       "       [2.92467582e-03, 1.24334037e-01, 7.05589354e-03, 2.59660697e-03,\n",
       "        1.09254956e-01, 1.74644624e-03, 2.40000725e-01, 2.09830119e-04,\n",
       "        7.59530449e-05, 6.61367225e-03, 4.79407748e-03, 2.11009122e-02,\n",
       "        2.28458157e-05, 7.82708055e-04, 1.70199543e-01, 3.70755966e-04,\n",
       "        4.09459043e-03, 2.12113908e-03, 1.34643097e-03, 7.62629032e-04,\n",
       "        7.83101932e-05, 4.77170646e-02, 2.29526475e-01, 1.37292081e-03,\n",
       "        9.45434498e-04, 1.99513398e-02],\n",
       "       [1.79126502e-07, 4.71935753e-04, 1.85916761e-05, 2.46611817e-05,\n",
       "        1.08426972e-03, 4.52809244e-01, 1.40851671e-05, 3.52473890e-08,\n",
       "        4.76191193e-02, 3.97473108e-03, 2.61741970e-03, 2.38706349e-07,\n",
       "        1.06227833e-06, 6.27983627e-06, 2.14432384e-05, 4.72549886e-01,\n",
       "        5.15316628e-07, 6.51566428e-04, 2.20810689e-04, 8.62713717e-03,\n",
       "        4.52887416e-09, 1.02755075e-05, 1.43114585e-06, 8.03312869e-04,\n",
       "        1.38281239e-03, 7.08905281e-03],\n",
       "       [1.00593825e-04, 4.30379547e-02, 1.74950391e-01, 1.34985805e-06,\n",
       "        2.10272446e-01, 5.16478121e-02, 3.98491800e-01, 8.22941074e-05,\n",
       "        1.56131777e-04, 9.03467153e-05, 2.97492224e-04, 9.44851935e-02,\n",
       "        2.89058335e-05, 5.24686940e-04, 2.72015645e-03, 1.83125201e-03,\n",
       "        2.12684245e-05, 1.95396296e-05, 8.45975650e-04, 1.01627468e-03,\n",
       "        3.97164422e-06, 7.73724460e-04, 4.28032875e-03, 1.88940368e-03,\n",
       "        7.12031833e-05, 1.23594133e-02],\n",
       "       [1.65723941e-06, 8.57038467e-05, 2.86990889e-05, 6.63105398e-04,\n",
       "        2.48278404e-04, 1.05064898e-03, 6.24726817e-04, 1.71129602e-06,\n",
       "        8.49059410e-03, 6.44162064e-03, 6.97388605e-04, 2.86488557e-05,\n",
       "        6.03033982e-07, 4.86721501e-06, 3.26129462e-04, 7.27913948e-03,\n",
       "        2.55603949e-03, 1.07819033e-05, 5.16374074e-02, 8.65116119e-02,\n",
       "        8.54887094e-06, 1.06572267e-03, 3.26467853e-06, 3.39274146e-02,\n",
       "        7.85759568e-01, 1.25461854e-02],\n",
       "       [5.56105794e-03, 2.06830003e-03, 3.03976424e-02, 4.95789649e-08,\n",
       "        1.27103189e-02, 2.13813604e-04, 6.83067501e-01, 3.38927750e-03,\n",
       "        9.47537302e-08, 4.05147557e-06, 5.90477663e-04, 1.81502104e-01,\n",
       "        3.33413621e-03, 3.52905393e-02, 2.26990711e-02, 5.94240191e-07,\n",
       "        8.92845783e-05, 1.60706040e-05, 7.60131516e-04, 1.82295707e-06,\n",
       "        7.55102781e-04, 7.14911439e-04, 1.21749844e-02, 4.52015735e-03,\n",
       "        9.40799055e-07, 1.37480863e-04],\n",
       "       [2.57168319e-02, 9.95896086e-02, 1.23150498e-02, 7.45257782e-03,\n",
       "        1.11813406e-02, 1.96872489e-03, 1.31168231e-01, 1.55086927e-02,\n",
       "        3.20438441e-04, 1.95275038e-03, 4.37313598e-03, 1.04507372e-01,\n",
       "        5.11433755e-04, 1.62867690e-03, 2.91864127e-01, 5.74643549e-04,\n",
       "        1.32452697e-01, 1.49602676e-03, 7.76070356e-02, 1.94469222e-03,\n",
       "        2.89708283e-02, 1.58297215e-02, 1.18116969e-02, 5.37884654e-03,\n",
       "        4.67460230e-03, 9.20023955e-03],\n",
       "       [3.95218842e-03, 5.16735390e-03, 3.55440570e-04, 6.64001709e-06,\n",
       "        8.80510826e-03, 7.68816273e-04, 1.77452378e-02, 6.41688588e-04,\n",
       "        1.32758680e-06, 2.02618219e-04, 1.96061246e-02, 2.66303681e-02,\n",
       "        1.45493969e-02, 6.54449344e-01, 1.68461464e-02, 1.42860845e-05,\n",
       "        3.99449747e-03, 1.46986879e-02, 5.52339130e-04, 2.53552867e-06,\n",
       "        3.72294448e-02, 1.52061313e-01, 5.88092022e-03, 1.57657545e-02,\n",
       "        4.65509729e-05, 2.59401531e-05],\n",
       "       [1.09434512e-03, 3.07252887e-03, 2.43906421e-03, 9.00697894e-03,\n",
       "        1.07470155e-03, 1.43259927e-03, 9.94906528e-04, 8.04133993e-03,\n",
       "        1.35079985e-02, 1.11312922e-02, 1.39497602e-02, 4.00939141e-04,\n",
       "        2.65021101e-02, 2.83031538e-03, 3.23451997e-04, 2.69930493e-02,\n",
       "        9.61808488e-03, 3.64525779e-03, 4.62119162e-01, 2.92896647e-02,\n",
       "        5.87903894e-03, 1.99988950e-03, 7.28984072e-04, 5.44303954e-02,\n",
       "        2.94816673e-01, 1.46775190e-02],\n",
       "       [1.16189457e-02, 3.14466213e-03, 1.71427918e-03, 1.14113390e-02,\n",
       "        2.59674876e-03, 5.08645433e-04, 4.20352165e-03, 2.80451030e-02,\n",
       "        1.49716227e-03, 2.52178181e-02, 9.66886524e-03, 5.62944543e-03,\n",
       "        2.81035490e-02, 7.84512423e-03, 6.37547299e-03, 3.82358627e-03,\n",
       "        1.66617185e-01, 4.16291971e-03, 2.05608428e-01, 6.94867875e-03,\n",
       "        1.35787174e-01, 3.90268490e-02, 3.09540099e-03, 6.49053007e-02,\n",
       "        2.15644211e-01, 6.79952372e-03],\n",
       "       [6.64099332e-07, 1.09397166e-03, 4.05210815e-03, 7.93962158e-07,\n",
       "        9.35727637e-03, 9.12583247e-02, 1.54263927e-02, 7.87019445e-08,\n",
       "        5.03803429e-04, 2.63220078e-04, 5.07956545e-04, 7.18199881e-04,\n",
       "        5.69514259e-06, 2.15502605e-05, 1.78112794e-04, 1.46227837e-01,\n",
       "        5.42538728e-05, 2.21189493e-05, 2.90046912e-03, 1.00775838e-01,\n",
       "        1.09778671e-06, 1.65422913e-04, 1.73048375e-06, 1.83114380e-01,\n",
       "        1.53003454e-01, 2.90345252e-01]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model2.predict(TEST_SET_2D)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a324cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'W'],\n",
       " ['B', 'F'],\n",
       " ['C', 'C'],\n",
       " ['D', 'T'],\n",
       " ['E', 'F'],\n",
       " ['F', 'F'],\n",
       " ['G', 'G'],\n",
       " ['H', 'N'],\n",
       " ['I', 'I'],\n",
       " ['J', 'D'],\n",
       " ['K', 'X'],\n",
       " ['L', 'L'],\n",
       " ['M', 'X'],\n",
       " ['N', 'N'],\n",
       " ['O', 'O'],\n",
       " ['P', 'F'],\n",
       " ['Q', 'G'],\n",
       " ['R', 'P'],\n",
       " ['S', 'G'],\n",
       " ['T', 'Y'],\n",
       " ['U', 'G'],\n",
       " ['V', 'O'],\n",
       " ['W', 'N'],\n",
       " ['X', 'S'],\n",
       " ['Y', 'Y'],\n",
       " ['Z', 'Z']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = []\n",
    "for num in range(len(predicted)):\n",
    "    max_val = 0\n",
    "    actual = chr(num+ord('A'))\n",
    "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    result_list.append([actual, predict])\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "692270c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images correctly classified: 34.61538461538461%\n",
      "Here are the list of test images that are misclassified and how they appear:\n",
      "Actual    Predicted\n",
      "A         W\n",
      "B         F\n",
      "D         T\n",
      "E         F\n",
      "H         N\n",
      "J         D\n",
      "K         X\n",
      "M         X\n",
      "P         F\n",
      "Q         G\n",
      "R         P\n",
      "S         G\n",
      "T         Y\n",
      "U         G\n",
      "V         O\n",
      "W         N\n",
      "X         S\n"
     ]
    }
   ],
   "source": [
    "list_misclassified = []\n",
    "num_total, num_correct = 26, 26\n",
    "for i in range(len(result_list)):\n",
    "    if result_list[i][0] != result_list[i][1]:\n",
    "        num_correct -= 1\n",
    "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
    "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_misclassified) == 0:\n",
    "    print(\"All test images are classified correctly.\")\n",
    "else:\n",
    "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_misclassified)):\n",
    "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a41d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Expected letter:\n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "    ####  ####  \n",
      "    ####  ####  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##########      \n",
      "##########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "##########      \n",
      "##########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##############\n",
      "  ##############\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##############  \n",
      "##############  \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ##  ##    ##  \n",
      "  ##  ##    ##  \n",
      "  ##    ##  ##  \n",
      "  ##    ##  ##  \n",
      "  ##      ####  \n",
      "  ##      ####  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "      ########  \n",
      "      ########  \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ########      \n",
      "  ########      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "######    ####  \n",
      "######    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "######  ######  \n",
      "######  ######  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "########        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####  ######    \n",
      "####  ######    \n",
      "  ########      \n",
      "  ########      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "######          \n",
      "######          \n",
      "  ######        \n",
      "  ######        \n",
      "      ######    \n",
      "      ######    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "##  ####  ##    \n",
      "##  ####  ##    \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "      ####      \n",
      "      ####      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "##############  \n",
      "##############  \n",
      "######  ######  \n",
      "######  ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ##  ##    ##  \n",
      "  ##  ##    ##  \n",
      "  ##    ##  ##  \n",
      "  ##    ##  ##  \n",
      "  ##      ####  \n",
      "  ##      ####  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##            \n",
      "  ##            \n",
      "    ########    \n",
      "    ########    \n",
      "            ##  \n",
      "            ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Use show() to display the misclassified images.\n",
    "for j in range(len(list_misclassified)):\n",
    "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
    "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
    "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
    "    print(\"================================================\")\n",
    "    print(\"Expected letter:\")\n",
    "    show(TRAINING_SET_2D[letter_train])\n",
    "    print(\"Predicted letter:\")\n",
    "    show(TEST_SET_2D[letter_test])\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66e46ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.26371878e-06, 2.43999428e-04, 7.32421468e-05, 9.24711057e-04,\n",
       "        5.12584927e-04, 1.45294867e-03, 1.17752049e-03, 3.52159577e-06,\n",
       "        6.62345532e-03, 8.46040994e-03, 9.45355278e-04, 6.88487344e-05,\n",
       "        1.21624305e-06, 7.32820035e-06, 4.98017354e-04, 9.86004341e-03,\n",
       "        3.73866525e-03, 1.87630540e-05, 6.18676059e-02, 8.27755108e-02,\n",
       "        1.48680720e-05, 1.35409180e-03, 7.74364344e-06, 3.54631320e-02,\n",
       "        7.60681808e-01, 2.32213195e-02],\n",
       "       [4.21405211e-03, 2.58969865e-03, 5.90583601e-04, 3.00789852e-05,\n",
       "        1.54880524e-01, 4.28789266e-04, 7.67880352e-04, 2.02361820e-03,\n",
       "        1.09816267e-06, 3.14366771e-03, 1.08918756e-01, 5.64739632e-04,\n",
       "        1.00275844e-01, 5.27045012e-01, 5.91238437e-04, 8.13277366e-05,\n",
       "        2.18560541e-04, 5.15160896e-02, 1.84463061e-04, 3.40461565e-06,\n",
       "        2.00900118e-04, 1.91272644e-03, 3.32671069e-02, 6.50847191e-03,\n",
       "        5.83125529e-06, 3.53736323e-05],\n",
       "       [1.72948494e-05, 7.35302782e-03, 2.07077549e-03, 1.69083492e-07,\n",
       "        9.02289331e-01, 7.08377436e-02, 3.51865683e-03, 8.88839452e-07,\n",
       "        2.16173771e-06, 6.74105366e-04, 2.57537980e-03, 5.33032417e-03,\n",
       "        2.49384266e-05, 3.33296863e-04, 6.59441212e-05, 6.30156253e-04,\n",
       "        1.00989075e-06, 1.60207157e-04, 8.70535769e-06, 2.50171928e-04,\n",
       "        3.36555583e-09, 6.43985259e-05, 5.22023707e-04, 2.66447477e-03,\n",
       "        2.43168938e-06, 6.02399290e-04],\n",
       "       [6.48606033e-07, 4.95490618e-04, 1.76259317e-04, 2.51799150e-07,\n",
       "        3.71154398e-02, 9.34432924e-01, 1.60907060e-04, 8.57876685e-08,\n",
       "        1.14884821e-03, 2.71070516e-03, 2.26382515e-03, 2.36821052e-05,\n",
       "        3.10257337e-06, 2.41193429e-05, 5.59280561e-05, 1.63251404e-02,\n",
       "        3.07140709e-07, 1.91310872e-04, 1.75177993e-05, 1.44812430e-03,\n",
       "        7.27969740e-10, 2.12386276e-05, 1.03236125e-05, 2.27237353e-03,\n",
       "        1.15461407e-05, 1.08978059e-03],\n",
       "       [1.46985792e-08, 2.05974739e-08, 2.80408273e-07, 9.40916925e-06,\n",
       "        3.56861415e-06, 1.02828105e-03, 1.63770244e-06, 2.07609130e-09,\n",
       "        9.51772630e-01, 1.27524778e-03, 3.47151763e-05, 3.76493459e-09,\n",
       "        4.52809212e-09, 2.64424727e-07, 1.55968132e-06, 1.50393302e-04,\n",
       "        2.27491537e-08, 8.19361361e-08, 1.60026888e-04, 3.81208248e-02,\n",
       "        2.76202602e-11, 4.36033179e-06, 2.30990267e-08, 7.81996350e-04,\n",
       "        6.47648331e-03, 1.78152637e-04],\n",
       "       [3.12758880e-06, 5.89443339e-08, 5.29173105e-09, 6.81410438e-06,\n",
       "        1.17556745e-07, 2.00124539e-09, 8.40082123e-07, 3.61995662e-05,\n",
       "        1.31260521e-07, 2.03744275e-04, 2.69166549e-07, 1.91675014e-07,\n",
       "        4.72088090e-07, 1.16707849e-06, 3.08738745e-05, 1.05339453e-08,\n",
       "        3.04874987e-03, 3.20420736e-06, 1.61381555e-04, 5.37562030e-08,\n",
       "        3.15865059e-03, 9.93188262e-01, 1.29191149e-05, 2.47049211e-05,\n",
       "        1.18158816e-04, 4.35889298e-08],\n",
       "       [8.70464163e-08, 1.46127331e-05, 1.77247392e-04, 2.14673541e-08,\n",
       "        3.23590748e-02, 8.97835076e-01, 1.41307013e-03, 2.12136819e-09,\n",
       "        1.06382677e-02, 1.34357682e-03, 2.55677314e-03, 3.74191040e-06,\n",
       "        2.41990534e-07, 5.83269248e-05, 2.86266641e-05, 1.17605906e-02,\n",
       "        2.05640340e-08, 6.91589594e-06, 2.86064133e-05, 1.72050986e-02,\n",
       "        1.21273672e-10, 9.10170456e-06, 1.98704060e-06, 1.88200083e-02,\n",
       "        1.86639576e-04, 5.55226160e-03],\n",
       "       [1.04802848e-05, 9.83462453e-01, 1.30887318e-03, 1.13057892e-03,\n",
       "        4.91557003e-04, 2.10398084e-05, 7.40774994e-05, 6.93912152e-05,\n",
       "        3.35635605e-07, 1.72607379e-05, 4.22502751e-04, 4.80430244e-05,\n",
       "        4.17031006e-05, 3.54585882e-06, 3.98989796e-05, 1.00010925e-03,\n",
       "        2.61665031e-04, 4.10081120e-03, 3.42927384e-03, 1.03363573e-05,\n",
       "        7.17062994e-06, 5.97691042e-06, 1.48029730e-03, 2.23353595e-06,\n",
       "        4.25005273e-05, 2.51802616e-03],\n",
       "       [2.60208966e-03, 4.02413681e-03, 1.89367900e-04, 4.26645602e-05,\n",
       "        6.98835720e-06, 1.38660559e-08, 9.16813035e-03, 5.42774564e-03,\n",
       "        1.89225879e-09, 4.72589045e-06, 2.93574503e-05, 3.88343382e-04,\n",
       "        5.14794056e-06, 3.97591884e-06, 8.90571356e-01, 8.06418754e-08,\n",
       "        4.44753580e-02, 1.44537644e-05, 3.12668248e-03, 9.43522167e-08,\n",
       "        3.33374776e-02, 2.69167853e-04, 6.16456708e-03, 8.42744612e-06,\n",
       "        4.06253821e-06, 1.35610069e-04],\n",
       "       [1.60947687e-03, 3.71902413e-03, 3.14883236e-03, 9.25982837e-03,\n",
       "        1.57178857e-03, 1.99560821e-03, 1.19239313e-03, 1.13122808e-02,\n",
       "        1.19872922e-02, 1.32621573e-02, 1.52620869e-02, 5.54562488e-04,\n",
       "        4.74302433e-02, 3.77537869e-03, 3.50104907e-04, 3.01156510e-02,\n",
       "        1.08922264e-02, 5.19162370e-03, 4.38144207e-01, 2.41957214e-02,\n",
       "        7.76263699e-03, 2.58650677e-03, 8.83514993e-04, 7.18232021e-02,\n",
       "        2.66711414e-01, 1.52621670e-02],\n",
       "       [6.20787366e-10, 1.93856451e-08, 3.52349645e-07, 2.83296430e-07,\n",
       "        5.63061178e-07, 1.78078413e-02, 6.94184564e-07, 5.44860164e-11,\n",
       "        9.28424954e-01, 2.39809342e-05, 2.03802915e-06, 1.16879739e-09,\n",
       "        9.82475559e-11, 1.33783455e-08, 5.51465916e-07, 1.00361678e-04,\n",
       "        6.59678756e-10, 4.33778302e-09, 4.16008806e-05, 5.32391742e-02,\n",
       "        3.63071823e-13, 5.68459058e-08, 6.33744113e-10, 1.48637664e-05,\n",
       "        2.21305119e-04, 1.21344565e-04],\n",
       "       [3.33103933e-03, 1.25395876e-04, 3.46046072e-05, 8.08128888e-08,\n",
       "        2.51849112e-03, 1.29732064e-06, 1.72876738e-04, 1.61900173e-03,\n",
       "        2.23574381e-09, 4.94844062e-05, 7.22394977e-03, 1.42539275e-05,\n",
       "        2.45941743e-01, 7.27781057e-01, 3.31106676e-05, 4.74298815e-07,\n",
       "        1.40156881e-05, 4.66772530e-04, 7.08031439e-05, 2.28997799e-08,\n",
       "        8.49050528e-04, 1.79966126e-04, 2.72304192e-03, 6.84653968e-03,\n",
       "        2.03811669e-06, 9.00504006e-07],\n",
       "       [9.48791012e-07, 2.53458216e-04, 1.77274853e-01, 5.28415636e-13,\n",
       "        6.92663540e-04, 9.87127805e-07, 8.16320240e-01, 9.13385520e-07,\n",
       "        3.54156079e-11, 9.18728871e-10, 3.16069482e-08, 4.50249203e-03,\n",
       "        8.61014371e-09, 2.12013151e-06, 3.15782381e-04, 8.84053830e-10,\n",
       "        7.71013420e-10, 1.16928698e-11, 1.10155361e-06, 9.72737890e-09,\n",
       "        3.69778408e-09, 4.13111536e-08, 5.96786267e-04, 2.53039912e-06,\n",
       "        5.27698329e-10, 3.50304945e-05],\n",
       "       [1.14993984e-02, 4.32625056e-05, 4.36278133e-05, 3.34714400e-07,\n",
       "        9.61589278e-04, 1.41017409e-09, 6.31543226e-04, 1.92292929e-02,\n",
       "        5.61429939e-12, 4.01836987e-05, 1.95417157e-03, 1.43025522e-04,\n",
       "        8.24978866e-04, 1.17359422e-02, 5.89209516e-03, 1.97997441e-09,\n",
       "        6.15679237e-05, 4.44287652e-05, 3.24619978e-05, 2.40869352e-10,\n",
       "        3.19845625e-04, 5.35508079e-05, 9.46463346e-01, 2.39821093e-05,\n",
       "        1.49839732e-08, 1.49139305e-06],\n",
       "       [5.30649480e-08, 3.48685489e-08, 2.12613529e-07, 5.40220317e-05,\n",
       "        3.82923599e-06, 3.78540048e-04, 4.01877651e-06, 1.00359570e-08,\n",
       "        9.53360558e-01, 1.43877242e-03, 4.07081061e-05, 1.37668295e-08,\n",
       "        5.55543123e-09, 4.16127421e-07, 3.77595711e-06, 9.57298980e-05,\n",
       "        1.45778884e-07, 9.12389666e-08, 3.90387140e-04, 3.11319474e-02,\n",
       "        1.77428100e-10, 1.49038242e-05, 6.68851641e-08, 7.78283400e-04,\n",
       "        1.21815437e-02, 1.22047772e-04],\n",
       "       [2.01970420e-06, 2.78133433e-03, 1.07275210e-02, 1.20412653e-06,\n",
       "        3.34238261e-02, 1.08790435e-01, 3.32439914e-02, 2.19649607e-07,\n",
       "        4.23800608e-04, 4.32886736e-04, 8.26852396e-04, 1.86161767e-03,\n",
       "        1.40385109e-05, 6.18549238e-05, 2.66491668e-04, 1.32320642e-01,\n",
       "        5.91051321e-05, 3.51479794e-05, 3.14179785e-03, 7.35863820e-02,\n",
       "        1.58635180e-06, 2.57796462e-04, 7.51765674e-06, 1.89060524e-01,\n",
       "        1.01703562e-01, 3.06967825e-01],\n",
       "       [9.39972281e-01, 6.93152178e-05, 5.67880306e-07, 1.36089890e-04,\n",
       "        3.41263949e-05, 8.15710788e-09, 5.54674887e-04, 2.01772377e-02,\n",
       "        1.03727986e-08, 1.38854712e-05, 1.34585876e-04, 2.12626765e-05,\n",
       "        9.68120876e-05, 1.20573223e-03, 1.72039568e-02, 3.99768774e-10,\n",
       "        1.61537857e-04, 4.38592106e-05, 1.14722192e-04, 2.24101715e-09,\n",
       "        2.59986008e-03, 8.16432294e-04, 1.66292526e-02, 1.34939364e-05,\n",
       "        1.67345448e-07, 6.46523688e-08],\n",
       "       [1.16032695e-07, 4.45960351e-04, 2.37034510e-05, 2.11094248e-05,\n",
       "        8.83062079e-04, 3.20687294e-01, 1.82088152e-05, 2.38463915e-08,\n",
       "        4.25647311e-02, 3.81869194e-03, 3.38806864e-03, 1.82823300e-07,\n",
       "        8.35380547e-07, 4.63930155e-06, 2.11328097e-05, 5.97060621e-01,\n",
       "        4.64002454e-07, 6.57873985e-04, 2.24461925e-04, 1.22748502e-02,\n",
       "        3.51274343e-09, 6.84657016e-06, 1.50217863e-06, 1.00460905e-03,\n",
       "        2.19417899e-03, 1.46968001e-02],\n",
       "       [2.61667064e-05, 1.17682125e-02, 4.82546445e-03, 5.96783198e-02,\n",
       "        2.93164718e-04, 1.13073643e-02, 1.84927194e-03, 1.31001761e-05,\n",
       "        1.07802801e-01, 9.21032671e-03, 5.32787410e-04, 1.94318462e-04,\n",
       "        9.28106942e-07, 1.16141450e-06, 3.03605106e-03, 7.51591995e-02,\n",
       "        8.69989599e-05, 3.42510815e-04, 1.54521484e-02, 2.59173632e-01,\n",
       "        4.27830543e-07, 9.00468876e-05, 5.24771167e-04, 1.51556989e-04,\n",
       "        4.03376333e-02, 3.98141682e-01],\n",
       "       [2.94969068e-05, 1.87738508e-03, 4.99774505e-05, 3.49007989e-03,\n",
       "        7.74186537e-06, 5.72495537e-07, 5.00615744e-04, 6.06951886e-04,\n",
       "        9.00483701e-06, 1.16917363e-04, 5.31084312e-04, 1.42507532e-04,\n",
       "        5.75376760e-07, 6.55622443e-07, 5.39339194e-03, 8.23850496e-05,\n",
       "        1.55804576e-02, 7.10624299e-05, 9.63825822e-01, 7.92807841e-05,\n",
       "        7.12055364e-04, 4.43030767e-05, 1.97481306e-04, 7.17580187e-05,\n",
       "        3.81103042e-03, 2.76735588e-03],\n",
       "       [1.22479651e-06, 2.27469755e-07, 3.28001870e-09, 2.15069856e-03,\n",
       "        6.43715339e-07, 4.68044306e-08, 1.37919514e-07, 1.70502790e-06,\n",
       "        1.52980778e-04, 9.96246636e-01, 1.40057164e-04, 2.90429121e-08,\n",
       "        7.90453214e-09, 4.35443077e-08, 3.67818167e-04, 3.63988249e-07,\n",
       "        4.54302426e-05, 9.90085755e-06, 8.91060263e-05, 4.87729210e-07,\n",
       "        3.78931198e-08, 6.14859106e-04, 2.78340303e-05, 2.83441623e-05,\n",
       "        1.18860669e-04, 2.68650956e-06],\n",
       "       [1.19801400e-04, 4.98746158e-06, 1.06658220e-07, 1.94857080e-06,\n",
       "        2.22886563e-08, 3.18972583e-12, 5.49213041e-07, 3.46839093e-02,\n",
       "        1.36910856e-11, 2.38526241e-07, 5.12859835e-08, 3.51345034e-06,\n",
       "        3.40139668e-04, 8.07553477e-07, 1.89725051e-06, 3.70901287e-09,\n",
       "        1.24577917e-02, 4.18420569e-07, 2.22383160e-03, 5.29855326e-10,\n",
       "        9.49727535e-01, 3.88156186e-04, 2.55555715e-05, 7.54836094e-07,\n",
       "        1.79178878e-05, 8.96964210e-08],\n",
       "       [2.54246726e-04, 1.36094886e-05, 1.95595271e-06, 6.36564890e-09,\n",
       "        3.85772495e-04, 3.20354876e-09, 7.13878194e-07, 3.87076125e-03,\n",
       "        8.78891404e-12, 4.99766384e-06, 1.25887431e-03, 1.44474200e-07,\n",
       "        9.60545838e-01, 3.26147936e-02, 5.40517995e-08, 5.17294865e-08,\n",
       "        1.65731581e-06, 6.88397195e-05, 7.05378334e-05, 1.62708708e-10,\n",
       "        5.15591564e-05, 7.03692535e-07, 4.20276599e-04, 4.34677728e-04,\n",
       "        8.82107614e-08, 2.30736052e-08],\n",
       "       [6.05961930e-07, 7.46120512e-03, 4.30109794e-04, 8.72977325e-05,\n",
       "        3.54699703e-04, 2.17407383e-02, 1.50377409e-05, 1.34195670e-06,\n",
       "        1.29133277e-03, 7.66193320e-04, 3.63840139e-04, 4.66298616e-06,\n",
       "        1.34961438e-05, 3.04323351e-07, 7.65663299e-06, 9.21803474e-01,\n",
       "        5.03686806e-06, 1.46073208e-03, 4.94843756e-04, 9.06064082e-03,\n",
       "        4.27149836e-08, 2.69124007e-06, 8.19540219e-06, 1.87240817e-04,\n",
       "        1.64467748e-03, 3.27938646e-02],\n",
       "       [8.37987391e-05, 1.32597910e-04, 7.38289785e-08, 2.15293956e-03,\n",
       "        1.79528001e-08, 9.23893340e-12, 1.08019558e-05, 1.34289195e-03,\n",
       "        6.55686672e-10, 1.17187374e-05, 1.26671648e-06, 3.00290344e-06,\n",
       "        4.31847234e-08, 1.95401983e-09, 4.06891368e-02, 2.41736178e-08,\n",
       "        9.18483317e-01, 1.93610776e-06, 3.72296828e-03, 1.63010032e-08,\n",
       "        3.30556035e-02, 8.71827870e-05, 1.80861025e-04, 6.95899232e-08,\n",
       "        2.90580047e-05, 1.08069016e-05],\n",
       "       [2.08611619e-02, 8.39900400e-04, 2.98621690e-05, 8.12070866e-05,\n",
       "        1.06485168e-05, 5.71977044e-09, 1.27993023e-03, 7.67893717e-02,\n",
       "        1.24970767e-09, 2.77794970e-05, 1.71040010e-04, 2.17432753e-04,\n",
       "        5.45284129e-04, 3.36880155e-04, 1.51444256e-01, 5.75281547e-08,\n",
       "        6.19789287e-02, 1.53575806e-04, 2.98278872e-03, 1.83529050e-08,\n",
       "        6.49050474e-01, 3.46255489e-03, 2.96881553e-02, 3.19950523e-05,\n",
       "        5.78616982e-06, 1.09354314e-05],\n",
       "       [3.78589965e-10, 1.06749045e-08, 3.37615148e-07, 2.06462076e-07,\n",
       "        4.64704556e-07, 1.15195792e-02, 5.91623404e-07, 3.58173734e-11,\n",
       "        9.25698876e-01, 1.61731896e-05, 1.14449574e-06, 9.09793851e-10,\n",
       "        6.03899847e-11, 9.57357837e-09, 2.79960602e-07, 9.08558941e-05,\n",
       "        3.83685167e-10, 2.00952344e-09, 3.81209647e-05, 6.23208992e-02,\n",
       "        2.38132051e-13, 3.70257212e-08, 4.86405749e-10, 9.73397619e-06,\n",
       "        2.08084501e-04, 9.45670399e-05],\n",
       "       [5.78502011e-07, 5.13718172e-04, 8.78450990e-01, 3.40571252e-11,\n",
       "        2.48452707e-04, 1.40748978e-06, 1.15853883e-01, 1.63709478e-06,\n",
       "        2.31057881e-10, 4.53725546e-09, 7.08177206e-09, 3.84108420e-03,\n",
       "        9.90073534e-09, 6.26538821e-08, 1.69966195e-04, 2.64222138e-08,\n",
       "        8.66826610e-09, 8.26389721e-11, 2.92651134e-06, 2.58058719e-07,\n",
       "        9.57668522e-09, 7.24606579e-08, 1.83340511e-04, 1.41624037e-06,\n",
       "        1.17138992e-08, 7.30140775e-04],\n",
       "       [2.33150258e-05, 2.42222421e-04, 1.39910890e-05, 1.29774226e-05,\n",
       "        1.84109644e-03, 7.46301084e-04, 1.42965127e-05, 9.21810170e-06,\n",
       "        9.72334383e-05, 2.46548676e-03, 8.82303178e-01, 4.57240219e-07,\n",
       "        8.33558035e-04, 1.66791119e-03, 8.62204906e-05, 2.50895479e-04,\n",
       "        1.31305342e-05, 8.26289803e-02, 3.44577187e-04, 6.49019639e-05,\n",
       "        1.51593284e-07, 3.56544551e-05, 9.26418070e-05, 2.60712933e-02,\n",
       "        3.62103347e-05, 1.03997801e-04],\n",
       "       [7.88192847e-06, 3.90554022e-04, 2.07976718e-03, 2.13788348e-10,\n",
       "        4.89617698e-03, 3.80887031e-05, 1.26217119e-02, 7.10198037e-07,\n",
       "        2.08434728e-11, 4.83218372e-08, 1.16812757e-07, 9.79538977e-01,\n",
       "        1.47923163e-06, 2.51453421e-05, 9.72898579e-06, 1.12906040e-08,\n",
       "        2.68997216e-08, 1.38196139e-08, 1.04423464e-07, 2.84198975e-08,\n",
       "        5.73311709e-09, 4.85641112e-06, 3.79724079e-04, 2.65588869e-06,\n",
       "        1.20526178e-09, 2.24819473e-06],\n",
       "       [1.30368951e-08, 1.57809211e-06, 6.30537079e-06, 4.91907394e-06,\n",
       "        2.12488794e-06, 1.24202579e-05, 2.35551602e-06, 4.04317575e-08,\n",
       "        4.46916121e-04, 1.13733047e-04, 9.51151087e-05, 6.97938916e-08,\n",
       "        4.00704863e-07, 7.29121780e-07, 1.69599886e-07, 4.16999171e-03,\n",
       "        2.35279708e-06, 9.48033005e-07, 3.26862652e-03, 2.90075913e-02,\n",
       "        8.66829737e-08, 2.23215352e-06, 5.49382833e-08, 6.84329029e-03,\n",
       "        9.47229743e-01, 8.78822431e-03]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_message = model2.predict(MESSAGE_2D)\n",
    "predicted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dceac894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YNEFIVFBOSINGWIZAPZSJUMPQUICKLY\n"
     ]
    }
   ],
   "source": [
    "message_list = \"\"\n",
    "for num in range(len(predicted_message)):\n",
    "    max_val = 0\n",
    "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    message_list += predict\n",
    "print(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f376a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy between predicted and actual message: 80.64516129032258%\n",
      "Here is the list of letters that are a mismatch between predicted and actual message:\n",
      "Actual    Predicted\n",
      "T         Y\n",
      "H         N\n",
      "E         F\n",
      "X         S\n",
      "R         P\n",
      "D         Z\n"
     ]
    }
   ],
   "source": [
    "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
    "list_mismatch = []\n",
    "num_total, num_correct = len(actual_message), len(message_list)\n",
    "for i in range(len(actual_message)):\n",
    "    if message_list[i] != actual_message[i]:\n",
    "        num_correct -= 1\n",
    "        list_mismatch.append([actual_message[i], message_list[i]])\n",
    "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_mismatch) == 0:\n",
    "    print(\"The message appears to be decoded correctly.\")\n",
    "else:\n",
    "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_mismatch)):\n",
    "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d3876",
   "metadata": {},
   "source": [
    "# Does the performance improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aba682",
   "metadata": {},
   "source": [
    "## A: No, the performance does not improve simply by adding additional hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515d356",
   "metadata": {},
   "source": [
    "# 8. Repeat experiment (7), adding additional layers of the same size until the message is decoded correctly. What results do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3456a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
      "                                                                 \n",
      " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer4 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer5 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer6 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer7 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer8 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer9 (Dense)       (None, 32)                1056      \n",
      "                                                                 \n",
      " Hidden_layer10 (Dense)      (None, 32)                1056      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,586\n",
      "Trainable params: 18,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Do number 7 again, except add as many layers as possible until the message can be decoded correctly.\n",
    "model3 = tf.keras.models.Sequential()\n",
    "model3.add(tf.keras.Input(shape=(256,)))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer4\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer5\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer6\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer7\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer8\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer9\"))\n",
    "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer10\"))\n",
    "model3.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8121c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/99999\n",
      "2/2 - 1s - loss: 3.2588 - categorical_accuracy: 0.0577 - 539ms/epoch - 269ms/step\n",
      "Epoch 2/99999\n",
      "2/2 - 0s - loss: 3.2531 - categorical_accuracy: 0.0769 - 5ms/epoch - 3ms/step\n",
      "Epoch 3/99999\n",
      "2/2 - 0s - loss: 3.2485 - categorical_accuracy: 0.0577 - 4ms/epoch - 2ms/step\n",
      "Epoch 4/99999\n",
      "2/2 - 0s - loss: 3.2441 - categorical_accuracy: 0.0962 - 4ms/epoch - 2ms/step\n",
      "Epoch 5/99999\n",
      "2/2 - 0s - loss: 3.2383 - categorical_accuracy: 0.0769 - 4ms/epoch - 2ms/step\n",
      "Epoch 6/99999\n",
      "2/2 - 0s - loss: 3.2304 - categorical_accuracy: 0.0577 - 4ms/epoch - 2ms/step\n",
      "Epoch 7/99999\n",
      "2/2 - 0s - loss: 3.2229 - categorical_accuracy: 0.0577 - 5ms/epoch - 2ms/step\n",
      "Epoch 8/99999\n",
      "2/2 - 0s - loss: 3.2126 - categorical_accuracy: 0.0577 - 5ms/epoch - 3ms/step\n",
      "Epoch 9/99999\n",
      "2/2 - 0s - loss: 3.1999 - categorical_accuracy: 0.0577 - 8ms/epoch - 4ms/step\n",
      "Epoch 10/99999\n",
      "2/2 - 0s - loss: 3.1863 - categorical_accuracy: 0.0769 - 4ms/epoch - 2ms/step\n",
      "Epoch 11/99999\n",
      "2/2 - 0s - loss: 3.1693 - categorical_accuracy: 0.0769 - 11ms/epoch - 5ms/step\n",
      "Epoch 12/99999\n",
      "2/2 - 0s - loss: 3.1470 - categorical_accuracy: 0.0769 - 11ms/epoch - 6ms/step\n",
      "Epoch 13/99999\n",
      "2/2 - 0s - loss: 3.1205 - categorical_accuracy: 0.0962 - 13ms/epoch - 7ms/step\n",
      "Epoch 14/99999\n",
      "2/2 - 0s - loss: 3.0900 - categorical_accuracy: 0.1346 - 5ms/epoch - 2ms/step\n",
      "Epoch 15/99999\n",
      "2/2 - 0s - loss: 3.0542 - categorical_accuracy: 0.1538 - 8ms/epoch - 4ms/step\n",
      "Epoch 16/99999\n",
      "2/2 - 0s - loss: 3.0152 - categorical_accuracy: 0.1346 - 5ms/epoch - 2ms/step\n",
      "Epoch 17/99999\n",
      "2/2 - 0s - loss: 2.9641 - categorical_accuracy: 0.1154 - 4ms/epoch - 2ms/step\n",
      "Epoch 18/99999\n",
      "2/2 - 0s - loss: 2.9076 - categorical_accuracy: 0.1346 - 5ms/epoch - 2ms/step\n",
      "Epoch 19/99999\n",
      "2/2 - 0s - loss: 2.8498 - categorical_accuracy: 0.1538 - 5ms/epoch - 3ms/step\n",
      "Epoch 20/99999\n",
      "2/2 - 0s - loss: 2.7807 - categorical_accuracy: 0.1538 - 3ms/epoch - 2ms/step\n",
      "Epoch 21/99999\n",
      "2/2 - 0s - loss: 2.7077 - categorical_accuracy: 0.1538 - 7ms/epoch - 4ms/step\n",
      "Epoch 22/99999\n",
      "2/2 - 0s - loss: 2.6408 - categorical_accuracy: 0.1731 - 4ms/epoch - 2ms/step\n",
      "Epoch 23/99999\n",
      "2/2 - 0s - loss: 2.5592 - categorical_accuracy: 0.1731 - 6ms/epoch - 3ms/step\n",
      "Epoch 24/99999\n",
      "2/2 - 0s - loss: 2.4759 - categorical_accuracy: 0.1538 - 3ms/epoch - 2ms/step\n",
      "Epoch 25/99999\n",
      "2/2 - 0s - loss: 2.3884 - categorical_accuracy: 0.2308 - 3ms/epoch - 2ms/step\n",
      "Epoch 26/99999\n",
      "2/2 - 0s - loss: 2.3154 - categorical_accuracy: 0.2500 - 6ms/epoch - 3ms/step\n",
      "Epoch 27/99999\n",
      "2/2 - 0s - loss: 2.2123 - categorical_accuracy: 0.3077 - 4ms/epoch - 2ms/step\n",
      "Epoch 28/99999\n",
      "2/2 - 0s - loss: 2.1195 - categorical_accuracy: 0.3462 - 7ms/epoch - 3ms/step\n",
      "Epoch 29/99999\n",
      "2/2 - 0s - loss: 2.0611 - categorical_accuracy: 0.4038 - 3ms/epoch - 2ms/step\n",
      "Epoch 30/99999\n",
      "2/2 - 0s - loss: 1.9531 - categorical_accuracy: 0.4231 - 5ms/epoch - 3ms/step\n",
      "Epoch 31/99999\n",
      "2/2 - 0s - loss: 1.8689 - categorical_accuracy: 0.4231 - 4ms/epoch - 2ms/step\n",
      "Epoch 32/99999\n",
      "2/2 - 0s - loss: 1.7649 - categorical_accuracy: 0.4231 - 3ms/epoch - 2ms/step\n",
      "Epoch 33/99999\n",
      "2/2 - 0s - loss: 1.7313 - categorical_accuracy: 0.4038 - 6ms/epoch - 3ms/step\n",
      "Epoch 34/99999\n",
      "2/2 - 0s - loss: 1.6413 - categorical_accuracy: 0.4808 - 3ms/epoch - 2ms/step\n",
      "Epoch 35/99999\n",
      "2/2 - 0s - loss: 1.5391 - categorical_accuracy: 0.5000 - 4ms/epoch - 2ms/step\n",
      "Epoch 36/99999\n",
      "2/2 - 0s - loss: 1.4335 - categorical_accuracy: 0.5577 - 4ms/epoch - 2ms/step\n",
      "Epoch 37/99999\n",
      "2/2 - 0s - loss: 1.3946 - categorical_accuracy: 0.5577 - 4ms/epoch - 2ms/step\n",
      "Epoch 38/99999\n",
      "2/2 - 0s - loss: 1.2784 - categorical_accuracy: 0.5962 - 6ms/epoch - 3ms/step\n",
      "Epoch 39/99999\n",
      "2/2 - 0s - loss: 1.2206 - categorical_accuracy: 0.6346 - 4ms/epoch - 2ms/step\n",
      "Epoch 40/99999\n",
      "2/2 - 0s - loss: 1.1207 - categorical_accuracy: 0.6538 - 4ms/epoch - 2ms/step\n",
      "Epoch 41/99999\n",
      "2/2 - 0s - loss: 1.0458 - categorical_accuracy: 0.8077 - 6ms/epoch - 3ms/step\n",
      "Epoch 42/99999\n",
      "2/2 - 0s - loss: 0.9748 - categorical_accuracy: 0.7500 - 4ms/epoch - 2ms/step\n",
      "Epoch 43/99999\n",
      "2/2 - 0s - loss: 0.8865 - categorical_accuracy: 0.8269 - 5ms/epoch - 2ms/step\n",
      "Epoch 44/99999\n",
      "2/2 - 0s - loss: 0.8266 - categorical_accuracy: 0.8462 - 4ms/epoch - 2ms/step\n",
      "Epoch 45/99999\n",
      "2/2 - 0s - loss: 0.7695 - categorical_accuracy: 0.8462 - 4ms/epoch - 2ms/step\n",
      "Epoch 46/99999\n",
      "2/2 - 0s - loss: 0.6944 - categorical_accuracy: 0.9038 - 3ms/epoch - 2ms/step\n",
      "Epoch 47/99999\n",
      "2/2 - 0s - loss: 0.6582 - categorical_accuracy: 0.8654 - 4ms/epoch - 2ms/step\n",
      "Epoch 48/99999\n",
      "2/2 - 0s - loss: 0.6038 - categorical_accuracy: 0.8846 - 3ms/epoch - 2ms/step\n",
      "Epoch 49/99999\n",
      "2/2 - 0s - loss: 0.5643 - categorical_accuracy: 0.8654 - 4ms/epoch - 2ms/step\n",
      "Epoch 50/99999\n",
      "2/2 - 0s - loss: 0.4965 - categorical_accuracy: 0.9231 - 4ms/epoch - 2ms/step\n",
      "Epoch 51/99999\n",
      "2/2 - 0s - loss: 0.4661 - categorical_accuracy: 0.8846 - 4ms/epoch - 2ms/step\n",
      "Epoch 52/99999\n",
      "2/2 - 0s - loss: 0.4034 - categorical_accuracy: 0.9615 - 4ms/epoch - 2ms/step\n",
      "Epoch 53/99999\n",
      "2/2 - 0s - loss: 0.3895 - categorical_accuracy: 0.9231 - 4ms/epoch - 2ms/step\n",
      "Epoch 54/99999\n",
      "2/2 - 0s - loss: 0.3438 - categorical_accuracy: 0.9231 - 3ms/epoch - 2ms/step\n",
      "Epoch 55/99999\n",
      "2/2 - 0s - loss: 0.3121 - categorical_accuracy: 0.9423 - 4ms/epoch - 2ms/step\n",
      "Epoch 56/99999\n",
      "2/2 - 0s - loss: 0.2812 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 57/99999\n",
      "2/2 - 0s - loss: 0.2569 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 58/99999\n",
      "2/2 - 0s - loss: 0.2356 - categorical_accuracy: 0.9615 - 4ms/epoch - 2ms/step\n",
      "Epoch 59/99999\n",
      "2/2 - 0s - loss: 0.2033 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 60/99999\n",
      "2/2 - 0s - loss: 0.2010 - categorical_accuracy: 0.9808 - 3ms/epoch - 1ms/step\n",
      "Epoch 61/99999\n",
      "2/2 - 0s - loss: 0.1806 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 62/99999\n",
      "2/2 - 0s - loss: 0.1735 - categorical_accuracy: 0.9808 - 5ms/epoch - 2ms/step\n",
      "Epoch 63/99999\n",
      "2/2 - 0s - loss: 0.1474 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 64/99999\n",
      "2/2 - 0s - loss: 0.1402 - categorical_accuracy: 0.9615 - 3ms/epoch - 2ms/step\n",
      "Epoch 65/99999\n",
      "2/2 - 0s - loss: 0.1256 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 66/99999\n",
      "2/2 - 0s - loss: 0.1065 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 67/99999\n",
      "2/2 - 0s - loss: 0.1045 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 68/99999\n",
      "2/2 - 0s - loss: 0.0911 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 69/99999\n",
      "2/2 - 0s - loss: 0.0916 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 70/99999\n",
      "2/2 - 0s - loss: 0.0921 - categorical_accuracy: 0.9615 - 4ms/epoch - 2ms/step\n",
      "Epoch 71/99999\n",
      "2/2 - 0s - loss: 0.0837 - categorical_accuracy: 0.9615 - 4ms/epoch - 2ms/step\n",
      "Epoch 72/99999\n",
      "2/2 - 0s - loss: 0.0613 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 73/99999\n",
      "2/2 - 0s - loss: 0.0729 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 74/99999\n",
      "2/2 - 0s - loss: 0.0562 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 75/99999\n",
      "2/2 - 0s - loss: 0.0586 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 76/99999\n",
      "2/2 - 0s - loss: 0.0473 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 77/99999\n",
      "2/2 - 0s - loss: 0.0511 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 78/99999\n",
      "2/2 - 0s - loss: 0.0470 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 79/99999\n",
      "2/2 - 0s - loss: 0.0428 - categorical_accuracy: 1.0000 - 3ms/epoch - 1ms/step\n",
      "Epoch 80/99999\n",
      "2/2 - 0s - loss: 0.0351 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 81/99999\n",
      "2/2 - 0s - loss: 0.0389 - categorical_accuracy: 0.9808 - 3ms/epoch - 2ms/step\n",
      "Epoch 82/99999\n",
      "2/2 - 0s - loss: 0.0372 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 83/99999\n",
      "2/2 - 0s - loss: 0.0298 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 84/99999\n",
      "2/2 - 0s - loss: 0.0431 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 85/99999\n",
      "2/2 - 0s - loss: 0.0749 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 86/99999\n",
      "2/2 - 0s - loss: 0.2076 - categorical_accuracy: 0.9423 - 3ms/epoch - 2ms/step\n",
      "Epoch 87/99999\n",
      "2/2 - 0s - loss: 0.4475 - categorical_accuracy: 0.8462 - 4ms/epoch - 2ms/step\n",
      "Epoch 88/99999\n",
      "2/2 - 0s - loss: 1.5051 - categorical_accuracy: 0.7500 - 4ms/epoch - 2ms/step\n",
      "Epoch 89/99999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 0.7971 - categorical_accuracy: 0.7885 - 3ms/epoch - 2ms/step\n",
      "Epoch 90/99999\n",
      "2/2 - 0s - loss: 1.8085 - categorical_accuracy: 0.6923 - 4ms/epoch - 2ms/step\n",
      "Epoch 91/99999\n",
      "2/2 - 0s - loss: 1.6313 - categorical_accuracy: 0.5577 - 4ms/epoch - 2ms/step\n",
      "Epoch 92/99999\n",
      "2/2 - 0s - loss: 1.1926 - categorical_accuracy: 0.6346 - 4ms/epoch - 2ms/step\n",
      "Epoch 93/99999\n",
      "2/2 - 0s - loss: 0.4110 - categorical_accuracy: 0.8269 - 4ms/epoch - 2ms/step\n",
      "Epoch 94/99999\n",
      "2/2 - 0s - loss: 0.5492 - categorical_accuracy: 0.7692 - 3ms/epoch - 1ms/step\n",
      "Epoch 95/99999\n",
      "2/2 - 0s - loss: 0.4158 - categorical_accuracy: 0.7885 - 5ms/epoch - 3ms/step\n",
      "Epoch 96/99999\n",
      "2/2 - 0s - loss: 0.2710 - categorical_accuracy: 0.9423 - 6ms/epoch - 3ms/step\n",
      "Epoch 97/99999\n",
      "2/2 - 0s - loss: 0.1950 - categorical_accuracy: 0.9808 - 7ms/epoch - 3ms/step\n",
      "Epoch 98/99999\n",
      "2/2 - 0s - loss: 0.1531 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 99/99999\n",
      "2/2 - 0s - loss: 0.2012 - categorical_accuracy: 0.9423 - 7ms/epoch - 3ms/step\n",
      "Epoch 100/99999\n",
      "2/2 - 0s - loss: 0.1622 - categorical_accuracy: 0.9808 - 4ms/epoch - 2ms/step\n",
      "Epoch 101/99999\n",
      "2/2 - 0s - loss: 0.1318 - categorical_accuracy: 0.9808 - 5ms/epoch - 3ms/step\n",
      "Epoch 102/99999\n",
      "2/2 - 0s - loss: 0.1171 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 103/99999\n",
      "2/2 - 0s - loss: 0.0895 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 104/99999\n",
      "2/2 - 0s - loss: 0.0778 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 105/99999\n",
      "2/2 - 0s - loss: 0.0748 - categorical_accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 106/99999\n",
      "2/2 - 0s - loss: 0.0702 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 107/99999\n",
      "2/2 - 0s - loss: 0.0606 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 108/99999\n",
      "2/2 - 0s - loss: 0.0547 - categorical_accuracy: 1.0000 - 3ms/epoch - 2ms/step\n",
      "Epoch 109/99999\n",
      "2/2 - 0s - loss: 0.0502 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 110/99999\n",
      "2/2 - 0s - loss: 0.0468 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 111/99999\n",
      "2/2 - 0s - loss: 0.0430 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 112/99999\n",
      "2/2 - 0s - loss: 0.0405 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 113/99999\n",
      "2/2 - 0s - loss: 0.0392 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 114/99999\n",
      "2/2 - 0s - loss: 0.0367 - categorical_accuracy: 1.0000 - 4ms/epoch - 2ms/step\n",
      "Epoch 115/99999\n",
      "2/2 - 0s - loss: 0.0346 - categorical_accuracy: 1.0000 - 5ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d3f95dc8b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
    "model3.fit(x=TRAINING_SET_2D, y=train_cat, epochs=99999, verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53d72068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step - loss: 6.9250 - categorical_accuracy: 0.0769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.925009727478027, 0.07692307978868484]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(x=TEST_SET_2D, y=test_cat, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bc55f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.81602609e-01, 2.08660262e-03, 2.27242010e-11, 2.24404386e-04,\n",
       "        1.67462963e-06, 1.98980520e-11, 1.13522572e-06, 3.40147485e-12,\n",
       "        2.24859104e-01, 3.15972080e-04, 1.02816741e-10, 1.33620139e-04,\n",
       "        1.09265852e-12, 4.99650099e-09, 5.27833169e-03, 6.73517742e-10,\n",
       "        2.94383848e-04, 1.77002621e-05, 1.01457257e-02, 2.61632383e-01,\n",
       "        2.56575561e-09, 3.95698127e-11, 8.31491946e-07, 2.15514273e-08,\n",
       "        1.34056341e-02, 2.47153838e-12],\n",
       "       [7.26213257e-05, 4.17654868e-03, 2.68439378e-13, 3.55660304e-04,\n",
       "        6.73488099e-10, 2.65299305e-17, 1.62411382e-07, 9.51506923e-15,\n",
       "        1.41433598e-02, 5.34644823e-05, 6.16538808e-15, 2.17696328e-09,\n",
       "        1.07463377e-15, 1.83594429e-09, 2.83599049e-02, 6.61910956e-13,\n",
       "        7.68100179e-08, 4.80028291e-07, 2.68173491e-04, 9.97831696e-04,\n",
       "        6.52469900e-09, 1.43935280e-11, 6.15956087e-05, 2.86002667e-12,\n",
       "        9.51510072e-01, 1.44756050e-14],\n",
       "       [7.03990111e-07, 3.16475621e-06, 3.31800617e-02, 3.10914334e-06,\n",
       "        2.32161366e-08, 6.88591285e-12, 3.84161860e-01, 1.05077497e-05,\n",
       "        2.04687312e-01, 1.93137661e-04, 1.20936065e-07, 4.95365006e-04,\n",
       "        1.09596733e-07, 3.70767521e-06, 3.29490244e-01, 2.42995424e-09,\n",
       "        2.19222549e-02, 2.67811533e-08, 7.72876234e-08, 2.47623702e-03,\n",
       "        6.94641722e-06, 4.20524302e-04, 1.30400610e-06, 2.23323554e-02,\n",
       "        6.03229797e-04, 7.64224023e-06],\n",
       "       [2.34444570e-02, 1.28098438e-03, 1.06154007e-12, 5.59708918e-04,\n",
       "        6.74036660e-10, 2.70401901e-16, 1.43340276e-06, 3.00261118e-14,\n",
       "        2.38026306e-01, 2.57051597e-05, 4.54457952e-15, 2.06209407e-07,\n",
       "        7.34059028e-16, 3.05680287e-10, 1.29959742e-02, 3.07405766e-13,\n",
       "        1.61641598e-04, 8.34132436e-07, 3.14359702e-02, 5.67303956e-01,\n",
       "        4.28092506e-10, 2.40423848e-12, 1.24362623e-05, 2.63965464e-11,\n",
       "        1.24750346e-01, 7.71541813e-15],\n",
       "       [3.46880188e-06, 4.09570130e-05, 8.61398064e-08, 2.45003633e-07,\n",
       "        1.70053929e-01, 9.51596758e-06, 9.85380666e-09, 1.84915944e-10,\n",
       "        1.42903868e-02, 3.06008762e-04, 1.00722099e-02, 5.24764732e-02,\n",
       "        1.07389118e-04, 4.49329964e-05, 1.78579910e-04, 2.63974725e-05,\n",
       "        6.79731105e-10, 1.31247607e-05, 5.35961449e-11, 6.57738929e-06,\n",
       "        5.39280220e-09, 5.80080384e-09, 1.30663465e-13, 7.52368510e-01,\n",
       "        4.20020889e-08, 1.09881159e-06],\n",
       "       [1.00263869e-05, 6.27644476e-05, 1.04122627e-10, 8.40625432e-07,\n",
       "        9.09296274e-01, 4.07880638e-03, 3.38735671e-11, 8.02924741e-11,\n",
       "        1.47218001e-04, 4.70099447e-04, 5.42420857e-02, 5.35472715e-03,\n",
       "        1.52422144e-04, 4.66185193e-05, 6.69211261e-07, 6.93000096e-04,\n",
       "        1.05120114e-11, 4.05708503e-04, 8.90640783e-10, 5.51781682e-07,\n",
       "        2.45362819e-09, 2.93544217e-10, 3.55274024e-13, 2.50382088e-02,\n",
       "        3.62959063e-09, 8.05999392e-08],\n",
       "       [2.86265986e-05, 9.18622663e-06, 1.13469439e-04, 2.57201646e-06,\n",
       "        5.17577181e-09, 7.84131467e-14, 7.94638246e-02, 1.89760485e-07,\n",
       "        4.22009081e-01, 4.88647202e-05, 1.05974685e-09, 8.71264492e-04,\n",
       "        3.11333598e-10, 2.93176559e-08, 5.37812747e-02, 2.47520251e-11,\n",
       "        3.94866645e-01, 5.41477663e-09, 3.93041034e-07, 4.81403545e-02,\n",
       "        8.09786016e-08, 1.71330942e-06, 5.86518425e-08, 5.91289659e-04,\n",
       "        7.10208406e-05, 7.81320786e-09],\n",
       "       [2.14038898e-10, 3.51453258e-04, 7.64237263e-17, 2.25220784e-08,\n",
       "        5.76254897e-05, 2.83743808e-04, 1.73270722e-15, 6.30305124e-07,\n",
       "        3.10075611e-11, 5.73741236e-05, 7.19423518e-02, 7.00693572e-14,\n",
       "        1.86897758e-02, 4.52130698e-02, 5.95031229e-12, 8.60751510e-01,\n",
       "        1.85036725e-18, 2.53619649e-03, 5.38746547e-11, 4.11666957e-13,\n",
       "        4.76666082e-06, 1.80883024e-08, 6.48871104e-12, 1.11462832e-04,\n",
       "        1.37739299e-12, 3.50777007e-08],\n",
       "       [7.34239247e-09, 2.64998789e-07, 3.06040281e-04, 1.32774701e-07,\n",
       "        1.53259913e-12, 8.53158840e-19, 2.74092704e-01, 5.18301313e-09,\n",
       "        4.56122249e-01, 1.89459877e-06, 5.09559625e-13, 6.86633484e-06,\n",
       "        8.78829399e-13, 2.96034841e-09, 2.46070996e-01, 6.01669525e-15,\n",
       "        2.10357588e-02, 9.65598088e-12, 1.39963174e-09, 2.06927978e-03,\n",
       "        2.18467449e-08, 9.10403116e-07, 1.10166862e-08, 1.24324913e-04,\n",
       "        1.68515704e-04, 1.05818354e-09],\n",
       "       [4.72904438e-10, 4.00970634e-07, 1.26908928e-01, 9.49459533e-08,\n",
       "        8.31165622e-11, 4.90833807e-14, 2.34711125e-01, 2.84620270e-04,\n",
       "        5.93657140e-04, 1.40638486e-03, 9.67714242e-10, 4.29874003e-09,\n",
       "        1.20838266e-07, 1.96161018e-05, 4.57020283e-01, 6.76139311e-10,\n",
       "        1.01833697e-03, 1.32984723e-09, 2.47752663e-08, 4.05780411e-05,\n",
       "        1.67487981e-03, 1.68569401e-01, 3.11799267e-05, 1.38733874e-03,\n",
       "        6.05067890e-03, 2.82407913e-04],\n",
       "       [1.01552223e-06, 6.03730896e-08, 1.81029029e-11, 6.71226488e-08,\n",
       "        8.15540016e-01, 8.78643021e-02, 2.37021611e-13, 2.56231776e-12,\n",
       "        7.76692161e-07, 3.08315357e-04, 8.40619113e-03, 3.50751504e-02,\n",
       "        1.63803379e-05, 1.94512518e-06, 2.41062548e-09, 1.69497216e-04,\n",
       "        3.52507737e-13, 1.25333354e-05, 4.87695336e-11, 1.92820142e-08,\n",
       "        6.68210903e-11, 4.10879490e-12, 8.51886192e-15, 5.26037998e-02,\n",
       "        2.87267866e-11, 1.02300977e-08],\n",
       "       [6.64959336e-03, 2.02909185e-04, 2.71125522e-08, 1.49176385e-05,\n",
       "        1.26578034e-05, 6.26612234e-11, 8.86462749e-06, 2.03939764e-11,\n",
       "        9.53659236e-01, 1.64802259e-05, 1.80369284e-08, 1.00602591e-02,\n",
       "        2.06162490e-10, 3.56412286e-08, 6.95658242e-03, 1.82069437e-09,\n",
       "        5.30935540e-05, 1.01673538e-06, 2.79740266e-06, 2.21936982e-02,\n",
       "        1.75230763e-09, 3.27296551e-10, 3.53759866e-09, 1.64596513e-05,\n",
       "        1.51263594e-04, 2.12303494e-10],\n",
       "       [3.45522366e-08, 2.67790301e-06, 4.48148567e-11, 2.33567938e-08,\n",
       "        8.72939173e-03, 1.75528005e-01, 7.97955452e-13, 2.40340825e-07,\n",
       "        1.69296523e-08, 1.63724573e-04, 6.75554633e-01, 3.23564166e-07,\n",
       "        7.16838520e-03, 2.59851711e-03, 3.03516157e-10, 5.86214028e-02,\n",
       "        2.00135691e-14, 7.27827137e-05, 2.14207901e-10, 5.18275589e-10,\n",
       "        3.04969689e-07, 3.50240015e-08, 6.76384621e-13, 7.15544894e-02,\n",
       "        1.17532997e-11, 4.99390489e-06],\n",
       "       [2.24239985e-03, 5.06939441e-02, 7.31813140e-18, 3.67651135e-02,\n",
       "        2.06241454e-03, 2.77177343e-04, 5.24016831e-14, 7.46741731e-12,\n",
       "        8.84559540e-07, 2.20648535e-02, 1.86860143e-05, 2.36363633e-07,\n",
       "        4.35059206e-08, 1.08477667e-04, 6.25735339e-08, 4.95943427e-03,\n",
       "        9.81789198e-15, 8.80785108e-01, 1.85468944e-05, 2.61434138e-07,\n",
       "        9.03180322e-08, 2.36677908e-12, 1.54114036e-06, 2.59014374e-08,\n",
       "        5.84682141e-07, 2.82265379e-12],\n",
       "       [1.21086487e-04, 6.51116643e-05, 1.06141801e-04, 2.72716643e-05,\n",
       "        7.98147752e-08, 5.82079133e-12, 3.46436240e-02, 1.85798413e-07,\n",
       "        7.22142577e-01, 2.02358438e-04, 9.65972102e-09, 1.08552095e-03,\n",
       "        2.81849477e-09, 4.25099302e-07, 1.37623087e-01, 1.01742048e-09,\n",
       "        5.17270677e-02, 2.12786247e-07, 6.14210467e-06, 5.01810089e-02,\n",
       "        8.59736190e-07, 3.00599299e-06, 1.71985369e-06, 3.90528614e-04,\n",
       "        1.67186139e-03, 6.65870843e-08],\n",
       "       [9.63369894e-05, 8.94663453e-01, 5.30928068e-18, 2.41157003e-02,\n",
       "        5.99716441e-06, 1.53520290e-08, 1.90384969e-12, 1.86423967e-12,\n",
       "        6.10771622e-07, 5.38754091e-03, 4.16638564e-08, 1.16050738e-10,\n",
       "        1.62806013e-09, 3.13198871e-05, 9.66845278e-07, 8.76340491e-05,\n",
       "        3.60445011e-14, 7.53484890e-02, 7.56213922e-05, 1.31633030e-07,\n",
       "        2.15474074e-07, 1.79380122e-11, 1.19852499e-04, 6.38457759e-11,\n",
       "        6.59758880e-05, 2.96086127e-13],\n",
       "       [3.42926432e-06, 2.05756533e-06, 1.19497827e-05, 8.78021808e-07,\n",
       "        2.85394364e-11, 3.12730387e-17, 5.10648154e-02, 6.18029405e-09,\n",
       "        4.46292609e-01, 5.48431944e-06, 8.92670378e-13, 4.91604915e-05,\n",
       "        7.60263001e-13, 1.20529486e-09, 4.85136509e-02, 5.07126554e-14,\n",
       "        4.16444153e-01, 1.39479386e-10, 2.11409798e-07, 3.74665000e-02,\n",
       "        6.13012885e-09, 1.34906585e-07, 1.99839771e-08, 3.55885350e-05,\n",
       "        1.09404529e-04, 1.45854245e-10],\n",
       "       [7.63303554e-03, 4.85327601e-01, 5.75439558e-14, 1.07731838e-02,\n",
       "        1.31147029e-02, 1.67334565e-05, 5.07008824e-10, 1.60636004e-10,\n",
       "        5.86264068e-04, 1.45711765e-01, 8.30990321e-05, 5.64092352e-06,\n",
       "        9.90911644e-07, 2.05647171e-04, 8.02953873e-05, 2.50121299e-03,\n",
       "        8.01999370e-11, 3.33652884e-01, 6.83557882e-05, 1.46247185e-05,\n",
       "        9.58654709e-07, 1.32244560e-09, 2.25299436e-06, 2.57717534e-06,\n",
       "        2.18232410e-04, 7.74158515e-10],\n",
       "       [5.72461837e-13, 2.65705182e-07, 7.34081790e-02, 5.00477049e-10,\n",
       "        3.19168173e-12, 2.31893242e-16, 4.70322259e-02, 1.55747384e-05,\n",
       "        5.29482495e-03, 3.93899245e-06, 4.53421467e-10, 3.28740840e-10,\n",
       "        5.94945355e-08, 2.03492164e-05, 8.22314918e-01, 7.87301890e-11,\n",
       "        8.02792965e-06, 7.99520103e-11, 2.86505871e-11, 1.47323078e-06,\n",
       "        1.60680691e-04, 4.50942181e-02, 3.69627031e-08, 5.32394508e-03,\n",
       "        8.22805567e-04, 4.98487905e-04],\n",
       "       [1.45781848e-06, 9.07444644e-07, 2.32246020e-04, 1.06818436e-06,\n",
       "        9.93300373e-11, 5.92446819e-16, 1.86349139e-01, 8.27190547e-08,\n",
       "        1.95987418e-01, 3.11282456e-05, 1.37083617e-11, 1.12462250e-04,\n",
       "        1.52794877e-11, 6.80052370e-09, 7.31505454e-02, 5.33354773e-13,\n",
       "        5.25141180e-01, 2.10874124e-10, 8.92096850e-08, 1.84585527e-02,\n",
       "        5.01558013e-08, 2.33103401e-06, 5.93457514e-08, 3.77119228e-04,\n",
       "        1.54201029e-04, 2.83964230e-09],\n",
       "       [2.53199661e-09, 1.97259240e-07, 9.18018166e-04, 2.31436815e-07,\n",
       "        1.27998816e-12, 5.46714190e-18, 3.47392738e-01, 6.69773073e-08,\n",
       "        7.86166638e-02, 1.85719600e-05, 1.29682137e-12, 2.31977097e-07,\n",
       "        8.95227555e-12, 2.14454250e-08, 5.66473663e-01, 8.23481298e-14,\n",
       "        4.95088939e-03, 3.55742762e-11, 2.81869927e-09, 4.51240368e-04,\n",
       "        8.45167563e-07, 2.63586226e-05, 2.84434435e-07, 6.58659264e-05,\n",
       "        1.08410115e-03, 2.27955983e-08],\n",
       "       [1.71872042e-02, 2.45837960e-03, 1.05165100e-05, 5.60866320e-04,\n",
       "        4.08764172e-05, 2.51344829e-08, 1.62406708e-03, 1.94923715e-07,\n",
       "        7.69123018e-01, 8.26727774e-04, 9.88485453e-07, 6.37393678e-03,\n",
       "        9.86798057e-08, 7.03743945e-06, 6.48131743e-02, 4.90429500e-07,\n",
       "        6.63877744e-03, 7.23543271e-05, 7.22608936e-04, 1.19787522e-01,\n",
       "        3.45911121e-06, 1.45625472e-06, 1.89585335e-05, 1.68215862e-04,\n",
       "        9.55878571e-03, 3.14628267e-07],\n",
       "       [3.80475819e-03, 2.03100775e-04, 9.06373680e-05, 1.99995746e-04,\n",
       "        1.29947864e-06, 6.83618495e-10, 2.56639104e-02, 3.76591493e-06,\n",
       "        2.02927217e-01, 3.04550678e-03, 8.38447676e-08, 2.56554224e-03,\n",
       "        2.14534221e-08, 1.24360668e-06, 8.70763361e-02, 2.88814572e-08,\n",
       "        4.70568657e-01, 1.87157752e-06, 2.04498894e-04, 2.00323418e-01,\n",
       "        5.13989016e-06, 1.74057932e-05, 1.43833468e-05, 5.04799711e-04,\n",
       "        2.77616829e-03, 2.50532281e-07],\n",
       "       [3.40937264e-02, 1.38953552e-02, 3.92284566e-10, 1.64608900e-02,\n",
       "        3.67410308e-07, 2.21588893e-11, 8.75747446e-06, 1.00500004e-10,\n",
       "        1.94677096e-02, 1.61834154e-03, 4.23481458e-11, 1.28481668e-06,\n",
       "        1.44999256e-11, 3.51897768e-07, 1.79082006e-02, 4.72471440e-09,\n",
       "        6.10840216e-05, 1.13223570e-04, 1.78012162e-01, 5.68236299e-02,\n",
       "        5.32704973e-07, 4.41241044e-09, 5.53255621e-03, 2.07182138e-09,\n",
       "        6.56001866e-01, 3.59419126e-11],\n",
       "       [8.85595742e-04, 4.95942986e-05, 3.49695100e-08, 1.60328258e-04,\n",
       "        1.85594373e-10, 2.28036198e-16, 6.77343749e-04, 1.83469226e-10,\n",
       "        1.21693447e-01, 6.50372749e-05, 3.15662616e-14, 3.30792454e-06,\n",
       "        6.12559184e-14, 1.63777170e-09, 2.90057659e-02, 3.06779477e-13,\n",
       "        1.68906778e-01, 1.27509470e-08, 1.15470309e-03, 6.70289397e-01,\n",
       "        9.94468241e-09, 4.84558793e-09, 8.60695218e-06, 4.76312039e-08,\n",
       "        7.09991343e-03, 3.49753776e-12],\n",
       "       [9.67551425e-18, 1.74314816e-11, 5.72969437e-01, 9.55492975e-15,\n",
       "        4.26069987e-13, 4.80693739e-16, 1.06448417e-04, 3.70708185e-05,\n",
       "        3.69034956e-06, 9.26190236e-09, 4.37531344e-09, 1.95694572e-11,\n",
       "        3.60904892e-06, 7.39411598e-06, 1.64454873e-03, 3.48690313e-11,\n",
       "        8.22619661e-09, 8.99867474e-14, 2.22028922e-16, 3.92989585e-10,\n",
       "        4.58443901e-06, 2.52877325e-02, 1.39674906e-13, 3.49395454e-01,\n",
       "        5.25792210e-09, 5.05400971e-02]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model3.predict(TEST_SET_2D)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a42e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'A'],\n",
       " ['B', 'Y'],\n",
       " ['C', 'G'],\n",
       " ['D', 'T'],\n",
       " ['E', 'X'],\n",
       " ['F', 'E'],\n",
       " ['G', 'I'],\n",
       " ['H', 'P'],\n",
       " ['I', 'I'],\n",
       " ['J', 'O'],\n",
       " ['K', 'E'],\n",
       " ['L', 'I'],\n",
       " ['M', 'K'],\n",
       " ['N', 'R'],\n",
       " ['O', 'I'],\n",
       " ['P', 'B'],\n",
       " ['Q', 'I'],\n",
       " ['R', 'B'],\n",
       " ['S', 'O'],\n",
       " ['T', 'Q'],\n",
       " ['U', 'O'],\n",
       " ['V', 'I'],\n",
       " ['W', 'Q'],\n",
       " ['X', 'Y'],\n",
       " ['Y', 'T'],\n",
       " ['Z', 'C']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = []\n",
    "for num in range(len(predicted)):\n",
    "    max_val = 0\n",
    "    actual = chr(num+ord('A'))\n",
    "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    result_list.append([actual, predict])\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbdf41df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images correctly classified: 7.6923076923076925%\n",
      "Here are the list of test images that are misclassified and how they appear:\n",
      "Actual    Predicted\n",
      "B         Y\n",
      "C         G\n",
      "D         T\n",
      "E         X\n",
      "F         E\n",
      "G         I\n",
      "H         P\n",
      "J         O\n",
      "K         E\n",
      "L         I\n",
      "M         K\n",
      "N         R\n",
      "O         I\n",
      "P         B\n",
      "Q         I\n",
      "R         B\n",
      "S         O\n",
      "T         Q\n",
      "U         O\n",
      "V         I\n",
      "W         Q\n",
      "X         Y\n",
      "Y         T\n",
      "Z         C\n"
     ]
    }
   ],
   "source": [
    "list_misclassified = []\n",
    "num_total, num_correct = 26, 26\n",
    "for i in range(len(result_list)):\n",
    "    if result_list[i][0] != result_list[i][1]:\n",
    "        num_correct -= 1\n",
    "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
    "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_misclassified) == 0:\n",
    "    print(\"All test images are classified correctly.\")\n",
    "else:\n",
    "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_misclassified)):\n",
    "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26954876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ########    \n",
      "    ########    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##    ######  \n",
      "  ##    ######  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ########  \n",
      "      ########  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##########      \n",
      "##########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "##########      \n",
      "##########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##############\n",
      "  ##############\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##############  \n",
      "##############  \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##          ##\n",
      "  ##          ##\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##############  \n",
      "##############  \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ##      \n",
      "  ####  ##      \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "########        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ############  \n",
      "  ############  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####            \n",
      "####    ######  \n",
      "####    ######  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "    ##########  \n",
      "    ##########  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "    ##          \n",
      "  ######        \n",
      "  ######        \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "      ########  \n",
      "      ########  \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "        ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "      ####      \n",
      "      ####      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "######    ####  \n",
      "######    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ########      \n",
      "  ########      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ############  \n",
      "  ############  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ######      \n",
      "    ######      \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ############  \n",
      "  ############  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "########        \n",
      "########        \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "######  ######  \n",
      "######  ######  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "##############  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ####      ##  \n",
      "  ####      ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ####        \n",
      "    ####        \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ####      ####\n",
      "  ####      ####\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "########  ####  \n",
      "########  ####  \n",
      "####  ########  \n",
      "####  ########  \n",
      "####    ######  \n",
      "####    ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##  ##      \n",
      "    ##  ##      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ######    ##  \n",
      "  ######    ##  \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "    ######      \n",
      "    ######      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "  ####          \n",
      "########        \n",
      "########        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##########    \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####  ######    \n",
      "####  ######    \n",
      "  ########      \n",
      "  ########      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "  ##########    \n",
      "  ##########    \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "######    ####  \n",
      "######    ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##########    \n",
      "  ##########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ########    \n",
      "    ########    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##########    \n",
      "  ##########    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "  ########      \n",
      "  ########      \n",
      "####    ####    \n",
      "####    ####    \n",
      "######          \n",
      "######          \n",
      "  ######        \n",
      "  ######        \n",
      "      ######    \n",
      "      ######    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "      ####      \n",
      "      ####      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "############    \n",
      "############    \n",
      "##  ####  ##    \n",
      "##  ####  ##    \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##    ##  ##  \n",
      "  ##    ##  ##  \n",
      "    ########    \n",
      "    ########    \n",
      "            ####\n",
      "            ####\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "############    \n",
      "############    \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ####      \n",
      "      ####      \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "    ##    ##    \n",
      "    ##    ##    \n",
      "      ####      \n",
      "      ####      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ######      \n",
      "    ######      \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "      ##        \n",
      "    ######      \n",
      "    ######      \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####  ##  ####  \n",
      "####  ##  ####  \n",
      "##############  \n",
      "##############  \n",
      "######  ######  \n",
      "######  ######  \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "    ########    \n",
      "    ########    \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##        ##  \n",
      "  ##    ##  ##  \n",
      "  ##    ##  ##  \n",
      "    ########    \n",
      "    ########    \n",
      "            ####\n",
      "            ####\n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "####      ####  \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "    ######      \n",
      "  ####  ####    \n",
      "  ####  ####    \n",
      "####      ####  \n",
      "####      ####  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##          ##\n",
      "  ##          ##\n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ##  ##    \n",
      "      ##  ##    \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "####    ####    \n",
      "  ########      \n",
      "  ########      \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "    ####        \n",
      "  ########      \n",
      "  ########      \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "  ##############\n",
      "  ##############\n",
      "  ##    ##    ##\n",
      "  ##    ##    ##\n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "        ##      \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n",
      "================================================\n",
      "Expected letter:\n",
      "##############  \n",
      "##############  \n",
      "####      ####  \n",
      "####      ####  \n",
      "##      ####    \n",
      "##      ####    \n",
      "      ####      \n",
      "      ####      \n",
      "    ####    ##  \n",
      "    ####    ##  \n",
      "  ####    ####  \n",
      "  ####    ####  \n",
      "##############  \n",
      "##############  \n",
      "                \n",
      "                \n",
      "Predicted letter:\n",
      "      ######    \n",
      "      ######    \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "  ##            \n",
      "    ##      ##  \n",
      "    ##      ##  \n",
      "      ######    \n",
      "      ######    \n",
      "                \n",
      "                \n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Use show() to display the misclassified images.\n",
    "for j in range(len(list_misclassified)):\n",
    "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
    "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
    "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
    "    print(\"================================================\")\n",
    "    print(\"Expected letter:\")\n",
    "    show(TRAINING_SET_2D[letter_train])\n",
    "    print(\"Predicted letter:\")\n",
    "    show(TEST_SET_2D[letter_test])\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac12583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.66152175e-07, 4.82716644e-07, 7.55167275e-04, 4.73423000e-07,\n",
       "        1.80509233e-10, 1.09483688e-15, 2.88976312e-01, 1.24710894e-07,\n",
       "        2.01579317e-01, 3.40706756e-05, 7.61565186e-11, 3.32278840e-04,\n",
       "        5.11682849e-11, 9.25569132e-09, 8.09499696e-02, 8.68667822e-13,\n",
       "        4.12830859e-01, 1.50058715e-10, 1.41860870e-08, 1.29368100e-02,\n",
       "        4.29776819e-08, 3.67107350e-06, 2.02791615e-08, 1.54745800e-03,\n",
       "        5.21261209e-05, 5.44099477e-09],\n",
       "       [2.59414729e-11, 4.82800999e-04, 9.13633626e-17, 1.50388750e-08,\n",
       "        9.85267252e-06, 4.54068686e-05, 2.78779952e-15, 5.52866686e-06,\n",
       "        1.04332142e-11, 5.61936031e-05, 2.97619160e-02, 2.98509340e-15,\n",
       "        4.06500697e-02, 1.38142109e-01, 8.70106417e-12, 7.88602948e-01,\n",
       "        1.53245341e-18, 2.09268276e-03, 4.38595028e-11, 1.97199733e-13,\n",
       "        4.18360360e-05, 1.06101972e-07, 1.66475705e-11, 1.08435277e-04,\n",
       "        2.16073318e-12, 1.04513106e-07],\n",
       "       [1.08208667e-04, 9.46459480e-08, 1.03192846e-16, 2.45302749e-06,\n",
       "        7.52508342e-01, 6.58566458e-03, 1.69920169e-17, 3.11076476e-16,\n",
       "        2.01770050e-07, 1.33421854e-04, 5.24560164e-06, 2.40563825e-01,\n",
       "        5.28638966e-10, 1.89960812e-08, 1.29337957e-10, 3.26802660e-06,\n",
       "        2.39683563e-16, 3.61926177e-05, 3.86127408e-10, 6.40012354e-09,\n",
       "        2.20538635e-13, 5.34056060e-17, 8.10261214e-15, 5.30826292e-05,\n",
       "        3.08684710e-11, 7.95126822e-13],\n",
       "       [6.60410615e-09, 4.90828178e-08, 5.34706489e-17, 7.72082132e-09,\n",
       "        1.22210262e-02, 9.55650032e-01, 1.18061783e-18, 5.27921944e-13,\n",
       "        2.10052462e-10, 5.51510784e-05, 2.29267534e-02, 6.87965269e-07,\n",
       "        6.03267472e-06, 2.48673823e-05, 1.42821600e-13, 7.78434612e-03,\n",
       "        3.59887996e-20, 2.96141116e-05, 6.32698483e-13, 1.64598543e-12,\n",
       "        5.32180792e-11, 1.18465174e-13, 4.93646514e-16, 1.30151387e-03,\n",
       "        1.40444610e-14, 1.83504073e-10],\n",
       "       [1.16461481e-03, 1.18966367e-04, 7.24799198e-09, 2.40662557e-05,\n",
       "        5.10403386e-10, 4.23241294e-16, 3.77543591e-04, 5.71988706e-11,\n",
       "        6.27118051e-01, 8.61354874e-06, 3.57279392e-13, 1.45420390e-05,\n",
       "        8.77968894e-14, 9.12492415e-10, 2.42004506e-02, 3.95227118e-13,\n",
       "        2.86926553e-02, 2.16646914e-08, 1.40207252e-04, 3.16919029e-01,\n",
       "        1.71960290e-09, 8.26198943e-10, 2.82395547e-07, 8.84923494e-08,\n",
       "        1.22089521e-03, 2.64745040e-12],\n",
       "       [2.43048565e-27, 1.52129902e-15, 1.18699821e-03, 2.15188406e-22,\n",
       "        6.12491145e-22, 2.31974425e-26, 3.99881259e-07, 1.79749004e-05,\n",
       "        2.45558002e-14, 1.99759764e-08, 1.83037881e-17, 2.76627561e-25,\n",
       "        1.58743799e-11, 5.44907897e-09, 1.54700974e-05, 4.25749365e-18,\n",
       "        3.90679432e-13, 1.31010344e-20, 9.49362556e-20, 3.92608495e-15,\n",
       "        3.02760571e-04, 9.98358548e-01, 1.13360947e-13, 3.54879610e-08,\n",
       "        4.25972625e-11, 1.17764910e-04],\n",
       "       [3.54964897e-04, 4.32198541e-03, 8.34033855e-08, 2.96768467e-05,\n",
       "        3.52584362e-01, 5.12948691e-06, 2.71093484e-07, 6.04770123e-10,\n",
       "        3.63786638e-01, 6.97416766e-03, 2.47003580e-03, 1.36288688e-01,\n",
       "        3.10255855e-05, 1.30754706e-04, 9.39257909e-03, 4.63007418e-05,\n",
       "        4.07042755e-08, 4.17514413e-04, 2.80395049e-08, 3.81400925e-04,\n",
       "        1.04013218e-07, 3.45443461e-08, 1.06250210e-10, 1.22755140e-01,\n",
       "        2.80841850e-05, 1.02894069e-06],\n",
       "       [2.31914736e-08, 9.88832355e-01, 1.28815192e-29, 1.76766526e-03,\n",
       "        2.79948342e-10, 9.18645382e-14, 1.64827692e-20, 4.23300011e-19,\n",
       "        2.03846271e-12, 3.68628171e-05, 6.62190951e-13, 5.60159214e-19,\n",
       "        9.68956479e-15, 1.31589800e-07, 1.34130389e-11, 5.35972902e-07,\n",
       "        5.77932555e-24, 9.36071016e-03, 7.15412583e-08, 4.15738326e-13,\n",
       "        4.13182197e-11, 6.48934399e-18, 1.63612310e-06, 2.92945414e-18,\n",
       "        1.85439983e-08, 1.53458735e-21],\n",
       "       [8.23322801e-14, 1.55797402e-08, 1.60726428e-04, 2.03432649e-09,\n",
       "        1.31779877e-16, 9.54622417e-23, 1.07123200e-02, 7.43279543e-07,\n",
       "        7.90595436e-07, 1.62359432e-03, 2.67339265e-17, 3.12705241e-16,\n",
       "        6.23787968e-13, 6.26127132e-08, 8.92525315e-01, 3.04303214e-15,\n",
       "        9.50219965e-06, 2.64641797e-13, 1.15973897e-09, 1.08701806e-06,\n",
       "        1.10599154e-03, 2.68756729e-02, 3.65522719e-04, 1.64645915e-08,\n",
       "        6.66186139e-02, 2.49559715e-08],\n",
       "       [7.59974793e-02, 6.73152925e-03, 4.86556672e-12, 2.00585891e-02,\n",
       "        4.93339272e-08, 2.57983804e-12, 6.55706344e-07, 3.01660238e-12,\n",
       "        3.51140252e-03, 5.17265755e-04, 1.04538112e-12, 1.68980392e-07,\n",
       "        4.18072911e-13, 3.12745705e-08, 2.38332758e-03, 5.05396447e-10,\n",
       "        1.57993436e-05, 5.59312284e-05, 5.88850379e-01, 4.54661101e-02,\n",
       "        3.68154680e-08, 8.93508115e-11, 4.17904230e-03, 5.10790230e-11,\n",
       "        2.52232164e-01, 4.92260854e-13],\n",
       "       [2.27074293e-06, 9.42407041e-07, 1.70841455e-12, 5.72518246e-08,\n",
       "        8.95222261e-15, 1.56440489e-23, 1.20464974e-05, 1.82544018e-16,\n",
       "        9.73267794e-01, 1.08368532e-08, 9.20851579e-19, 3.19941265e-08,\n",
       "        1.35180777e-19, 1.08853837e-13, 6.66808290e-03, 6.29006698e-19,\n",
       "        1.89676313e-04, 6.54502624e-12, 1.58444891e-07, 1.97314750e-02,\n",
       "        2.21554139e-13, 3.64297690e-14, 2.00894815e-10, 5.64921061e-11,\n",
       "        1.27388194e-04, 2.84302076e-17],\n",
       "       [5.03551408e-21, 1.22807239e-06, 3.05430011e-20, 8.14560827e-16,\n",
       "        7.60779703e-11, 5.75331206e-11, 4.57076225e-20, 5.35066472e-04,\n",
       "        6.28977550e-18, 2.09655138e-08, 3.94635077e-04, 4.99541854e-26,\n",
       "        2.26388976e-01, 7.64482617e-01, 6.49092624e-16, 7.68931815e-03,\n",
       "        4.75972077e-26, 1.06562894e-07, 5.78026119e-18, 1.23487289e-20,\n",
       "        4.53916262e-04, 6.91873936e-07, 8.34290395e-18, 5.26896911e-05,\n",
       "        4.14843195e-18, 7.22694551e-07],\n",
       "       [2.06672418e-15, 2.44650220e-13, 3.57063487e-02, 7.03529466e-13,\n",
       "        5.74562962e-18, 1.15980468e-23, 9.40393090e-01, 4.52123139e-08,\n",
       "        8.37718471e-05, 1.41362690e-07, 2.15906983e-15, 4.97502928e-10,\n",
       "        1.12254566e-13, 3.22030215e-11, 2.20082011e-02, 1.75804956e-18,\n",
       "        7.99176283e-04, 1.55050357e-17, 1.24066313e-15, 2.24312629e-07,\n",
       "        1.49198998e-09, 1.87970742e-04, 1.13096277e-11, 8.21125053e-04,\n",
       "        5.01803363e-08, 3.35517880e-09],\n",
       "       [1.05660293e-10, 6.52352627e-03, 4.61220988e-25, 4.99905960e-04,\n",
       "        1.31380199e-15, 6.90135073e-20, 5.13049762e-16, 3.14007554e-17,\n",
       "        7.56978776e-15, 4.86929854e-03, 5.32534710e-20, 9.53892421e-24,\n",
       "        8.12152478e-18, 4.05951717e-09, 1.20152146e-08, 1.00484595e-11,\n",
       "        6.60164213e-19, 5.56883094e-07, 5.97335020e-05, 9.54269476e-12,\n",
       "        4.49180050e-07, 2.20022212e-14, 9.87018585e-01, 2.88696858e-21,\n",
       "        1.02783600e-03, 1.11797927e-20],\n",
       "       [1.10228871e-06, 2.36966935e-06, 9.92493597e-07, 4.92207334e-07,\n",
       "        6.14620681e-12, 2.75299542e-18, 1.71918701e-02, 1.85262250e-10,\n",
       "        8.83712649e-01, 1.58913281e-06, 1.39689502e-13, 1.07840206e-05,\n",
       "        9.65421997e-14, 4.92898500e-10, 5.86839058e-02, 8.44162183e-15,\n",
       "        2.34582871e-02, 1.43955556e-10, 8.56362163e-08, 1.66770481e-02,\n",
       "        1.89757010e-09, 1.15448140e-08, 1.19932189e-08, 4.63551851e-06,\n",
       "        2.54209037e-04, 2.11023872e-11],\n",
       "       [1.31454797e-13, 3.96993682e-09, 1.93941712e-01, 1.43139485e-11,\n",
       "        5.04293551e-10, 6.17224587e-13, 7.51161075e-04, 1.21815528e-05,\n",
       "        3.40952480e-04, 1.41303008e-06, 3.49358430e-07, 2.87627078e-08,\n",
       "        1.03465654e-05, 2.68720851e-05, 1.34503255e-02, 4.21305035e-09,\n",
       "        3.33974924e-07, 3.43092291e-11, 7.29670230e-14, 6.02588983e-08,\n",
       "        1.35902683e-05, 7.10780872e-03, 1.72062833e-11, 7.79116571e-01,\n",
       "        5.48890966e-07, 5.22582233e-03],\n",
       "       [9.34583008e-01, 3.37851874e-04, 7.50984073e-13, 2.20378442e-03,\n",
       "        1.97755285e-02, 1.00670150e-06, 3.84241555e-10, 6.96413142e-13,\n",
       "        6.89356180e-04, 1.91776678e-02, 3.25471206e-08, 2.07189452e-02,\n",
       "        1.13721185e-10, 2.58860933e-08, 6.23031447e-05, 3.84147597e-07,\n",
       "        9.52300283e-08, 5.14151528e-04, 9.78181837e-04, 7.52389664e-04,\n",
       "        1.21907162e-09, 9.96255595e-13, 1.24423195e-07, 2.66906113e-07,\n",
       "        2.04762429e-04, 7.34265340e-12],\n",
       "       [3.19812417e-01, 4.25072432e-01, 1.59800507e-14, 1.20006368e-01,\n",
       "        1.54962169e-03, 1.59017777e-08, 2.94007996e-09, 3.47387456e-13,\n",
       "        6.55113207e-03, 7.44512156e-02, 3.66397011e-08, 4.43014796e-05,\n",
       "        2.77793538e-10, 9.17339719e-07, 1.20761804e-03, 2.74806303e-06,\n",
       "        3.47008067e-09, 3.19010057e-02, 1.72605924e-03, 5.84603345e-04,\n",
       "        2.39214835e-08, 1.48093465e-11, 2.39422443e-05, 8.97507491e-09,\n",
       "        1.70655381e-02, 2.27048046e-12],\n",
       "       [1.72333270e-02, 8.76432052e-04, 2.70776587e-12, 2.73904065e-04,\n",
       "        7.69314445e-10, 3.49066724e-16, 2.63519883e-06, 5.78950475e-14,\n",
       "        3.70085418e-01, 1.96749097e-05, 1.13039232e-14, 4.95894824e-07,\n",
       "        1.31246834e-15, 3.12228687e-10, 1.50612928e-02, 3.43492948e-13,\n",
       "        2.71708413e-04, 5.79074026e-07, 1.22759864e-02, 5.19689441e-01,\n",
       "        3.93210770e-10, 3.79859946e-12, 4.70633313e-06, 9.01139857e-11,\n",
       "        6.42043874e-02, 1.64218462e-14],\n",
       "       [1.59323905e-02, 3.00796473e-05, 2.61067466e-18, 2.52893195e-03,\n",
       "        1.86392113e-12, 2.83553688e-18, 7.05378325e-11, 3.24087978e-18,\n",
       "        4.54903739e-06, 1.65987476e-05, 8.69864748e-20, 1.19651971e-11,\n",
       "        4.52894116e-20, 9.54707233e-13, 9.11675215e-06, 5.82063889e-15,\n",
       "        1.34588149e-07, 7.58109522e-08, 9.55977619e-01, 4.32316167e-03,\n",
       "        2.22550082e-12, 2.86569351e-16, 4.28714469e-04, 5.27397223e-17,\n",
       "        2.07485836e-02, 5.64117597e-20],\n",
       "       [1.46611956e-06, 3.31243973e-05, 6.27729024e-09, 1.65771562e-05,\n",
       "        2.38988105e-06, 1.36501242e-07, 2.49119466e-07, 3.79761815e-07,\n",
       "        2.10915815e-07, 9.97114897e-01, 9.57797539e-08, 1.35152667e-09,\n",
       "        8.07897834e-07, 2.64350765e-05, 1.32273228e-04, 1.74260731e-05,\n",
       "        1.36160352e-07, 2.79489250e-05, 4.51030392e-05, 2.74585113e-06,\n",
       "        1.69051916e-03, 5.02083958e-06, 2.03864474e-04, 1.41270982e-06,\n",
       "        6.76435651e-04, 3.43382794e-07],\n",
       "       [2.90650712e-24, 2.40275932e-09, 1.42857471e-11, 1.91560951e-17,\n",
       "        1.57424061e-18, 5.10482398e-21, 6.59127397e-12, 7.89528713e-04,\n",
       "        8.63617720e-17, 7.09767164e-07, 3.75455926e-14, 1.05550374e-28,\n",
       "        9.20527725e-08, 6.57297496e-05, 5.17285770e-09, 2.33287729e-11,\n",
       "        1.68132285e-17, 7.44395255e-14, 7.93959080e-16, 8.16095664e-17,\n",
       "        9.79328394e-01, 1.98003035e-02, 5.42680310e-11, 1.21324739e-09,\n",
       "        2.61950600e-10, 1.52135435e-05],\n",
       "       [5.99337403e-33, 1.69500102e-13, 1.97093199e-25, 1.00870196e-26,\n",
       "        1.03825307e-15, 3.73169682e-15, 1.66390429e-28, 5.49939170e-04,\n",
       "        1.83001047e-26, 1.38231101e-14, 1.21481695e-04, 3.29454569e-36,\n",
       "        9.57951427e-01, 4.10073884e-02, 2.21783175e-24, 1.63833647e-05,\n",
       "        1.58101748e-36, 4.67002298e-14, 1.17013136e-28, 8.28475065e-31,\n",
       "        4.31075591e-07, 3.29032668e-09, 1.53097710e-30, 3.51970113e-04,\n",
       "        2.88764005e-29, 9.33109220e-07],\n",
       "       [9.79017667e-09, 9.06737914e-05, 3.20826201e-20, 8.94603147e-08,\n",
       "        1.06077222e-03, 1.90697704e-02, 8.33838124e-19, 7.58823282e-11,\n",
       "        3.03765173e-11, 1.51723216e-04, 6.80051520e-02, 1.21814971e-11,\n",
       "        1.92884763e-04, 2.54615815e-03, 2.09507501e-13, 9.01633203e-01,\n",
       "        2.90234978e-21, 7.22453790e-03, 5.74401074e-12, 2.96297166e-13,\n",
       "        1.32647662e-08, 3.98453926e-12, 1.52732631e-13, 2.49638215e-05,\n",
       "        5.95686965e-14, 3.39838331e-11],\n",
       "       [7.99169758e-13, 1.03077974e-12, 1.80345796e-06, 4.08954287e-10,\n",
       "        1.77295299e-21, 1.72923318e-31, 4.04528528e-02, 2.68344185e-13,\n",
       "        8.56243947e-04, 5.18734211e-08, 5.39118981e-25, 4.20881142e-11,\n",
       "        3.03441469e-22, 1.81565387e-15, 1.94975585e-02, 5.95115779e-25,\n",
       "        9.38552141e-01, 1.47601178e-19, 5.96316182e-12, 6.20280276e-04,\n",
       "        2.20084519e-12, 9.98349514e-10, 2.00274797e-10, 5.34430189e-09,\n",
       "        1.90912415e-05, 1.56093336e-16],\n",
       "       [3.19012767e-14, 5.49955992e-04, 8.45495396e-10, 1.63338623e-08,\n",
       "        3.28929727e-14, 4.57542250e-17, 6.98151382e-07, 6.43146086e-06,\n",
       "        3.93447691e-10, 1.15677575e-02, 1.77768566e-13, 5.06326753e-20,\n",
       "        4.19584278e-09, 3.29624920e-04, 2.34624813e-03, 1.28189004e-09,\n",
       "        4.95428976e-11, 1.26370541e-08, 1.23846036e-07, 2.73993805e-09,\n",
       "        9.10275578e-01, 2.44417656e-02, 2.99044382e-02, 2.06587858e-10,\n",
       "        2.05772016e-02, 1.41765852e-07],\n",
       "       [8.30796864e-07, 4.86601095e-07, 1.55032844e-11, 2.75270526e-08,\n",
       "        4.29160762e-14, 2.22694045e-22, 1.54741301e-05, 6.34359544e-16,\n",
       "        9.86748338e-01, 1.04385691e-08, 1.45660004e-17, 9.82648487e-08,\n",
       "        1.77795607e-18, 4.13788958e-13, 7.37893069e-03, 6.15130257e-18,\n",
       "        7.68692116e-05, 1.08006841e-11, 3.54499505e-08, 5.67489816e-03,\n",
       "        6.29615934e-13, 1.49451659e-13, 1.67796596e-10, 3.55832308e-10,\n",
       "        1.04160965e-04, 3.48505830e-16],\n",
       "       [3.51803356e-17, 2.49854174e-12, 9.53594506e-01, 3.06543426e-14,\n",
       "        2.18834392e-15, 4.77275769e-19, 5.70288952e-03, 3.08921858e-06,\n",
       "        3.01061561e-07, 9.51025072e-07, 2.32990125e-12, 2.08121671e-12,\n",
       "        1.51775281e-09, 5.31591233e-08, 5.25315152e-03, 1.59104839e-14,\n",
       "        5.33811090e-07, 1.34891458e-15, 8.18295745e-15, 5.05086017e-09,\n",
       "        2.00719683e-06, 3.48244309e-02, 1.25456784e-10, 5.38812717e-04,\n",
       "        4.13826022e-08, 7.93262006e-05],\n",
       "       [1.35951581e-16, 8.29528113e-11, 1.11625067e-20, 3.76649532e-16,\n",
       "        1.20763871e-05, 1.05747103e-03, 1.98873743e-23, 3.99025951e-11,\n",
       "        9.35839587e-17, 1.20271526e-08, 9.77569640e-01, 3.22911009e-16,\n",
       "        2.39495211e-03, 1.25606719e-04, 1.92735577e-19, 1.28374649e-02,\n",
       "        5.43287542e-27, 1.17911076e-08, 2.48721009e-19, 1.03626583e-19,\n",
       "        5.50406075e-12, 2.34343334e-13, 8.74647793e-24, 6.00277400e-03,\n",
       "        3.47129040e-22, 5.51774182e-10],\n",
       "       [1.07657754e-04, 3.62300123e-09, 5.66261431e-16, 1.43878069e-07,\n",
       "        3.11845634e-02, 4.97104395e-07, 2.36521616e-16, 3.34043904e-18,\n",
       "        6.24989184e-07, 1.92126708e-06, 3.85531873e-09, 9.68704164e-01,\n",
       "        7.12437813e-13, 7.79524188e-12, 1.67573710e-09, 6.00435146e-10,\n",
       "        6.02543663e-14, 1.26304570e-07, 5.86756199e-11, 4.68352646e-08,\n",
       "        7.03109926e-16, 1.82766756e-18, 5.59286538e-16, 3.16425968e-07,\n",
       "        3.57725204e-11, 8.82958240e-15],\n",
       "       [1.26231855e-04, 4.39815819e-02, 1.36554874e-14, 2.39600241e-02,\n",
       "        1.92353489e-09, 9.17128820e-15, 1.79948305e-08, 1.60504111e-13,\n",
       "        3.33003009e-05, 1.88579992e-03, 4.08398223e-14, 6.89716548e-11,\n",
       "        4.59096567e-14, 5.60193421e-08, 2.11220444e-03, 1.96140104e-10,\n",
       "        4.21170432e-09, 3.33809039e-05, 1.06055671e-02, 1.07177257e-04,\n",
       "        2.53154667e-07, 6.59632696e-11, 5.38252667e-02, 2.84538696e-13,\n",
       "        8.63329113e-01, 2.94300208e-14]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_message = model3.predict(MESSAGE_2D)\n",
    "predicted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5169f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QPEFIVIBOSINGWIXABTSJUMPQUICKLY\n"
     ]
    }
   ],
   "source": [
    "message_list = \"\"\n",
    "for num in range(len(predicted_message)):\n",
    "    max_val = 0\n",
    "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
    "    predict = chr(int(max_val)+ord('A'))\n",
    "    message_list += predict\n",
    "print(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4298afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy between predicted and actual message: 77.41935483870968%\n",
      "Here is the list of letters that are a mismatch between predicted and actual message:\n",
      "Actual    Predicted\n",
      "T         Q\n",
      "H         P\n",
      "E         I\n",
      "X         S\n",
      "Z         X\n",
      "R         B\n",
      "D         T\n"
     ]
    }
   ],
   "source": [
    "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
    "list_mismatch = []\n",
    "num_total, num_correct = len(actual_message), len(message_list)\n",
    "for i in range(len(actual_message)):\n",
    "    if message_list[i] != actual_message[i]:\n",
    "        num_correct -= 1\n",
    "        list_mismatch.append([actual_message[i], message_list[i]])\n",
    "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
    "if len(list_mismatch) == 0:\n",
    "    print(\"The message appears to be decoded correctly.\")\n",
    "else:\n",
    "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
    "    print(\"Actual    Predicted\")\n",
    "    for i in range(len(list_mismatch)):\n",
    "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120a931",
   "metadata": {},
   "source": [
    "## A: As more hidden layers are added, the categorical accuracy obtained will gradually worsen due to the vanishing gradient. Also, no matter how many hidden layers are added, all the letters in the message will never be decoded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffdd0d",
   "metadata": {},
   "source": [
    "# 9. Load the EMNIST Letters dataset, and use plt.imshow() to verify that the image data has been loaded correctly and that the corresponding labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b8b078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt # Needed to do matplotlib operations\n",
    "emnist_data = np.load('emnist_letters.npz')\n",
    "train_img = emnist_data['train_images']\n",
    "train_label = emnist_data['train_labels']\n",
    "test_img = emnist_data['test_images']\n",
    "test_label = emnist_data['test_labels']\n",
    "validate_img = emnist_data['validate_images']\n",
    "validate_label = emnist_data['validate_labels']\n",
    "\n",
    "# Reshape the images and convert them into float32, and divide by 255.\n",
    "train_img = train_img.reshape((104000, 28, 28))\n",
    "train_img = train_img.astype(\"float32\") / 255\n",
    "test_img = test_img.reshape((20800, 28, 28))\n",
    "test_img = test_img.astype(\"float32\") / 255\n",
    "validate_img = validate_img.reshape((20800, 28, 28))\n",
    "validate_img = validate_img.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63ae17c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.npyio.NpzFile object at 0x000002D3FA7C3520>\n",
      "(104000, 28, 28)\n",
      "(104000, 27)\n",
      "(20800, 28, 28)\n",
      "(20800, 27)\n",
      "(20800, 28, 28)\n",
      "(20800, 27)\n"
     ]
    }
   ],
   "source": [
    "print(emnist_data)\n",
    "print(train_img.shape)\n",
    "print(train_label.shape)\n",
    "print(test_img.shape)\n",
    "print(test_label.shape)\n",
    "print(validate_img.shape)\n",
    "print(validate_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4833abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d40f9ee3d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print out the first 3 letters.\n",
    "plt.imshow(train_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c553ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    }
   ],
   "source": [
    "print(chr(np.argmax(train_label[0])-1+ord('A')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89f614d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d411ad9490>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7499b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n"
     ]
    }
   ],
   "source": [
    "print(chr(np.argmax(train_label[1])-1+ord('A')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fde1520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d411b49460>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAO4ElEQVR4nO3dbYxc5XnG8evaZW2DgcTGhrrYBAImKpTy0pWhBbUgp5HjNMVWQxpapW6DatSCAmrShNIPQapUoSaEIjVJuwkWTpoagQjCjSgFOUgEARaL5fg1DtRxzGJjB5wGY8F6X+5+2ON2bfY8s5458+J9/j9pNbPnnmfOvaO95szMmXMeR4QATH1d7W4AQGsQdiAThB3IBGEHMkHYgUyc1MqVTfP0mKGZrVwlkJV3dUiHY9AT1RoKu+0lku6T1C3pWxFxd+r2MzRTV3pxI6sEkLA+1pXW6n4Zb7tb0tckfVTSRZJutH1RvfcHoLkaec++SNIrEbEzIg5LelDS9dW0BaBqjYT9bEmvjvt9oFh2FNsrbffb7h/SYAOrA9CIRsI+0YcA7/nubUT0RURvRPT2aHoDqwPQiEbCPiBpwbjf50va01g7AJqlkbC/KGmh7fNsT5P0KUlrq2kLQNXq3vUWEcO2b5X0Xxrb9bYqIrZW1hla4qT57/mY5Sgjc9+frHcdHk7WR3/y09JaDB1OjkW1GtrPHhGPS3q8ol4ANBFflwUyQdiBTBB2IBOEHcgEYQcyQdiBTLT0eHa0XtfM9PkDdt47O1m/7eIfJOu7B89I1h/5/tWltQu+NZAcO7w7XRdnRj4ubNmBTBB2IBOEHcgEYQcyQdiBTBB2IBPsepvidt7xG8n6lt/+52S9a8ITEo33arL695/ZWFq78/evSI7d9Ofp85eObtyWrONobNmBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgE+9mnAPdMK63d8PFnG7rvh99OH8J65wvLk/VNi79ePnbu88mxX+g7OVnffV368N3RQ4eS9dywZQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBPsZ58CRq4sP+77ltnp49WXbP/jZH3a509N1i/cnp6l+7c++9eltf7b70uOvedXn07Wly36q2S9++kNyXpuGgq77V2SDkoakTQcEb1VNAWgelVs2a+LiDcquB8ATcR7diATjYY9JD1p+yXbKye6ge2Vtvtt9w9psMHVAahXoy/jr46IPbbPlPSU7R9HxDPjbxARfZL6JOl0z2ZyLqBNGtqyR8Se4nK/pEclLaqiKQDVqzvstmfaPu3IdUkfkbSlqsYAVKuRl/FnSXrU9pH7+feIeKKSrnBcfrpsRmltTnf6mPDXnjs7WT9360vJegwdTtbPeXB3aa3/L7uTY6+anq4f+sIvk/X3/bD83zuGh5Njp6K6wx4ROyVdWmEvAJqIXW9AJgg7kAnCDmSCsAOZIOxAJjjEdQoYOX2ktPbGyDvJsQueTNdr7VqrZWTv66W1z++4ITl23SVrkvWvfOjhZP0f5v1BaW341YHk2KmILTuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lgP/sJ4KQF85P1f/vwv5bWPrHtT5NjT3shfQqCRk8tlDqU9JQvvz859vv/MjdZX3rKvmR9ZM77yovsZwcwVRF2IBOEHcgEYQcyQdiBTBB2IBOEHcgE+9lPAIMXnJmsXzqt/Jjz17elx546vLOunqrQ84t3k/WDI+nTYOP4sGUHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAT7GfvAO6Zlqzv+li6frLTdUCaxJbd9irb+21vGbdstu2nbL9cXM5qbpsAGjWZl/EPSFpyzLI7JK2LiIWS1hW/A+hgNcMeEc9IOnDM4uslrS6ur5a0rNq2AFSt3g/ozoqIvZJUXJZ+Adv2Stv9tvuHNFjn6gA0qumfxkdEX0T0RkRvj6Y3e3UAStQb9n2250lScbm/upYANEO9YV8raUVxfYWkx6ppB0Cz1NzPbnuNpGslzbE9IOlLku6W9JDtmyTtlpSeaBsNiSn61afomaJ/WIeqGfaIuLGktLjiXgA0EU+tQCYIO5AJwg5kgrADmSDsQCY4xLUTdLlGvTVtNEPXzJmltR2fPiU59jdn/CxZPzhaPh20JHUdLq+PJEdOTSfwvxGA40HYgUwQdiAThB3IBGEHMkHYgUwQdiAT7Gdvge6LP5Ss71p+RrJ+y4efqH/lNZ7Oa53G2jPSZxca6l2YrA/+7bGnL/x/Oy7+enJsl3qS9UuevylZP2fHj5P13LBlBzJB2IFMEHYgE4QdyARhBzJB2IFMEHYgE46Ilq3sdM+OKz0FT0rb1Z0sH1h7frL+3OVr0nevGse7J2w+PJSsv/TuB5L107rfSdaXnrIvWU9NJ/3maPq+P7n9T5L1mZ9J/23DA68l61PR+lint+LAhP8wbNmBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgEx7NXYTR9FvLh/5yTrP/y0neT9VldJx93S0dcMq3GMeHT9tR935L0Zo2//e43Li2tPflP1yTHnvHwpmR9+NChZB1Hq7llt73K9n7bW8Ytu8v2a7Y3Fj9Lm9smgEZN5mX8A5KWTLD83oi4rPh5vNq2AFStZtgj4hlJ5ecWAnBCaOQDulttbype5s8qu5Htlbb7bfcPabCB1QFoRL1h/4ak8yVdJmmvpHvKbhgRfRHRGxG9PUqfvBBA89QV9ojYFxEjETEq6ZuSFlXbFoCq1RV22/PG/bpc0pay2wLoDDX3s9teI+laSXNsD0j6kqRrbV8mKSTtknRz81o88c37Tvq5sO/mK5L1L56xve51N3o8+1Ckj9X/8hMfT9YvfOB/Smuzt72YHDs6nJ5/HcenZtgj4sYJFt/fhF4ANBFflwUyQdiBTBB2IBOEHcgEYQcywSGuLTDy1lvJ+v0/uC5Z/+wnfpSsp07X/IeP3ZYcu/BvNiTrtVww9EKyPtrQvaNKbNmBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHcgE+9k7wOgp6dMxN3bn6XIMHW7eutFR2LIDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJ9rO3Qlf6dMwXXziQrE93etrl9LrrH4qphX8FIBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcywX72VhhNH6++dcf8ZH3wgvS0y6nzxv/R7z6XHLuhZ0ayzvHuU0fNLbvtBbaftr3d9lbbtxXLZ9t+yvbLxeWs5rcLoF6TeRk/LOlzEfFrkq6SdIvtiyTdIWldRCyUtK74HUCHqhn2iNgbERuK6wclbZd0tqTrJa0ubrZa0rIm9QigAsf1AZ3tcyVdLmm9pLMiYq809oQg6cySMStt99vuH9Jgg+0CqNekw277VEmPSLo9ItIzFY4TEX0R0RsRvT2aXk+PACowqbDb7tFY0L8bEd8rFu+zPa+oz5O0vzktAqhCzV1vti3pfknbI+Kr40prJa2QdHdx+VhTOsxA1zvpQ2Abcf6M9HPwBp3TtHWjs0xmP/vVkj4tabPtjcWyOzUW8ods3yRpt6QbmtIhgErUDHtEPCvJJeXF1bYDoFn4uiyQCcIOZIKwA5kg7EAmCDuQCQ5xbQWX7cwYM33+28l6V43n5G7znI3a+C8BMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAT7GdvhYhkeXD3qcn66FWjyfpIpOuAxJYdyAZhBzJB2IFMEHYgE4QdyARhBzJB2IFMsJ+9A/S8nT7e/eDocLJ+cnf5lM3/sf/S9MrjzXQdUwZbdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMjGZ+dkXSPq2pF+RNCqpLyLus32XpL+Q9PPipndGxOPNanQqO++hXyTrt1y7PFn/2NzNpbVtz38wve7hfck6po7JfKlmWNLnImKD7dMkvWT7qaJ2b0R8pXntAajKZOZn3ytpb3H9oO3tks5udmMAqnVc79ltnyvpcknri0W32t5ke5XtWSVjVtrut90/pMHGugVQt0mH3fapkh6RdHtEvCXpG5LOl3SZxrb890w0LiL6IqI3Inp7NL3xjgHUZVJht92jsaB/NyK+J0kRsS8iRiJiVNI3JS1qXpsAGlUz7LYt6X5J2yPiq+OWzxt3s+WStlTfHoCqOGqc5tj2NZJ+KGmzxna9SdKdkm7U2Ev4kLRL0s3Fh3mlTvfsuNKLG+t4CvL09NubrgvOTdZHp5V/ztr9evoQ1uG9ryfrOLGsj3V6Kw5MeMz0ZD6Nf1bSRIPZpw6cQPgGHZAJwg5kgrADmSDsQCYIO5AJwg5kglNJd4AYTB8zMLJ1R933nT4JNXLClh3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUzUPJ690pXZP5f0s3GL5kh6o2UNHJ9O7a1T+5LorV5V9vaBiJg7UaGlYX/Pyu3+iOhtWwMJndpbp/Yl0Vu9WtUbL+OBTBB2IBPtDntfm9ef0qm9dWpfEr3VqyW9tfU9O4DWafeWHUCLEHYgE20Ju+0ltnfYfsX2He3ooYztXbY3295ou7/Nvayyvd/2lnHLZtt+yvbLxeWEc+y1qbe7bL9WPHYbbS9tU28LbD9te7vtrbZvK5a39bFL9NWSx63l79ltd0v6iaTfkzQg6UVJN0bEtpY2UsL2Lkm9EdH2L2DY/h1Jb0v6dkT8erHsHyUdiIi7iyfKWRHxxQ7p7S5Jb7d7Gu9itqJ546cZl7RM0p+pjY9doq9PqgWPWzu27IskvRIROyPisKQHJV3fhj46XkQ8I+nAMYuvl7S6uL5aY/8sLVfSW0eIiL0RsaG4flDSkWnG2/rYJfpqiXaE/WxJr477fUCdNd97SHrS9ku2V7a7mQmcdWSareLyzDb3c6ya03i30jHTjHfMY1fP9OeNakfYJ5pKqpP2/10dEVdI+qikW4qXq5icSU3j3SoTTDPeEeqd/rxR7Qj7gKQF436fL2lPG/qYUETsKS73S3pUnTcV9b4jM+gWl/vb3M//6aRpvCeaZlwd8Ni1c/rzdoT9RUkLbZ9ne5qkT0la24Y+3sP2zOKDE9meKekj6rypqNdKWlFcXyHpsTb2cpROmca7bJpxtfmxa/v05xHR8h9JSzX2ifx/S/q7dvRQ0tcHJf2o+Nna7t4krdHYy7ohjb0iuknSGZLWSXq5uJzdQb19R2NTe2/SWLDmtam3azT21nCTpI3Fz9J2P3aJvlryuPF1WSATfIMOyARhBzJB2IFMEHYgE4QdyARhBzJB2IFM/C/8P2qoZphfdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_img[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55f57f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    }
   ],
   "source": [
    "print(chr(np.argmax(train_label[2])-1+ord('A')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b28a35",
   "metadata": {},
   "source": [
    "# 10. Apply the network architecture from Cholletâ€™s MNIST notebook to the EMNIST Letters data. (You will need to modify the numbers of inputs and outputs, but should leave the dense layer intact.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d72726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(27, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "386973a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47f82768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 2.3292 - accuracy: 0.4218\n",
      "Epoch 2/5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.4081 - accuracy: 0.5967\n",
      "Epoch 3/5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.2076 - accuracy: 0.6479\n",
      "Epoch 4/5\n",
      "813/813 [==============================] - 6s 7ms/step - loss: 1.1196 - accuracy: 0.6734\n",
      "Epoch 5/5\n",
      "813/813 [==============================] - 5s 6ms/step - loss: 1.0606 - accuracy: 0.6907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d3f971ee20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_img, train_label, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e647002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 1s 2ms/step - loss: 1.0451 - accuracy: 0.6977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0451291799545288, 0.697692334651947]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_img, y=test_label, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a69d1d",
   "metadata": {},
   "source": [
    "# What accuracy do you achieve? How does this compare with the accuracy for MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a7ddf",
   "metadata": {},
   "source": [
    "## A: The accuracy obtained is 69.77%, and it is below MNIST's accuracy of 98.89%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393a9ec",
   "metadata": {},
   "source": [
    "# 11. Keeping the same number of layers in the network (i.e. an MLP with a single hidden layer), modify the architecture to improve the accuracy. You will need to decide on an appropriate number of neurons in the hidden layer. Keep in mind that:\n",
    "## a. There are 27 classes rather than 10, so you will need a larger hidden layer than the MNIST network.\n",
    "## b. In addition to having more classes, EMNIST Letters mixes upper- and lowercase letters within each class, so even with enough neurons in the hidden layer, your accuracy is likely to be lower.  See the details in the EMNIST paper for the kind of performance you might reasonably expect.\n",
    "## c. The Keras fit() method can take a validation_data parameter in order to evaluate metrics on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "174e26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation=\"relu\"),     #doubled the dense layer 512 (acc:69%)->1024 (acc:71%)\n",
    "    layers.Dense(27, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6c6b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc6e8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "813/813 [==============================] - 11s 13ms/step - loss: 2.1145 - accuracy: 0.4599 - val_loss: 1.4403 - val_accuracy: 0.5928\n",
      "Epoch 2/5\n",
      "813/813 [==============================] - 11s 14ms/step - loss: 1.2845 - accuracy: 0.6277 - val_loss: 1.1876 - val_accuracy: 0.6549\n",
      "Epoch 3/5\n",
      "813/813 [==============================] - 11s 13ms/step - loss: 1.1330 - accuracy: 0.6692 - val_loss: 1.0919 - val_accuracy: 0.6851\n",
      "Epoch 4/5\n",
      "813/813 [==============================] - 11s 13ms/step - loss: 1.0523 - accuracy: 0.6926 - val_loss: 1.0203 - val_accuracy: 0.7040\n",
      "Epoch 5/5\n",
      "813/813 [==============================] - 11s 13ms/step - loss: 0.9820 - accuracy: 0.7146 - val_loss: 0.9543 - val_accuracy: 0.7229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d412deddc0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_img, train_label, validation_data=(validate_img, validate_label), epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d422fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 2s 3ms/step - loss: 0.9602 - accuracy: 0.7212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9601578712463379, 0.7211538553237915]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_img, test_label, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1bf2c",
   "metadata": {},
   "source": [
    "## The accuracy is slightly better than the previous one, and it ended up being 72.12%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa086f",
   "metadata": {},
   "source": [
    "# 12. The Keras examples include a Simple MNIST convnet. Note the accuracy obtained by that code compared to the previous example from Chollet.\n",
    "# Rather than building a deeper MLP, letâ€™s apply this architecture to the EMNIST Letters data. What accuracy do you achieve? How does this compare with the accuracy for the MNIST dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba885d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104000, 28, 28, 1)\n",
      "(28, 28, 1)\n",
      "(20800, 28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 27\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "train_img = train_img.reshape(104000, 28, 28)\n",
    "test_img = test_img.reshape(20800, 28, 28)\n",
    "\n",
    "train_img_convnet = np.expand_dims(train_img, -1)\n",
    "test_img_convnet = np.expand_dims(test_img, -1)\n",
    "\n",
    "# #check\n",
    "print(train_img_convnet.shape)\n",
    "print(train_img_convnet[0].shape)\n",
    "print(test_img_convnet.shape)\n",
    "print(test_img_convnet[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d92245b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 27)                43227     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,043\n",
      "Trainable params: 62,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97c165d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cf21f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "732/732 [==============================] - 37s 50ms/step - loss: 2.2025 - accuracy: 0.3643 - val_loss: 1.2511 - val_accuracy: 0.6313\n",
      "Epoch 2/5\n",
      "732/732 [==============================] - 37s 50ms/step - loss: 1.2470 - accuracy: 0.6262 - val_loss: 0.9901 - val_accuracy: 0.7111\n",
      "Epoch 3/5\n",
      "732/732 [==============================] - 36s 50ms/step - loss: 1.0021 - accuracy: 0.6976 - val_loss: 0.7587 - val_accuracy: 0.7727\n",
      "Epoch 4/5\n",
      "732/732 [==============================] - 38s 52ms/step - loss: 0.8342 - accuracy: 0.7461 - val_loss: 0.6299 - val_accuracy: 0.8084\n",
      "Epoch 5/5\n",
      "732/732 [==============================] - 41s 57ms/step - loss: 0.7364 - accuracy: 0.7744 - val_loss: 0.5594 - val_accuracy: 0.8297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d41320dd00>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_img_convnet, train_label, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "baad2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 4s 6ms/step - loss: 0.5577 - accuracy: 0.8367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5577357411384583, 0.8366826772689819]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(test_img_convnet, test_label, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcdaa2",
   "metadata": {},
   "source": [
    "## The accuracy obtained is 83.67%, and it is still below the accuracy for MNIST, despite being significantly better than the previous two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d61b8",
   "metadata": {},
   "source": [
    "# 13. Use plt.imshow() to view some of the misclassified images and examine their labels. Describe what you think might have gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c8d1f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of images that appears to be misclassified:\n",
      "[3, 19, 21, 33, 36, 37, 41, 44, 47, 48, 51, 52, 57, 68, 69, 76, 80, 90, 95, 106, 111, 112, 117, 118, 119, 121, 123, 128, 132, 138, 143, 145, 157, 159, 164, 165, 170, 178, 179, 180, 184, 189, 190, 197]\n"
     ]
    }
   ],
   "source": [
    "# Only plt.imshow() will be used to view the misclassified images.\n",
    "misclassified_images = []\n",
    "for i in range(200):\n",
    "    predict_image = model3.predict(test_img[i].reshape(1, 28, 28))\n",
    "    predict_letter = chr(np.argmax(predict_image)-1+ord('A'))\n",
    "    actual_letter = chr(np.argmax(test_label[i])-1+ord('A'))\n",
    "    if predict_letter != actual_letter:\n",
    "        misclassified_images.append(i)\n",
    "print(\"List of images that appears to be misclassified:\")\n",
    "print(misclassified_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f900650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41b5def40>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[0]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3918979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41376b5e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[1]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a2dde87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d4130c94c0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[2]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb370189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41b667100>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[3]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b325d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41b6b7af0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[4]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "885affa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41b7103a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[5]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43f08eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d412d71f70>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARZklEQVR4nO3dfZBV9XkH8O93X5FdiC4KroCCSIraUsAdtLXJqAxWSVtfGqlMk7GtLWFGnVjNtJZOGtO0E5smdZw2JYMNFVtLamuspHWiDGItbYewUMpLUfEFYVm6JKwVVtzX+/SPvbYL7nnOct/Ohef7mdm5u/fZs/fZu/e75977O+f3o5lBRM5+NVk3ICKVobCLBKGwiwShsIsEobCLBFFXyRtrYKONQ1Mlb1IklF68j37r42i1osJO8iYAjwGoBfAXZvaI9/3j0ISruaiYmxQRxxbbmFgr+Gk8yVoA3wRwM4ArACwjeUWhP09EyquY1+wLAbxhZm+ZWT+A7wC4pTRtiUipFRP2qQAOjvi6I3/dSUguJ9lOsn0AfUXcnIgUo5iwj/YmwEeOvTWz1WbWZmZt9Wgs4uZEpBjFhL0DwPQRX08D0FlcOyJSLsWEfSuA2SRnkmwAcCeA9aVpS0RKreChNzMbJHkvgBcwPPS2xsz2lKwzESmposbZzex5AM+XqBcRKSMdLisShMIuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhJERZdsljMQR139d0S9iP1FbqjwbeW0ac8uEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEkSYcXY2Nrr1mstmuPVcQ/JdVdM/6G5rBzr9em+fWy8njvPvl/eWXOnWT0z29xf1PZZYm/xSh7vt4AG/Dkv+2fJRRYWd5H4AxwEMARg0s7ZSNCUipVeKPfv1ZvajEvwcESkjvWYXCaLYsBuAF0luI7l8tG8guZxkO8n2AWT32lQkumKfxl9rZp0kJwPYQPJVM3tl5DeY2WoAqwFgIlv0jopIRoras5tZZ/7yCIBnASwsRVMiUnoFh51kE8kJH34O4EYAu0vVmIiUVjFP46cAeJbD5zvXAfgbM/t+Sboqg7Rx9HGrut36py7YlVh7s3eyu+26rVe79ZqeWrdeTrlm/5zyVTf8pVu/qO49t/4/uXGJtRWf+oy77SVfaXbruZ2vunWNw5+s4LCb2VsAfrKEvYhIGWnoTSQIhV0kCIVdJAiFXSQIhV0kiDCnuKaZ1eyfy7NswoHEWv3EQ+62K5f8oKCeqkEj6936gPn7i0bmEmsbF37L3XbRF1e49ZkPTnPrg+8cdOvRaM8uEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEkSYcXZ72x9z/efHrnHrN/zSrMTafbM2+due845bn1BTvj/D8Zw/zfW/9E516ztPXOzWn967wK1/9apnE2u3Nrmb4vEFT7r1+xfd49ZbnnCm8A64XLT27CJBKOwiQSjsIkEo7CJBKOwiQSjsIkEo7CJB0Co43e5EttjVXFSx2zsdrPPHumtbL0ys9V3mTyXdcb2/LPJAc/n+BvU9dOtTX+53641dPW6d73/g1vd99bzE2o5PPO5uOwB/LPyqdQ+49ctWbkus2YD/e5+ptthGHLPuUf/o2rOLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBBHmfPY0Nuif9z14sCOxVuvUAGDG5oaCeqqEtPHmtLO+a+bOcetLL9+eWGuk//Db2ecvZX1u2orNQ/HOWfek7tlJriF5hOTuEde1kNxAcl/+MvnICRGpCmN5Gv8EgJtOue4hABvNbDaAjfmvRaSKpYbdzF4B0H3K1bcAWJv/fC2AW0vbloiUWqFv0E0xs8MAkL9MPDic5HKS7STbB9BX4M2JSLHK/m68ma02szYza6uHf0KIiJRPoWHvItkKAPnLI6VrSUTKodCwrwdwV/7zuwA8V5p2RKRcUsfZSa4DcB2A80l2APgSgEcAPE3ybgAHANxRziarXo0/HlwsGxxw66xNvn3LFXeufM348W6984YWt/7ljyWPs+dS9jX/eGyeW79g67tuPRdwbnhPatjNbFlCqTpnoRCRUelwWZEgFHaRIBR2kSAUdpEgFHaRIHSK6xjVNCWvL9z9i3PdbY/O9Ye/Gt7z/+dOf+G4Wz+weEJibcJB/7Yn/cMetz4w91K3fsevveTW5zck/27v5nrdbde9dK1b//ibu9y6nEx7dpEgFHaRIBR2kSAUdpEgFHaRIBR2kSAUdpEg4oyz01+6uO7iaW6947bpibXfXPH37ra3Nb/j1nf3+zP4rFjwGbf+53O/lVg7NOhP/Pt78/2zk63BH6d/6rzkZZEBoAbnJNY2nLjY3Xbm9/xprnPvv+/W5WTas4sEobCLBKGwiwShsIsEobCLBKGwiwShsIsEcfaMs6eMo6ctLfz2F/3poB9f8GeJtbZGf8riAfN7G0r5nzuz5dSl9k52dKg5sba02V+/Y/Gnv+HWOwf9h8ikmuRxdAA4mvsgsfYHO5e42166r8ut+4tsy6m0ZxcJQmEXCUJhFwlCYRcJQmEXCUJhFwlCYRcJ4qwZZ6+58sfc+qEv+9tvXJB8TjgAnF+bPJ78TM9kd9vfefnTbn3SD/w/w+R/P+rWv/DA0sTaz96UfHwAkD5OPqnBLaPP/NHupXt/ObE24w/94xMGD3X6N15OKctwe8tkj4UNOb97mZaaTt2zk1xD8gjJ3SOue5jkIZI78h/+0REikrmxPI1/AsBNo1z/qJnNy388X9q2RKTUUsNuZq8A8I/XFJGqV8wbdPeS3Jl/mp840RnJ5STbSbYPoK+ImxORYhQa9lUAZgGYB+AwgMSzKcxstZm1mVlbPfyJFUWkfAoKu5l1mdmQmeUAPA5gYWnbEpFSKyjsJFtHfHkbgN1J3ysi1SF1nJ3kOgDXATifZAeALwG4juQ8AAZgP4DPla/F/+etkb7vV891t315wdfd+uTa8W7973omJdb+6E+Xudte/tf+GuhDx3rcujX5vSH3Mb/uqKX//37Icm79P/r9h1DP062JtXG7t7rbwvw564tRO3GiWz9665V+fW5xvV20Ofl+PWe9Pxd/oePwqWE3s9Eeyd8u6NZEJDM6XFYkCIVdJAiFXSQIhV0kCIVdJIgz6xTXWcnLJt+56F/dTVtThta8KY8BYOWLyaeRzvnbfe62uQ963Xrd1OThKQA41jbVra/4qZcSa+fQP0c1bWjt3ZT75Te2+6OuMzd0JNYGB/3TY1nnPzxrWy90632XJZ96/MYv+PfL7y952q3/XFPy7wUAJ1KGxz6BLyTWLvsn//RZK3DoTXt2kSAUdpEgFHaRIBR2kSAUdpEgFHaRIBR2kSCqa5w9ZdnlY3POTax9svnVom76eM4/ZXFc6/uJtddWzvJ/uP9r4cI5/rLK9830x3xvbvKmXC5udqAn3/sJt966yv/5ua4fJtbqLkk+bgIAuhZPc+t1tyf/bAC4b9ZzibXF4w+426ZNsZ12v57ACbdedyLlQVEG2rOLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBFFd4+wpUweP70pePuq/B9OmU/bHsmfU+ee7t1+zJrGWu8Y/JzxNPYtb/reujCvtdA8mT98NAG/f7j+E+PPzEmtpxxc89vFvuvX5Df758I1M7i2Hce622/r9c8Z39frHCGx6d45bv/iF5HkCbKDf3bZQ2rOLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBEEr47K4p5rIFruaiwrevm5a8vzpr3/tAnfbh6/6nlu/4Zx33PqEmsIPSRiCfx/v7vfHyR89dKNbXzoleenj25q63W1rUk62H4Q/3jxghc1hDqQfX1CTsi/a0++Ps2/rvSSx9nqvP+f899f+tFtv3XzMrdd8MODWc2/sT6xZX/LxJGm22EYcs+5R/6ipe3aS00luIrmX5B6Sn89f30JyA8l9+cvzCu5QRMpuLE/jBwE8aGaXA7gGwD0krwDwEICNZjYbwMb81yJSpVLDbmaHzWx7/vPjAPYCmArgFgBr89+2FsCtZepRRErgtN6gIzkDwHwAWwBMMbPDwPA/BACjLqxFcjnJdpLtAyj8tYiIFGfMYSfZDOAZAPebmf/uxAhmttrM2sysrb6MJ2yIiG9MYSdZj+GgP2Vm381f3UWyNV9vRdppZSKSqdShN5LE8GvybjO7f8T1fwzgqJk9QvIhAC1m9lvezyp26M2barpu6kXupn2zp7j1juv9Zx0DzYUPUTLlDNhz9/rDX5M3+1Mmd96YvDTxA/f401Avm9Dl1tOG5j6wwk/HPJ7zh85eOHGpW//K87e79dnrkqf/rjnh921vH3TruRP+VNFZ8YbexjJ4fC2AzwLYRXJH/rqVAB4B8DTJuwEcAHBHCXoVkTJJDbuZbUbyMgdF7KZFpJJ0uKxIEAq7SBAKu0gQCrtIEAq7SBDVNZV0GueYgMGOQ+6mtSn1GZsbCmqpFGzIP010KOfXWzuTx8ofzS11t93z6//m1mvhHySwbuvVbr2mJ/k01voefwx/2ib/8Oo5r/vLLg8eSl7KuvATc89c2rOLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBHFGTSUtp69mvL8UNWf6Sw+nsQPJY9kAYL2FT0VWrqWLz2ZFTSUtImcHhV0kCIVdJAiFXSQIhV0kCIVdJAiFXSSIM+t8djltqfOb73mtMo1I5rRnFwlCYRcJQmEXCUJhFwlCYRcJQmEXCUJhFwkiNewkp5PcRHIvyT0kP5+//mGSh0juyH8sKX+7IlKosRxUMwjgQTPbTnICgG0kN+Rrj5rZ18vXnoiUyljWZz8M4HD+8+Mk9wKYWu7GRKS0Tus1O8kZAOYD2JK/6l6SO0muIXlewjbLSbaTbB9A4VMUiUhxxhx2ks0AngFwv5kdA7AKwCwA8zC85//GaNuZ2WozazOztno0Ft+xiBRkTGEnWY/hoD9lZt8FADPrMrMhM8sBeBzAwvK1KSLFGsu78QTwbQB7zexPRlzfOuLbbgOwu/TtiUipjOXd+GsBfBbALpI78tetBLCM5DwABmA/gM+VoT8RKZGxvBu/GcBo81A/X/p2RKRcdASdSBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQNLPK3Rj5QwDvjLjqfAA/qlgDp6dae6vWvgD1VqhS9naJmV0wWqGiYf/IjZPtZtaWWQOOau2tWvsC1FuhKtWbnsaLBKGwiwSRddhXZ3z7nmrtrVr7AtRboSrSW6av2UWkcrLes4tIhSjsIkFkEnaSN5F8jeQbJB/KoockJPeT3JVfhro9417WkDxCcveI61pIbiC5L3856hp7GfVWFct4O8uMZ3rfZb38ecVfs5OsBfA6gMUAOgBsBbDMzP6roo0kILkfQJuZZX4ABslPAugB8KSZ/Xj+uq8B6DazR/L/KM8zs9+ukt4eBtCT9TLe+dWKWkcuMw7gVgC/ggzvO6evpajA/ZbFnn0hgDfM7C0z6wfwHQC3ZNBH1TOzVwB0n3L1LQDW5j9fi+EHS8Ul9FYVzOywmW3Pf34cwIfLjGd63zl9VUQWYZ8K4OCIrztQXeu9G4AXSW4juTzrZkYxxcwOA8MPHgCTM+7nVKnLeFfSKcuMV819V8jy58XKIuyjLSVVTeN/15rZAgA3A7gn/3RVxmZMy3hXyijLjFeFQpc/L1YWYe8AMH3E19MAdGbQx6jMrDN/eQTAs6i+pai7PlxBN395JON+/k81LeM92jLjqIL7Lsvlz7MI+1YAs0nOJNkA4E4A6zPo4yNINuXfOAHJJgA3ovqWol4P4K7853cBeC7DXk5SLct4Jy0zjozvu8yXPzezin8AWILhd+TfBPC7WfSQ0NelAP4z/7En694ArMPw07oBDD8juhvAJAAbAezLX7ZUUW9/BWAXgJ0YDlZrRr39DIZfGu4EsCP/sSTr+87pqyL3mw6XFQlCR9CJBKGwiwShsIsEobCLBKGwiwShsIsEobCLBPG/tKg8xm0VpWcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[6]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c148e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d41b497580>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[-1]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "120c0e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d413026af0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[misclassified_images[-2]].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c00d40",
   "metadata": {},
   "source": [
    "## A: Although the letters are the same in both the images and the labels, the images does not necessarily match the ones from the labels. It is as if the letters are being written in a different manner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
